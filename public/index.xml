<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ryan Yang</title>
    <link>https://www.yangcs.net/</link>
    <description>Recent content on Ryan Yang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Tue, 23 Jan 2018 08:26:58 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Linux全局智能分流方案</title>
      <link>https://www.yangcs.net/posts/linux-circumvent/</link>
      <pubDate>Tue, 23 Jan 2018 08:26:58 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/linux-circumvent/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;本来我是决定不再写这样的文章了的。但是呢，最近连续配置了两次 &lt;code&gt;ArchLinux&lt;/code&gt;，在配置这种东西的时候连续撞到了同样的坑，加上这段时间经常有人问我关于 &lt;code&gt;Linux&lt;/code&gt; 下的 &lt;code&gt;shadowsocks&lt;/code&gt; 的问题，所以我想了想还是写一篇记录一下吧，也免得自己以后再忘记了。&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;这里有两种方案，都可以实现全局智能分流。第一种方案的思路是使用 &lt;code&gt;ipset&lt;/code&gt; 载入 &lt;code&gt;chnroute&lt;/code&gt; 的 &lt;code&gt;IP&lt;/code&gt; 列表并使用 &lt;code&gt;iptables&lt;/code&gt; 实现带自动分流国内外流量的全局代理。为什么不用 &lt;code&gt;PAC&lt;/code&gt; 呢？因为 &lt;code&gt;PAC&lt;/code&gt; 这种东西只对浏览器有用。难道你在浏览器之外就不需要科学上网了吗？反正我是不信的……&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;font color=Blue&gt;本教程所用系统为 &lt;code&gt;Archlinux&lt;/code&gt;，其他发型版类似，请自行参考相关资料。&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-markdown-1-style-margin-bottom-2em-margin-right-5px-padding-8px-15px-letter-spacing-2px-background-image-linear-gradient-to-right-bottom-rgb-0-188-212-rgb-63-81-181-background-color-rgb-63-81-181-color-rgb-255-255-255-border-left-10px-solid-rgb-51-51-51-border-radius-5px-text-shadow-rgb-102-102-102-1px-1px-1px-box-shadow-rgb-102-102-102-1px-1px-2px-1-通过-iptables-实现智能分流-p&#34;&gt;&lt;p markdown=&#34;1&#34; style=&#34;margin-bottom:2em; margin-right: 5px; padding: 8px 15px; letter-spacing: 2px; background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181)); background-color: rgb(63, 81, 181); color: rgb(255, 255, 255); border-left: 10px solid rgb(51, 51, 51); border-radius:5px; text-shadow: rgb(102, 102, 102) 1px 1px 1px; box-shadow: rgb(102, 102, 102) 1px 1px 2px;&#34;&gt;1. 通过 iptables 实现智能分流&lt;/p&gt;&lt;/h2&gt;

&lt;h3 id=&#34;1-1-安装相关软件&#34;&gt;1.1 安装相关软件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;shadowsocks-libev&lt;/li&gt;
&lt;li&gt;ipset&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pacman -S shadowsocks-libev ipset
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-2-配置shadowsocks-libev-略过&#34;&gt;1.2 配置shadowsocks-libev（略过）&lt;/h3&gt;

&lt;p&gt;假设shadowsocks配置文件为/etc/shadowsocks.json&lt;/p&gt;

&lt;h3 id=&#34;1-3-获取中国ip段&#34;&gt;1.3 获取中国IP段&lt;/h3&gt;

&lt;p&gt;将以下命令写入脚本保存执行（假设保存在/home/yang/bin/路由表/目录下）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/sh
wget -c http://ftp.apnic.net/stats/apnic/delegated-apnic-latest
cat delegated-apnic-latest | awk -F &#39;|&#39; &#39;/CN/&amp;amp;&amp;amp;/ipv4/ {print $4 &amp;quot;/&amp;quot; 32-log($5)/log(2)}&#39; | cat &amp;gt; /home/yang/bin/路由表/cn_rules.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-4-创建启动和关闭脚本&#34;&gt;1.4 创建启动和关闭脚本&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vim /home/yang/bin/shadowsocks/ss-up.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash

SOCKS_SERVER=$SERVER_IP # SOCKS 服务器的 IP 地址
# Setup the ipset
ipset -N chnroute hash:net maxelem 65536

for ip in $(cat &#39;/home/yang/bin/路由表/cn_rules.conf&#39;); do
  ipset add chnroute $ip
done

# 在nat表中新增一个链，名叫：SHADOWSOCKS
iptables -t nat -N SHADOWSOCKS

# Allow connection to the server
iptables -t nat -A SHADOWSOCKS -d $SOCKS_SERVER -j RETURN

# Allow connection to reserved networks
iptables -t nat -A SHADOWSOCKS -d 0.0.0.0/8 -j RETURN
iptables -t nat -A SHADOWSOCKS -d 10.0.0.0/8 -j RETURN
iptables -t nat -A SHADOWSOCKS -d 127.0.0.0/8 -j RETURN
iptables -t nat -A SHADOWSOCKS -d 169.254.0.0/16 -j RETURN
iptables -t nat -A SHADOWSOCKS -d 172.16.0.0/12 -j RETURN
iptables -t nat -A SHADOWSOCKS -d 192.168.0.0/16 -j RETURN
iptables -t nat -A SHADOWSOCKS -d 224.0.0.0/4 -j RETURN
iptables -t nat -A SHADOWSOCKS -d 240.0.0.0/4 -j RETURN

# Allow connection to chinese IPs
iptables -t nat -A SHADOWSOCKS -p tcp -m set --match-set chnroute dst -j RETURN
# 如果你想对 icmp 协议也实现智能分流，可以加上下面这一条
# iptables -t nat -A SHADOWSOCKS -p icmp -m set --match-set chnroute dst -j RETURN

# Redirect to Shadowsocks
# 把1081改成你的shadowsocks本地端口
iptables -t nat -A SHADOWSOCKS -p tcp -j REDIRECT --to-port 1081
# 如果你想对 icmp 协议也实现智能分流，可以加上下面这一条
# iptables -t nat -A SHADOWSOCKS -p icmp -j REDIRECT --to-port 1081

# 将SHADOWSOCKS链中所有的规则追加到OUTPUT链中
iptables -t nat -A OUTPUT -p tcp -j SHADOWSOCKS
# 如果你想对 icmp 协议也实现智能分流，可以加上下面这一条
# iptables -t nat -A OUTPUT -p icmp -j SHADOWSOCKS

# 内网流量流经 shadowsocks 规则链
iptables -t nat -A PREROUTING -s 192.168/16 -j SHADOWSOCKS
# 内网流量源NAT
iptables -t nat -A POSTROUTING -s 192.168/16 -j MASQUERADE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;emsp;&amp;emsp;这是在启动 &lt;code&gt;shadowsocks&lt;/code&gt; 之前执行的脚本，用来设置 &lt;code&gt;iptables&lt;/code&gt; 规则，对全局应用代理并将 &lt;code&gt;chnroute&lt;/code&gt; 导入 &lt;code&gt;ipset&lt;/code&gt; 来实现自动分流。注意要把服务器 &lt;code&gt;IP&lt;/code&gt; 和本地端口相关的代码全部替换成你自己的。
&amp;emsp;&amp;emsp;这里就有一个坑了，就是在把 &lt;code&gt;chnroute.txt&lt;/code&gt; 加入 &lt;code&gt;ipset&lt;/code&gt; 的时候。因为 &lt;code&gt;chnroute.txt&lt;/code&gt; 是一个 &lt;code&gt;IP&lt;/code&gt; 段列表，而中国持有的 &lt;code&gt;IP&lt;/code&gt; 数量上还是比较大的，所以如果使用 &lt;code&gt;hash:ip&lt;/code&gt; 来导入的话会使内存溢出。我在第二次重新配置的时候就撞进了这个大坑……
&amp;emsp;&amp;emsp;但是你也不能尝试把整个列表导入 &lt;code&gt;iptables&lt;/code&gt;。虽然导入 &lt;code&gt;iptables&lt;/code&gt; 不会导致内存溢出，但是 &lt;code&gt;iptables&lt;/code&gt; 是线性查表，即使你全部导入进去，也会因为低下的性能而抓狂。
&lt;br \&gt;
然后再创建 &lt;code&gt;/home/yang/bin/shadowsocks/ss-down.sh&lt;/code&gt;, 这是用来清除上述规则的脚本，比较简单&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash

# iptables -t nat -D OUTPUT -p icmp -j SHADOWSOCKS
iptables -t nat -D OUTPUT -p tcp -j SHADOWSOCKS
iptables -t nat -F SHADOWSOCKS
iptables -t nat -X SHADOWSOCKS
ipset destroy chnroute
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接着执行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ chmod +x ss-up.sh
$ chmod +x ss-down.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-5-配置ss-redir服务&#34;&gt;1.5 配置ss-redir服务&lt;/h3&gt;

&lt;p&gt;首先，默认的 &lt;code&gt;ss-local&lt;/code&gt; 并不能用来作为 &lt;code&gt;iptables&lt;/code&gt; 流量转发的目标，因为它是 &lt;code&gt;socks5&lt;/code&gt; 代理而非透明代理。我们至少要把 &lt;code&gt;systemd&lt;/code&gt; 执行的程序改成 &lt;code&gt;ss-redir&lt;/code&gt;。其次，上述两个脚本还不能自动执行，必须让 &lt;code&gt;systemd&lt;/code&gt; 分别在启动 &lt;code&gt;shadowsocks&lt;/code&gt; 之前和关闭之后将脚本执行，这样才能自动配置好 &lt;code&gt;iptables&lt;/code&gt; 规则。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vim /usr/lib/systemd/system/shadowsocks-libev@.service
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[Unit]
Description=Shadowsocks-Libev Client Service
After=network.target

[Service]
User=root
CapabilityBoundingSet=~CAP_SYS_ADMIN
ExecStart=
ExecStartPre=/home/yang/bin/shadowsocks/ss-up.sh
ExecStart=/usr/bin/ss-redir -u -c /etc/%i.json
ExecStopPost=/home/yang/bin/shadowsocks/ss-down.sh

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后启动服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl start shadowsocks-libev@shadowsocks
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;开机自启&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl enable shadowsocks-libev@shadowsocks
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-6-配置智能-dns-服务&#34;&gt;1.6 配置智能 DNS 服务&lt;/h3&gt;

&lt;p&gt;完成了以上工作之后是不是就可以实现全局科学上网了呢？答案是否定的，我们还有最后一项工作需要完成，那就是解决 &lt;code&gt;DNS&lt;/code&gt; 污染问题。如果你不知道什么是 &lt;code&gt;DNS&lt;/code&gt; 污染，我可以简单地给你普及一下：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;DNS&lt;/code&gt; 污染是一种让一般用户由于得到虚假目标主机 &lt;code&gt;IP&lt;/code&gt; 而不能与其通信的方法，是一种 &lt;code&gt;DNS&lt;/code&gt; 缓存投毒攻击（DNS cache poisoning）。其工作方式是：由于通常的 &lt;code&gt;DNS&lt;/code&gt; 查询没有任何认证机制，而且 &lt;code&gt;DNS&lt;/code&gt; 查询通常基于的 &lt;code&gt;UDP&lt;/code&gt; 是无连接不可靠的协议，因此 &lt;code&gt;DNS&lt;/code&gt; 的查询非常容易被篡改，通过对 &lt;code&gt;UDP&lt;/code&gt; 端口 53 上的 &lt;code&gt;DNS&lt;/code&gt; 查询进行入侵检测，一经发现与关键词相匹配的请求则立即伪装成目标域名的解析服务器（NS，Name Server）给查询者返回虚假结果。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;DNS&lt;/code&gt; 污染症状：目前一些被禁止访问的网站很多就是通过 &lt;code&gt;DNS&lt;/code&gt; 污染来实现的，例如 &lt;code&gt;YouTube&lt;/code&gt;、&lt;code&gt;Facebook&lt;/code&gt; 等网站。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;应对dns污染的方法&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对于 &lt;code&gt;DNS&lt;/code&gt; 污染，可以说，个人用户很难单单靠设置解决，通常可以使用 &lt;code&gt;VPN&lt;/code&gt; 或者域名远程解析的方法解决，但这大多需要购买付费的 &lt;code&gt;VPN&lt;/code&gt; 或 &lt;code&gt;SSH&lt;/code&gt; 等&lt;/li&gt;
&lt;li&gt;修改 &lt;code&gt;Hosts&lt;/code&gt; 的方法，手动设置域名正确的 &lt;code&gt;IP&lt;/code&gt; 地址&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dns&lt;/code&gt; 加密解析：&lt;a href=&#34;https://dnscrypt.org/&#34; target=&#34;_blank&#34;&gt;DNSCrypt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;忽略 &lt;code&gt;DNS&lt;/code&gt; 投毒污染小工具：&lt;a href=&#34;https://github.com/chengr28/Pcap_DNSProxy&#34; target=&#34;_blank&#34;&gt;Pcap_DNSProxy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们选择用 &lt;code&gt;Pcap_DNSProxy&lt;/code&gt; 来解决这个问题，以前用的是 &lt;code&gt;Pdnsd + Dnsmasq&lt;/code&gt; 组合， 后来发现 &lt;code&gt;TCP&lt;/code&gt; 请求效率太低加上家里网络与那些国外的 &lt;code&gt;DNS&lt;/code&gt; 丢包实在是严重， 所以打算用 &lt;code&gt;Pcap_DNSProxy&lt;/code&gt; 代替 &lt;code&gt;Pdnsd&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;关于 &lt;code&gt;Pcap_DNSProxy&lt;/code&gt; 的详细介绍，可以参考:
&lt;a href=&#34;https://github.com/chengr28/Pcap_DNSProxy&#34; target=&#34;_blank&#34;&gt;https://github.com/chengr28/Pcap_DNSProxy&lt;/a&gt;
安装过程可以参考：
&lt;a href=&#34;https://github.com/chengr28/Pcap_DNSProxy/blob/master/Documents/ReadMe_Linux.zh-Hans.txt&#34; target=&#34;_blank&#34;&gt;https://github.com/chengr28/Pcap_DNSProxy/blob/master/Documents/ReadMe_Linux.zh-Hans.txt&lt;/a&gt;
更详细的使用说明可以参考：
&lt;a href=&#34;https://github.com/chengr28/Pcap_DNSProxy/blob/master/Documents/ReadMe.zh-Hans.txt&#34; target=&#34;_blank&#34;&gt;https://github.com/chengr28/Pcap_DNSProxy/blob/master/Documents/ReadMe.zh-Hans.txt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这里主要重点强调一些需要注意的配置项：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DNS&lt;/code&gt; - 境外域名解析参数区域（这是最关键的一项配置）&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[DNS]
# 这里一定要填 IPv4 + TCP！！！表示只使用 TCP 协议向境外远程 DNS 服务器发出请求
Outgoing Protocol = IPv4 + TCP
# 建议当系统使用全局代理功能时启用，程序将除境内服务器外的所有请求直接交给系统而不作任何过滤等处理，系统会将请求自动发往远程服务器进行解析
Direct Request = IPv4
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Local DNS&lt;/code&gt; - 境内域名解析参数区域&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[Local DNS]
# 发送请求到境内 DNS 服务器时所使用的协议
Local Protocol = IPv4 + UDP
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Addresses&lt;/code&gt; - 普通模式地址区域&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[Addresses]
...
...
# IPv4 主要境外 DNS 服务器地址
IPv4 Main DNS Address = 8.8.4.4:53
# IPv4 备用境外 DNS 服务器地址
IPv4 Alternate DNS Address = 8.8.8.8:53|208.67.220.220:443|208.67.222.222:5353
# IPv4 主要境内 DNS 服务器地址，用于境内域名解析，推荐使用 onedns
IPv4 Local Main DNS Address = 112.124.47.27:53
# IPv4 备用境内 DNS 服务器地址，用于境内域名解析
IPv4 Local Alternate DNS Address = 114.215.126.16:53
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-7-配置系统-dns-服务器设置&#34;&gt;1.7 配置系统 DNS 服务器设置&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;可参见 &lt;a href=&#34;https://developers.google.com/speed/public-dns/docs/using&#34; target=&#34;_blank&#34;&gt;https://developers.google.com/speed/public-dns/docs/using&lt;/a&gt; 中 &lt;code&gt;Changing your DNS servers settings&lt;/code&gt; 中 &lt;code&gt;Linux&lt;/code&gt; 一节&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;图形界面以 &lt;code&gt;GNOME 3&lt;/code&gt; 为例：&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;打开所有程序列表，并 -&amp;gt; 设置 – 硬件分类 – 网络&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果要对当前的网络配置进行编辑 -&amp;gt; 单击齿轮按钮&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;选中 &lt;code&gt;IPv4&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;DNS&lt;/code&gt; 栏目中，将自动拨向关闭&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在服务器中填入 &lt;code&gt;127.0.0.1&lt;/code&gt; （或103.214.195.99:7300）并应用&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;选中 &lt;code&gt;IPv6&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;DNS&lt;/code&gt; 栏目中，将自动拨向关闭&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在服务器中填入 ::1 并应用&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;请务必确保只填入这两个地址，填入其它地址可能会导致系统选择其它 DNS 服务器绕过程序的代理&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;重启网络连接&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;直接修改系统文件修改 DNS 服务器设置：&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;自动获取地址(DHCP)时：&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;以 &lt;code&gt;root&lt;/code&gt; 权限进入 &lt;code&gt;/etc/dhcp&lt;/code&gt; 或 &lt;code&gt;/etc/dhcp3&lt;/code&gt; 目录（视乎 dhclient.conf 文件位置）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;直接修改 &lt;code&gt;dhclient.conf&lt;/code&gt; 文件，修改或添加 &lt;code&gt;prepend domain-name-servers&lt;/code&gt; 一项即可&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果 &lt;code&gt;prepend domain-name-servers&lt;/code&gt; 一项被 # 注释则需要把注释去掉以使配置生效，不需要添加新的条目&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;dhclient.conf&lt;/code&gt; 文件可能存在多个 &lt;code&gt;prepend domain-name-servers&lt;/code&gt; 项，是各个网络接口的配置项目，直接修改总的配置项目即可&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用 &lt;code&gt;service network(/networking) restart&lt;/code&gt; 或 &lt;code&gt;ifdown/ifup&lt;/code&gt; 或 &lt;code&gt;ifconfig stop/start&lt;/code&gt; 重启网络服务/网络端口&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;非自动获取地址(DHCP)时：&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;以 &lt;code&gt;root&lt;/code&gt; 权限进入 &lt;code&gt;/etc&lt;/code&gt; 目录&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;直接修改 &lt;code&gt;resolv.conf&lt;/code&gt; 文件里的 &lt;code&gt;nameserver&lt;/code&gt; 即可&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果重启后配置被覆盖，则需要修改或新建 &lt;code&gt;/etc/resolvconf/resolv.conf.d&lt;/code&gt; 文件，内容和 &lt;code&gt;resolv.conf&lt;/code&gt; 一样&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用 &lt;code&gt;service network(/networking) restart&lt;/code&gt; 或 &lt;code&gt;ifdown/ifup&lt;/code&gt; 或 &lt;code&gt;ifconfig stop/start&lt;/code&gt; 重启网络服务/网络端口&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;1-8-打开流量转发&#34;&gt;1.8 打开流量转发&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat /etc/sysctl.d/30-ipforward.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;net.ipv4.ip_forward=1

net.ipv6.conf.all.forwarding = 1

net.ipv4.tcp_congestion_control=westwood

net.ipv4.tcp_syn_retries = 5

net.ipv4.tcp_synack_retries = 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑完成后，执行以下命令使变动立即生效&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sysctl -p
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-markdown-1-style-margin-bottom-2em-margin-right-5px-padding-8px-15px-letter-spacing-2px-background-image-linear-gradient-to-right-bottom-rgb-0-188-212-rgb-63-81-181-background-color-rgb-63-81-181-color-rgb-255-255-255-border-left-10px-solid-rgb-51-51-51-border-radius-5px-text-shadow-rgb-102-102-102-1px-1px-1px-box-shadow-rgb-102-102-102-1px-1px-2px-2-通过-nftables-实现智能分流-p&#34;&gt;&lt;p markdown=&#34;1&#34; style=&#34;margin-bottom:2em; margin-right: 5px; padding: 8px 15px; letter-spacing: 2px; background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181)); background-color: rgb(63, 81, 181); color: rgb(255, 255, 255); border-left: 10px solid rgb(51, 51, 51); border-radius:5px; text-shadow: rgb(102, 102, 102) 1px 1px 1px; box-shadow: rgb(102, 102, 102) 1px 1px 2px;&#34;&gt;2. 通过 nftables 实现智能分流&lt;/p&gt;&lt;/h2&gt;

&lt;h3 id=&#34;2-1-安装相关软件&#34;&gt;2.1 安装相关软件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;shadowsocks-libev&lt;/li&gt;
&lt;li&gt;nftables&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pacman -S shadowsocks-libev nftables
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-2-配置shadowsocks-libev-略过&#34;&gt;2.2 配置shadowsocks-libev（略过）&lt;/h3&gt;

&lt;p&gt;假设shadowsocks配置文件为/etc/shadowsocks.json&lt;/p&gt;

&lt;h3 id=&#34;2-3-获取中国ip段&#34;&gt;2.3 获取中国IP段&lt;/h3&gt;

&lt;p&gt;将以下命令写入脚本保存执行（假设保存在/home/yang/bin/路由表/目录下）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/sh
wget -c http://ftp.apnic.net/stats/apnic/delegated-apnic-latest
cat delegated-apnic-latest | awk -F &#39;|&#39; &#39;/CN/&amp;amp;&amp;amp;/ipv4/ {print $4 &amp;quot;/&amp;quot; 32-log($5)/log(2)}&#39; | cat &amp;gt; /home/yang/bin/路由表/cn_rules.conf
cat cn_rules.conf|sed &#39;:label;N;s/\n/, /;b label&#39;|sed &#39;s/$/&amp;amp; }/g&#39;|sed &#39;s/^/{ &amp;amp;/g&#39; &amp;gt; /home/yang/bin/路由表/cn_rules1.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-4-创建启动和关闭脚本&#34;&gt;2.4 创建启动和关闭脚本&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vim /home/yang/bin/shadowsocks/nftables-up.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#! /bin/bash

nft_pre=&amp;quot;/usr/sbin/nft add rule nat prerouting&amp;quot;
nft_out=&amp;quot;/usr/sbin/nft add rule nat output&amp;quot;
chnroute=$(cat &#39;/home/yang/bin/路由表/cn_rules1.conf&#39;)

/usr/bin/nft -f /etc/nftables.conf

${nft_pre} tcp dport 8385 return
${nft_pre} ip daddr 139.162.87.98 return
${nft_pre} ip daddr { 0.0.0.0/8, 10.0.0.0/8, 127.0.0.0/8, 169.254.0.0/16, 172.16.0.0/12, 192.168.0.0/16, 224.0.0.0/4, 240.0.0.0/4, 172.16.39.0/24} return
${nft_pre} ip daddr $chnroute return
${nft_pre} tcp sport { 32768-61000} redirect to 1081
#${nft_pre} ip protocol icmp redirect to 1081
# 内网流量源NAT
nft add rule nat postrouting ip saddr 192.168.0.0/12 masquerade

${nft_out} tcp dport 8385 return
${nft_out} ip daddr 139.162.87.98 return
${nft_out} ip daddr { 0.0.0.0/8, 10.0.0.0/8, 127.0.0.0/8, 169.254.0.0/16, 172.16.0.0/12, 192.168.0.0/16, 224.0.0.0/4, 240.0.0.0/4, 172.16.39.0/24} return
${nft_out} ip daddr $chnroute return
# /proc/sys/net/ipv4/ip_local_port_range，本地发起的连接的端口范围
${nft_out} tcp sport { 32768-61000} redirect to 1081
${nft_out} ip protocol icmp redirect to 1081
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;emsp;&amp;emsp;这是在启动 &lt;code&gt;shadowsocks&lt;/code&gt; 之前执行的脚本，用来设置 &lt;code&gt;nftables&lt;/code&gt; 规则。
然后再创建 &lt;code&gt;/home/yang/bin/shadowsocks/nftables-down.sh&lt;/code&gt;, 这是用来清除上述规则的脚本，比较简单&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash

sudo nft flush table nat
#sudo nft flush table filter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接着执行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ chmod +x nftables-up.sh
$ chmod +x nftables-down.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-5-配置ss-redir服务&#34;&gt;2.5 配置ss-redir服务&lt;/h3&gt;

&lt;p&gt;首先，默认的 &lt;code&gt;ss-local&lt;/code&gt; 并不能用来作为 &lt;code&gt;nftables&lt;/code&gt; 流量转发的目标，因为它是 &lt;code&gt;socks5&lt;/code&gt; 代理而非透明代理。我们至少要把 &lt;code&gt;systemd&lt;/code&gt; 执行的程序改成 &lt;code&gt;ss-redir&lt;/code&gt;。其次，上述两个脚本还不能自动执行，必须让 &lt;code&gt;systemd&lt;/code&gt; 分别在启动 &lt;code&gt;shadowsocks&lt;/code&gt; 之前和关闭之后将脚本执行，这样才能自动配置好 &lt;code&gt;nftables&lt;/code&gt; 规则。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vim /usr/lib/systemd/system/shadowsocks-libev@.service
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[Unit]
Description=Shadowsocks-Libev Client Service
After=network.target

[Service]
User=root
CapabilityBoundingSet=~CAP_SYS_ADMIN
ExecStart=
ExecStartPre=/home/yang/bin/shadowsocks/nftables-up.sh
ExecStart=/usr/bin/ss-redir -u -c /etc/%i.json
ExecStopPost=/home/yang/bin/shadowsocks/nftables-down.sh

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后启动服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl start nftables
$ systemctl start shadowsocks-libev@shadowsocks
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;开机自启&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl enable nftables
$ systemctl enable shadowsocks-libev@shadowsocks
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-6-配置智能-dns-服务&#34;&gt;2.6 配置智能 DNS 服务&lt;/h3&gt;

&lt;p&gt;同上&lt;/p&gt;

&lt;h3 id=&#34;2-7-配置系统-dns-服务器设置&#34;&gt;2.7 配置系统 DNS 服务器设置&lt;/h3&gt;

&lt;p&gt;同上&lt;/p&gt;

&lt;h3 id=&#34;2-8-打开流量转发&#34;&gt;2.8 打开流量转发&lt;/h3&gt;

&lt;p&gt;同上&lt;/p&gt;

&lt;h2 id=&#34;p-markdown-1-style-margin-bottom-2em-margin-right-5px-padding-8px-15px-letter-spacing-2px-background-image-linear-gradient-to-right-bottom-rgb-0-188-212-rgb-63-81-181-background-color-rgb-63-81-181-color-rgb-255-255-255-border-left-10px-solid-rgb-51-51-51-border-radius-5px-text-shadow-rgb-102-102-102-1px-1px-1px-box-shadow-rgb-102-102-102-1px-1px-2px-3-通过策略路由实现智能分流-p&#34;&gt;&lt;p markdown=&#34;1&#34; style=&#34;margin-bottom:2em; margin-right: 5px; padding: 8px 15px; letter-spacing: 2px; background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181)); background-color: rgb(63, 81, 181); color: rgb(255, 255, 255); border-left: 10px solid rgb(51, 51, 51); border-radius:5px; text-shadow: rgb(102, 102, 102) 1px 1px 1px; box-shadow: rgb(102, 102, 102) 1px 1px 2px;&#34;&gt;3. 通过策略路由实现智能分流&lt;/p&gt;&lt;/h2&gt;

&lt;h3 id=&#34;3-1-安装相关软件&#34;&gt;3.1 安装相关软件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;badvpn&lt;/li&gt;
&lt;li&gt;shadowsocks&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pacman -S badvpn shadowsocks
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-2-配置shadowsocks-略过&#34;&gt;3.2 配置shadowsocks（略过）&lt;/h3&gt;

&lt;p&gt;假设shadowsocks配置文件为/etc/shadowsocks.json&lt;/p&gt;

&lt;h3 id=&#34;3-3-获取中国ip段&#34;&gt;3.3 获取中国IP段&lt;/h3&gt;

&lt;p&gt;将以下命令写入脚本保存执行（假设保存在/home/yang/bin/路由表/目录下）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/sh
wget -c http://ftp.apnic.net/stats/apnic/delegated-apnic-latest
cat delegated-apnic-latest | awk -F &#39;|&#39; &#39;/CN/&amp;amp;&amp;amp;/ipv4/ {print $4 &amp;quot;/&amp;quot; 32-log($5)/log(2)}&#39; | cat &amp;gt; /home/yang/bin/路由表/cn_rules.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-4-配置智能-dns-服务&#34;&gt;3.4 配置智能 DNS 服务&lt;/h3&gt;

&lt;p&gt;同上&lt;/p&gt;

&lt;h3 id=&#34;3-5-配置系统-dns-服务器设置&#34;&gt;3.5 配置系统 DNS 服务器设置&lt;/h3&gt;

&lt;p&gt;同上&lt;/p&gt;

&lt;h3 id=&#34;3-6-写路由表启动和终止脚本&#34;&gt;3.6 写路由表启动和终止脚本&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vim /usr/local/bin/socksfwd
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
SOCKS_SERVER=$SERVER_IP # SOCKS 服务器的 IP 地址
SOCKS_PORT=1081 # 本地SOCKS 服务器的端口
GATEWAY_IP=$(ip route|grep &amp;quot;default&amp;quot;|awk &#39;{print $3}&#39;) # 家用网关（路由器）的 IP 地址，你也可以手动指定
TUN_NETWORK_DEV=tun0 # 选一个不冲突的 tun 设备号
TUN_NETWORK_PREFIX=10.0.0 # 选一个不冲突的内网 IP 段的前缀


start_fwd() {
ip tuntap del dev &amp;quot;$TUN_NETWORK_DEV&amp;quot; mode tun
# 添加虚拟网卡
ip tuntap add dev &amp;quot;$TUN_NETWORK_DEV&amp;quot; mode tun
# 给虚拟网卡绑定IP地址
ip addr add &amp;quot;$TUN_NETWORK_PREFIX.1/24&amp;quot; dev &amp;quot;$TUN_NETWORK_DEV&amp;quot;
# 启动虚拟网卡
ip link set &amp;quot;$TUN_NETWORK_DEV&amp;quot; up
ip route del default via &amp;quot;$GATEWAY_IP&amp;quot;
ip route add &amp;quot;$SOCKS_SERVER&amp;quot; via &amp;quot;$GATEWAY_IP&amp;quot;
# 特殊ip段走家用网关（路由器）的 IP 地址（如局域网联机）
# ip route add &amp;quot;172.16.39.0/24&amp;quot; via &amp;quot;$GATEWAY_IP&amp;quot;
# 国内网段走家用网关（路由器）的 IP 地址
for i in $(cat /home/yang/bin/路由表/cn_rules.conf)
do
ip route add &amp;quot;$i&amp;quot; via &amp;quot;$GATEWAY_IP&amp;quot;
done
# 将默认网关设为虚拟网卡的IP地址
ip route add 0.0.0.0/1 via &amp;quot;$TUN_NETWORK_PREFIX.1&amp;quot;
ip route add 128.0.0.0/1 via &amp;quot;$TUN_NETWORK_PREFIX.1&amp;quot;
# 将socks5转为vpn
badvpn-tun2socks --tundev &amp;quot;$TUN_NETWORK_DEV&amp;quot; --netif-ipaddr &amp;quot;$TUN_NETWORK_PREFIX.2&amp;quot; --netif-netmask 255.255.255.0 --socks-server-addr &amp;quot;127.0.0.1:$SOCKS_PORT&amp;quot;
TUN2SOCKS_PID=&amp;quot;$!&amp;quot;
}


stop_fwd() {
ip route del 128.0.0.0/1 via &amp;quot;$TUN_NETWORK_PREFIX.1&amp;quot;
ip route del 0.0.0.0/1 via &amp;quot;$TUN_NETWORK_PREFIX.1&amp;quot;
for i in $(cat /home/yang/bin/路由表/cn_rules.conf)
do
ip route del &amp;quot;$i&amp;quot; via &amp;quot;$GATEWAY_IP&amp;quot;
done
ip route del &amp;quot;172.16.39.0/24&amp;quot; via &amp;quot;$GATEWAY_IP&amp;quot;
ip route del &amp;quot;$SOCKS_SERVER&amp;quot; via &amp;quot;$GATEWAY_IP&amp;quot;
ip route add default via &amp;quot;$GATEWAY_IP&amp;quot;
ip link set &amp;quot;$TUN_NETWORK_DEV&amp;quot; down
ip addr del &amp;quot;$TUN_NETWORK_PREFIX.1/24&amp;quot; dev &amp;quot;$TUN_NETWORK_DEV&amp;quot;
ip tuntap del dev &amp;quot;$TUN_NETWORK_DEV&amp;quot; mode tun
}



start_fwd
trap stop_fwd INT TERM
wait &amp;quot;$TUN2SOCKS_PID&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vim /etc/systemd/system/socksfwd.service
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[Unit]

Description=Transparent SOCKS5 forwarding

After=network-online.target

[Service]

Type=simple

ExecStart=/usr/local/bin/socksfwd

LimitNOFILE=1048576


[Install]

WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl start socksfwd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;开机自启&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl enable socksfwd
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-7-打开流量转发&#34;&gt;3.7 打开流量转发&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat /etc/sysctl.d/30-ipforward.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;net.ipv4.ip_forward=1

net.ipv6.conf.all.forwarding = 1

net.ipv4.tcp_congestion_control=westwood

net.ipv4.tcp_syn_retries = 5

net.ipv4.tcp_synack_retries = 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑完成后，执行以下命令使变动立即生效&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sysctl -p
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>为 Envoy 启用证书验证</title>
      <link>https://www.yangcs.net/posts/setting-up-ssl-in-envoy/</link>
      <pubDate>Tue, 03 Jul 2018 06:43:33 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/setting-up-ssl-in-envoy/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;如果你准备将服务暴露在互联网上，最好启用 &lt;code&gt;SSL/TLS&lt;/code&gt; 加密协议。当使用 Envoy 作为前端代理或者服务网格代理时，可以通过 SSL/TLS 协议来加密客户端和代理之间的所有通信流量。&lt;/p&gt;

&lt;p&gt;Envoy 同时支持监听器中的 &lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/latest/api-v1/listeners/listeners.html#config-listener-ssl-context&#34; target=&#34;_blank&#34;&gt;TLS 终止&lt;/a&gt; 和与上游集群建立连接时的 &lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/latest/api-v1/cluster_manager/cluster_ssl#config-cluster-manager-cluster-ssl&#34; target=&#34;_blank&#34;&gt;TLS 发起&lt;/a&gt;。不管是为现代 web 服务提供标准的边缘代理功能，还是同具有高级 TLS 要求（TLS1.2, SNI, 等等）的外部服务建立连接，Envoy 都提供了充分的支持。&lt;/p&gt;

&lt;p&gt;本文将会演示如何在前端代理中设置 TLS 终止，同时指定访问域名。主要分三个步骤：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;创建 Envoy 需要使用的证书&lt;/li&gt;
&lt;li&gt;为 Envoy 启用证书验证&lt;/li&gt;
&lt;li&gt;配置 Envoy 将 80 端口重定向到 443 端口&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;p-id-h2-1-创建证书-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. 创建证书&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;如果要启用 HTTPS，我们就需要从证书授权机构(以下简称 CA) 处获取一个证书。如果你还没有证书，你可以从 &lt;a href=&#34;https://letsencrypt.org/&#34; target=&#34;_blank&#34;&gt;Let’s Encrypt&lt;/a&gt; 获得网站域名的免费的证书，因为 Let’s Encrypt 就是一个 &lt;code&gt;CA&lt;/code&gt;。本文为了测试使用 &lt;code&gt;OpenSSL&lt;/code&gt; 生成私钥文件 &lt;code&gt;example-com.key&lt;/code&gt; 和 自签名证书 &lt;code&gt;example-com.crt&lt;/code&gt;。&lt;/p&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;需要注意的是 &lt;code&gt;Common Name&lt;/code&gt; 字段，本文测试使用的是 &lt;code&gt;example.com&lt;/code&gt;。&lt;/p&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 继续沿用前文使用的示例
$ cd envoy/examples/front-proxy

# 生成2048位的加密私钥
$ openssl genrsa -out example-com.key 2048

#生成证书签名请求(CSR)
$ openssl req -new -key example-com.key -out example-com.csr

You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter &#39;.&#39;, the field will be left blank.
-----
Country Name (2 letter code) [XX]:CN
State or Province Name (full name) []:CA
Locality Name (eg, city) [Default City]:Shanghai
Organization Name (eg, company) [Default Company Ltd]:Daocloud
Organizational Unit Name (eg, section) []:Envoy Division
Common Name (eg, your name or your server&#39;s hostname) []:example.com
Email Address []:chuansheng.yang@daocloud.io

Please enter the following &#39;extra&#39; attributes
to be sent with your certificate request
A challenge password []:
An optional company name []:

# 生成X509自签名证书
$ openssl x509 -req -days 365 -in example-com.csr -signkey example-com.key -out example-com.crt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-2-为-envoy-启用证书验证-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. 为 Envoy 启用证书验证&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;修改 &lt;code&gt;Dockerfile-frontenvoy&lt;/code&gt; 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Dockerfile&#34;&gt;ADD ./example-com.crt /etc/example-com.crt
ADD ./example-com.key /etc/example-com.key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改 &lt;code&gt;front-envoy.yaml&lt;/code&gt; 配置文件，在 &lt;code&gt;filters&lt;/code&gt; 列表后面添加 &lt;code&gt;tls_context&lt;/code&gt; 配置项：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;tls_context:
  common_tls_context:
    tls_certificates:
      - certificate_chain:
          filename: &amp;quot;/etc/example-com.crt&amp;quot;
        private_key:
          filename: &amp;quot;/etc/example-com.key&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将监听器的监听端口改为标准的 TLS 端口：443。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;  - address:
      socket_address:
        address: 0.0.0.0
        port_value: 443
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还要指定访问的域名，不再使用之前的通配符匹配：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;domains:
- &amp;quot;example.com&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Envoy 可以通过在同一个监听器中配置多个监听器过滤器链来支持多个域名的 &lt;code&gt;SNI&lt;/code&gt;（如 example.com 和 www.example.com），你可以在 &lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/latest/faq/sni&#34; target=&#34;_blank&#34;&gt;Envoy 官方文档&lt;/a&gt; 中看到一个示例。&lt;/p&gt;

&lt;p&gt;最后修改 &lt;code&gt;docker-compose.yaml&lt;/code&gt; 文件，将 443 端口暴露出来，同时将 8080 端口替换为 80 端口。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;services:
  front-envoy:
  ...
    expose:
      - &amp;quot;80&amp;quot;
      - &amp;quot;443&amp;quot;
    ports:
      - &amp;quot;80:80&amp;quot;
      - &amp;quot;443:443&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重启该示例服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-compose down --remove-orphans
$ docker-compose up --build -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面就可以使用 curl 来进行测试了。这里有两个需要注意的地方：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;为了确保 curl 能成功验证证书，必须通过 &lt;code&gt;--cacert&lt;/code&gt; 参数将证书文件传递给 Envoy。&lt;/li&gt;
&lt;li&gt;由于 DNS 无法解析 example.com，所以需要通过参数 &lt;code&gt;--connect-to&lt;/code&gt; 明确指定连接到 localhost，同时在请求的头文件中申明 localhost 的域名为 &lt;code&gt;example.com&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl --cacert example-com.crt --connect-to localhost -H &#39;Host: example.com&#39; https://localhost/service/1

Hello from behind Envoy (service 1)! hostname: 56e8a5bff6bd resolvedhostname: 172.18.0.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果你的 curl 版本不支持 &lt;code&gt;--connect-to&lt;/code&gt; 参数，可以在 &lt;code&gt;/etc/hosts&lt;/code&gt; 中添加一个条目：&lt;code&gt;127.0.0.1    example.com&lt;/code&gt;，然后直接通过域名访问：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl --cacert example-com.crt https://example.com/service/1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-将-80-端口重定向到-443-端口-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 将 80 端口重定向到 443 端口&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;为了将所有 80 端口的流量重定向到 443 端口，可以将 443 端口的路由配置复制一份，然后稍作修改：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;  - address:
      socket_address:
        address: 0.0.0.0
        port_value: 80
    filter_chains:
    - filters:
      - name: envoy.http_connection_manager
        config:
          codec_type: auto
          stat_prefix: ingress_http
          route_config:
            virtual_hosts:
            - name: backend
              domains:
              - &amp;quot;example.com&amp;quot;
              routes:
              - match:
                  prefix: &amp;quot;/&amp;quot;
                redirect:
                  path_redirect: &amp;quot;/&amp;quot;
                  https_redirect: true
          http_filters:
          - name: envoy.router
            config: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重启服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-compose down --remove-orphans
$ docker-compose up --build -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再次通过 &lt;code&gt;HTTP&lt;/code&gt; 协议访问 service1，将会返回 &lt;code&gt;301&lt;/code&gt; 状态码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl -I -H &#39;Host: example.com&#39; http://localhost/service/1

HTTP/1.1 301 Moved Permanently
location: https://example.com/
date: Tue, 03 Jul 2018 06:32:13 GMT
server: envoy
content-length: 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK，大功告成！完整的 front-proxy.yaml 配置文件内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;static_resources:
  listeners:
  - address:
      socket_address:
        address: 0.0.0.0
        port_value: 80
    filter_chains:
    - filters:
      - name: envoy.http_connection_manager
        config:
          codec_type: auto
          stat_prefix: ingress_http
          route_config:
            virtual_hosts:
            - name: backend
              domains:
              - &amp;quot;example.com&amp;quot;
              routes:
              - match:
                  prefix: &amp;quot;/&amp;quot;
                redirect:
                  path_redirect: &amp;quot;/&amp;quot;
                  https_redirect: true
          http_filters:
          - name: envoy.router
            config: {}
  - address:
      socket_address:
        address: 0.0.0.0
        port_value: 443
    filter_chains:
    - filters:
      - name: envoy.http_connection_manager
        config:
          codec_type: auto
          stat_prefix: ingress_http
          route_config:
            name: local_route
            virtual_hosts:
            - name: backend
              domains:
              - &amp;quot;example.com&amp;quot;
              routes:
              - match:
                  prefix: &amp;quot;/service/1&amp;quot;
                route:
                  cluster: service1
              - match:
                  prefix: &amp;quot;/service/2&amp;quot;
                route:
                  cluster: service2
          http_filters:
          - name: envoy.router
            config: {}
      tls_context:
        common_tls_context:
          tls_certificates:
            - certificate_chain:
                filename: &amp;quot;/etc/example-com.crt&amp;quot;
              private_key:
                filename: &amp;quot;/etc/example-com.key&amp;quot;
  clusters:
  - name: service1
    connect_timeout: 0.25s
    type: strict_dns
    lb_policy: round_robin
    http2_protocol_options: {}
    hosts:
    - socket_address:
        address: service1
        port_value: 80
  - name: service2
    connect_timeout: 0.25s
    type: strict_dns
    lb_policy: round_robin
    http2_protocol_options: {}
    hosts:
    - socket_address:
        address: service2
        port_value: 80
admin:
  access_log_path: &amp;quot;/dev/null&amp;quot;
  address:
    socket_address:
      address: 0.0.0.0
      port_value: 8001
&lt;/code&gt;&lt;/pre&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>通过 Envoy 实现增量部署</title>
      <link>https://www.yangcs.net/posts/incremental-deploys/</link>
      <pubDate>Mon, 02 Jul 2018 05:37:37 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/incremental-deploys/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;微服务最常见的工作流程之一就是版本更新。不同于基础架构更新，通过流量管理可以优雅地实现微服务的版本更新。当新发布的版本有缺陷时，这种方法就可以避免版本缺陷对用户造成的不良影响。&lt;/p&gt;

&lt;p&gt;本文将继续沿用前文使用的示例，在原有配置文件的基础上新增了个别服务的新版本来演示流量是如何切换的（包括基于请求头的路由和加权负载均衡）。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-基于请求头的路由-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. 基于请求头的路由&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;为了说明基于请求头的路由对微服务产生的影响，首先创建一个新版本的 &lt;code&gt;service1&lt;/code&gt; 。这里仍然使用 Envoy 仓库中的 &lt;a href=&#34;https://github.com/envoyproxy/envoy/tree/master/examples/front-proxy&#34; target=&#34;_blank&#34;&gt;front-proxy&lt;/a&gt; 示例，修改 &lt;a href=&#34;https://github.com/envoyproxy/envoy/blob/master/examples/front-proxy/docker-compose.yml&#34; target=&#34;_blank&#34;&gt;docker-compose.yml&lt;/a&gt; 文件，添加一个名为 &lt;code&gt;service1a&lt;/code&gt; 的新服务。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;  service1a:
    build:
      context: .
      dockerfile: Dockerfile-service
    volumes:
      - ./service-envoy.yaml:/etc/service-envoy.yaml
    networks:
      envoymesh:
        aliases:
          - service1a
    environment:
      - SERVICE_NAME=1a
    expose:
      - &amp;quot;80&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了确保 Envoy 可以发现该服务，需要将该服务添加到 &lt;code&gt;clusters&lt;/code&gt; 配置项中。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;  - name: service1a
    connect_timeout: 0.25s
    type: strict_dns
    lb_policy: round_robin
    http2_protocol_options: {}
    hosts:
    - socket_address:
        address: service1a
        port_value: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了使新加的服务路由可达，需要在 &lt;code&gt;match&lt;/code&gt; 配置项中添加一个带有 &lt;code&gt;headers&lt;/code&gt; 字段的新路由。因为路由规则列表是按顺序匹配的，所以我们需要将该规则添加到路由规则列表的顶部，这样与新规则匹配的包含该头文件的请求就会被转发到新服务，而不包含该头文件的请求仍然被转发到 service1。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;routes:
- match:
    prefix: &amp;quot;/service/1&amp;quot;
    headers:
      - name: &amp;quot;x-canary-version&amp;quot;
        value: &amp;quot;service1a&amp;quot;
  route:
    cluster: service1a
- match:
    prefix: &amp;quot;/service/1&amp;quot;
  route:
    cluster: service1
- match:
    prefix: &amp;quot;/service/2&amp;quot;
  route:
    cluster: service2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后重启该示例服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-compose down --remove-orphans
$ docker-compose up --build -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果客户端发出的请求没有携带头文件，就会收到来自 &lt;code&gt;service1&lt;/code&gt; 的响应：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl localhost:8000/service/1

Hello from behind Envoy (service 1)! hostname: d0adee810fc4 resolvedhostname: 172.18.0.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果请求携带了头文件 &lt;code&gt;x-canary-version&lt;/code&gt;，Envoy 就会将请求转发到 service 1a。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl -H &#39;x-canary-version: service1a&#39; localhost:8000/service/1

Hello from behind Envoy (service 1a)! hostname: 569ee89eebc8 resolvedhostname: 172.18.0.6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Envoy 基于头文件的路由功能解锁了&lt;a href=&#34;https://opensource.com/article/17/8/testing-production&#34; target=&#34;_blank&#34;&gt;在生产环境中测试开发代码&lt;/a&gt;的能力。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-2-加权负载均衡-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. 加权负载均衡&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;接下来进一步修改配置来实现对 service1 新版本的增量发布，使用 &lt;code&gt;clusters&lt;/code&gt; 数组替代原来的 &lt;code&gt;cluster&lt;/code&gt; 键值对，从而实现将 25% 的流量转发到该服务的新版本上。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;- match:
    prefix: &amp;quot;/service/1&amp;quot;
  route:
    weighted_clusters:
      clusters:
      - name: service1a
        weight: 25
      - name: service1
        weight: 75
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后重启该示例服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-compose down --remove-orphans
$ docker-compose up --build -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时如果客户端发出的请求没有携带头文件，就会有 25% 的流量转发到 service 1a。&lt;/p&gt;

&lt;p&gt;增量部署是个非常强大的功能，它还可以和监控配合使用，以确保服务的版本差异（或者后端服务的架构差异）不会对该服务的版本更新产生不良影响。如果想模拟新版本的成功发布，可以将 service1a 的权重设置为 &lt;code&gt;100&lt;/code&gt;，然后所有的流量都会被转发到 service 1a。同样，如果新发布的版本有缺陷，你可以通过将 service1a 的权重设置为 &lt;code&gt;0&lt;/code&gt; 来回滚到之前的版本。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-最佳实践-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 最佳实践&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;学会了如何配置基于请求头的路由和加权负载均衡之后，就可以在生产或测试环境中进行实践了。首先需要将&lt;strong&gt;部署&lt;/strong&gt;和&lt;strong&gt;发布&lt;/strong&gt;这两个流程分离，部署了新版本之后，你就可以通过配置基于请求头的路由来让你的团队在内部进行测试，同时又不影响用户的使用。一旦测试通过，就可以通过滚动发布模式（逐步增加权重，如 1%，5%，10%，50% &amp;hellip;）来优雅地发布新版本。&lt;/p&gt;

&lt;p&gt;通过将&lt;strong&gt;部署&lt;/strong&gt;和&lt;strong&gt;发布&lt;/strong&gt;这两个流程分离，使用基于请求头的路由在新版本发布之前进行测试，然后通过滚动部署模式来增量发布，你的团队将会从中受益匪浅。&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>HTTP 路由解析</title>
      <link>https://www.yangcs.net/posts/routing-basics/</link>
      <pubDate>Fri, 29 Jun 2018 09:57:33 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/routing-basics/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;本文将更详细地讨论 Envoy 的 HTTP 路由，如果你已经看过了我的上篇文章：&lt;a href=&#34;https://www.yangcs.net/posts/run-envoy-on-your-laptop/&#34; target=&#34;_blank&#34;&gt;在你的笔记本上运行 Envoy&lt;/a&gt;，现在就可以更深入地了解如何在静态文件中配置路由（Route）、集群（Cluster）和监听器（Listener）了。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-相关组件-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. 相关组件&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;路由&#34;&gt;路由&lt;/h3&gt;

&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;路由&lt;/span&gt; 是一组将虚拟主机与集群相匹配的规则，通过路由你可以很轻松地创建流量切换规则。路由的定义方式有两种：通过静态配置文件定义或通过路由发现服务（&lt;code&gt;RDS&lt;/code&gt;）进行配置。&lt;/p&gt;

&lt;h3 id=&#34;集群&#34;&gt;集群&lt;/h3&gt;

&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;集群&lt;/span&gt; 是一组逻辑上相似的上游主机，它接收来自 Envoy 的流量。集群可以通过负载均衡策略来提高基础架构的弹性。集群可以通过静态文件进行配置，也可以通过集群发现服务（&lt;code&gt;CDS&lt;/code&gt;）API 动态获取。&lt;/p&gt;

&lt;h3 id=&#34;监听器&#34;&gt;监听器&lt;/h3&gt;

&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;监听器&lt;/span&gt; 是可以接受来自下游客户端的连接的命名网络位置（例如，端口，unix域套接字等）。Envoy 公开一个或多个下游主机连接的侦听器。同样，监听器可以通过静态定义，也可以通过监听器发现服务（LDS）动态获取。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-2-配置路由-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. 配置路由&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;Envoy 的路由定义将 &lt;code&gt;域 + URL&lt;/code&gt; 映射到集群。在上一篇文章中，我们定义了两个集群（service1 和 service2），每一个集群都匹配一个单独的 URL（&lt;code&gt;/service1&lt;/code&gt; 和 &lt;code&gt;/service2&lt;/code&gt;）。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;virtual_hosts:
  - name: backend
    domains:
    - &amp;quot;*&amp;quot;
    routes:
    - match:
        prefix: &amp;quot;/service/1&amp;quot;
      route:
        cluster: service1
    - match:
        prefix: &amp;quot;/service/2&amp;quot;
      route:
        cluster: service2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;集群从 DNS 中获取集群成员数据，并对集群中的所有主机使用&lt;strong&gt;轮询&lt;/strong&gt;的方式进行负载均衡。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;clusters:
  - name: service1
      connect_timeout: 0.25s
      type: strict_dns
      lb_policy: round_robin
      http2_protocol_options: {}
      hosts:
      - socket_address:
          address: service1
          port_value: 80
  - name: service2
      connect_timeout: 0.25s
      type: strict_dns
      lb_policy: round_robin
      http2_protocol_options: {}
      hosts:
      - socket_address:
          address: service2
          port_value: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-配置监听器-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 配置监听器&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;路由的配置包含在监听器的配置中，现在我们再回过头来看一下监听器的配置。监听器通过&lt;strong&gt;监听器过滤器&lt;/strong&gt;（Listener filter）来操作路由配置中定义的两个服务。监听器的 API 非常简单，它的作用是在不更改 Envoy 的核心功能的情况下添加更多的集成功能。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;listeners:
  - address:
      socket_address:
        address: 0.0.0.0
        port_value: 80
    filter_chains:
    - filters:
      - name: envoy.http_connection_manager
        config:
          codec_type: auto
          stat_prefix: ingress_http
          route_config:
            name: local_route
            virtual_hosts:
            - name: backend
              domains:
              - &amp;quot;*&amp;quot;
              routes:
              - match:
                  prefix: &amp;quot;/service/1&amp;quot;
                route:
                  cluster: service1
              - match:
                  prefix: &amp;quot;/service/2&amp;quot;
                route:
                  cluster: service2
          http_filters:
          - name: envoy.router
            config: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-4-动态发现路由-集群和监听器-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. 动态发现路由、集群和监听器&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;到目前为止我们都是通过静态配置文件来配置路由和集群，但你也可以通过 &lt;code&gt;RDS&lt;/code&gt; 和 &lt;code&gt;CDS&lt;/code&gt; 来动态更新路由和集群。特别是当你的基础架构规模非常大时，你可以通过配置动态服务发现的规则来简化你的重复配置成本，并且可以将同一套动态服务发现规则应用于多个 Envoy 集群。&lt;/p&gt;

&lt;p&gt;现在你已经了解了如何配置基本的路由、集群和监听器，下一节我们将学习如何在增量部署中设置更复杂的流量切换和过滤规则。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-5-参考-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;5. 参考&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://servicemesher.github.io/envoy/&#34; target=&#34;_blank&#34;&gt;Envoy 官方文档中文版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>在你的笔记本上运行 Envoy</title>
      <link>https://www.yangcs.net/posts/run-envoy-on-your-laptop/</link>
      <pubDate>Thu, 28 Jun 2018 08:54:18 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/run-envoy-on-your-laptop/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-前言-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. 前言&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;过去一年中，Kubernetes 已经赢得了容器编排大战，如果说 2017 年是 Kubernetes 的元年，那么 2018 将会是 &lt;code&gt;Service Mesh&lt;/code&gt;（服务网格） 的元年，在未来两年中，Service Mesh 将迎来爆发式增长，成为下一代的微服务架构。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://istio.io/&#34; target=&#34;_blank&#34;&gt;Istio&lt;/a&gt; 作为 Service Mesh 新秀，初出茅庐便声势浩荡，前有 Google，IBM 和 Lyft 倾情奉献，后有业界大佬俯首膜拜。作为一名&lt;strong&gt;斜杠青年&lt;/strong&gt;，如果再不停下脚步认真审视一下这位后起之秀，未免显得太不符合潮流了。&lt;/p&gt;

&lt;p&gt;Istio 这个大家庭的家庭成员很多，为了能够顺利打入 Istio 内部，我们先从它的核心家庭成员 &lt;code&gt;Envoy&lt;/code&gt; 入手。&lt;/p&gt;

&lt;p&gt;从今天起，我将带领大家从零开始学习和使用 Envoy，着重于经验分享和总结，同时也会有相关的概念解析，希望能够帮助大家少走弯路，能不采坑尽量不采坑。&lt;/p&gt;

&lt;p&gt;本篇是 Envoy 系列教程的第一篇，介绍如何在笔记本电脑上运行 Envoy、测试代理配置并观察结果，让我们开始吧！&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-2-前提-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. 前提&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;你可以选择从源代码构建 Envoy，但最简单的办法是通过 &lt;code&gt;Docker&lt;/code&gt; 容器来运行。所以在开始之前，你需要安装并配置以下工具：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/install/&#34; target=&#34;_blank&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/compose/install/&#34; target=&#34;_blank&#34;&gt;Docker Compose&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://help.github.com/articles/set-up-git/&#34; target=&#34;_blank&#34;&gt;Git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://curl.haxx.se/&#34; target=&#34;_blank&#34;&gt;curl&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们使用 Docker 和 Docker Compose 来编排和运行 Envoy 的示例服务，使用 curl 来访问 Envoy 示例服务。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-部署-envoy-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 部署 Envoy&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/envoyproxy/envoy&#34; target=&#34;_blank&#34;&gt;Envoy 官方&lt;/a&gt;提供了一组 Envoy 的用例，我们将要使用的用例是前端代理，它会将流量发送到两个服务后端。首先克隆 Envoy 的代码仓库并转到 &lt;code&gt;examples/front-proxy&lt;/code&gt; 目录：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/envoyproxy/envoy
$ cd envoy/examples/front-proxy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;后端服务&lt;/span&gt; 是一个非常简单的 &lt;code&gt;Flask&lt;/code&gt; 应用程序，在 &lt;code&gt;service.py&lt;/code&gt; 中定义。其中 Envoy 作为一个边车（Sidecar）伴随每个服务一起运行在同一个容器中，所有的规则配置都通过 YAML 文件 &lt;code&gt;service-envoy.yaml&lt;/code&gt; 来完成。最后 &lt;code&gt;Dockerfile-service&lt;/code&gt; 创建一个在启动时同时运行服务和 Envoy 的容器。&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;前端代理&lt;/span&gt; 比后端服务更简单，它使用配置文件 &lt;code&gt;front-envoy.yaml&lt;/code&gt; 来运行 Envoy，使用 &lt;code&gt;Dockerfile-frontenvoy&lt;/code&gt; 来构建容器镜像。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker-compose.yaml&lt;/code&gt; 文件描述了如何构建、打包和运行前端代理与服务。&lt;/p&gt;

&lt;p&gt;整体架构如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/envoyproxy-docker-compose.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;使用 docker-compose 启动容器：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-compose up --build -d
$ docker-compose ps

          Name                        Command               State                      Ports
----------------------------------------------------------------------------------------------------------------
frontproxy_front-envoy_1   /bin/sh -c /usr/local/bin/ ...   Up      0.0.0.0:8000-&amp;gt;80/tcp, 0.0.0.0:8001-&amp;gt;8001/tcp
frontproxy_service1_1      /bin/sh -c /usr/local/bin/ ...   Up      80/tcp
frontproxy_service2_1      /bin/sh -c /usr/local/bin/ ...   Up      80/tcp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该命令将会启动一个前端代理和两个服务实例：service1 和 service2。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-配置-envoy-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 配置 Envoy&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;为了达到演示的目的，本文采用的是 Envoy 的静态配置。后续教程将会告诉你们如何使用动态配置来发挥 Envoy 的强大功能。&lt;/p&gt;

&lt;p&gt;为了了解 Envoy 是如何配置的，先来看看 &lt;code&gt;docker-compose.yaml&lt;/code&gt; 文件的前端代理部分的配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;  front-envoy:
    build:
      context: ../
      dockerfile: front-proxy/Dockerfile-frontenvoy
    volumes:
      - ./front-envoy.yaml:/etc/front-envoy.yaml
    networks:
      - envoymesh
    expose:
      - &amp;quot;80&amp;quot;
      - &amp;quot;8001&amp;quot;
    ports:
      - &amp;quot;8000:80&amp;quot;
      - &amp;quot;8001:8001&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上到下做了这么几件事：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;使用位于当前目录中的 &lt;code&gt;Dockerfile-frontenvoy&lt;/code&gt; 构建镜像。&lt;/li&gt;
&lt;li&gt;将 &lt;code&gt;front-envoy.yaml&lt;/code&gt; 文件作为 &lt;code&gt;/etc/front-envoy.yaml&lt;/code&gt; 挂载到容器中的 &lt;code&gt;/etc&lt;/code&gt; 目录。&lt;/li&gt;
&lt;li&gt;为这个容器创建并使用名为 &lt;code&gt;envoymesh&lt;/code&gt; 的 Docker 网络。&lt;/li&gt;
&lt;li&gt;暴露 80 端口（用于一般通用流量）和 8001 端口（用于管理服务）。&lt;/li&gt;
&lt;li&gt;将主机的 8000 端口和 8001 端口分别映射到容器的 80 端口和 8001 端口。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;前面已经了解到了前端代理使用 front-envoy.yaml 来配置 Envoy，下面来深入解析一下。该配置文件有两大配置项：&lt;code&gt;static_resources&lt;/code&gt; 和 &lt;code&gt;admin&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;static_resources:
admin:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;admin 配置项的内容非常简单：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;admin:
  access_log_path: &amp;quot;/dev/null&amp;quot;
  address:
    socket_address:
      address: 0.0.0.0
      port_value: 8001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;access_log_path&lt;/code&gt; 字段的值设置为 &lt;code&gt;/dev/null&lt;/code&gt;，意味着 admin 服务的访问日志将会被丢弃，在测试或生产环境中，你最好将这个值修改为不同的目录。socket_address 字段告诉 Envoy 创建一个监听在 8001 端口的 admin 服务。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;static_resources&lt;/code&gt; 配置项定义了一组静态配置的集群（Cluster）和侦听器（Listener）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;集群&lt;/strong&gt;是 Envoy 连接到的一组逻辑上相似的上游主机。Envoy 通过服务发现发现集群中的成员。Envoy 可以通过主动运行状况检查来确定集群成员的健康状况。Envoy 如何将请求路由到集群成员由负载均衡策略确定。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;侦听器&lt;/strong&gt;是服务(程序)监听者，就是真正干活的。 它是可以由下游客户端连接的命名网络位置（例如，端口、unix域套接字等）。&lt;/p&gt;

&lt;h3 id=&#34;listener-配置&#34;&gt;Listener 配置&lt;/h3&gt;

&lt;p&gt;该示例中的前端代理有一个监听在 80 端口的侦听器，并配置了一个&lt;strong&gt;监听器过滤器链&lt;/strong&gt;（filter_chains），用来管理 HTTP 流量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;  listeners:
  - address:
      socket_address:
        address: 0.0.0.0
        port_value: 80
    filter_chains:
    - filters:
      - name: envoy.http_connection_manager
        config:
          codec_type: auto
          stat_prefix: ingress_http
          route_config:
            name: local_route
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 HTTP 连接管理过滤器中，每一个虚拟主机都有单独的配置，并且都配置为接收所有域的流量：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;            virtual_hosts:
            - name: backend
              domains:
              - &amp;quot;*&amp;quot;
              routes:
              - match:
                  prefix: &amp;quot;/service/1&amp;quot;
                route:
                  cluster: service1
              - match:
                  prefix: &amp;quot;/service/2&amp;quot; 
                route:
                  cluster: service2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HTTP 路由规则将 &lt;code&gt;/service/1&lt;/code&gt; 和 &lt;code&gt;/service/1&lt;/code&gt; 的流量转发到各自的 Cluster。&lt;/p&gt;

&lt;h3 id=&#34;cluster-配置&#34;&gt;Cluster 配置&lt;/h3&gt;

&lt;p&gt;接下来看一下静态 Cluster 的定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;  clusters:
  - name: service1
    connect_timeout: 0.25s
    type: strict_dns
    lb_policy: round_robin
    http2_protocol_options: {}
    hosts:
    - socket_address:
        address: service1
        port_value: 80
  - name: service2
    connect_timeout: 0.25s
    type: strict_dns
    lb_policy: round_robin
    http2_protocol_options: {}
    hosts:
    - socket_address:
        address: service2
        port_value: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 Cluster 的配置中，你可以自定义超时、断路器和服务发现等。Cluster 由 Endpoint（端点）组成，其中 Endpoint 是一组可以为 Cluster 的请求提供服务的网络位置。本例中的 Endpoint 是通过 DNS 域名的方式定义的，Envoy 可以从域名中读取 Endpoint。Endpoint 也可以直接定义为 socket 地址，或者通过 &lt;code&gt;EDS&lt;/code&gt;（Endpoint Discovery Service）动态读取。&lt;/p&gt;

&lt;h3 id=&#34;修改配置&#34;&gt;修改配置&lt;/h3&gt;

&lt;p&gt;你可以通过修改配置文件重新构建镜像来进行测试。&lt;code&gt;Listener filter&lt;/code&gt;（监听器过滤器）的作用是在不更改 Envoy 的核心功能的情况下添加更多的集成功能。例如，如果想要将访问日志添加到 HTTP 过滤器中，可以在 filter 的配置中添加 &lt;code&gt;access_log&lt;/code&gt; 配置项：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;    - filters:
      - name: envoy.http_connection_manager
        config:
          codec_type: auto
          stat_prefix: ingress_http
          access_log:
            - name: envoy.file_access_log
              config:
                path: &amp;quot;/var/log/access.log&amp;quot;
          route_config:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后停止服务，重新构建并运行容器：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-compose down
$ docker-compose up --build -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过 &lt;code&gt;curl&lt;/code&gt; 访问服务，然后通过 &lt;code&gt;docker-compose exec front-envoy /bin/bash&lt;/code&gt; 命令进入容器的终端，你会看到 &lt;code&gt;/var/log/access.log&lt;/code&gt; 文件记录着你的请求结果。&lt;/p&gt;

&lt;h3 id=&#34;admin-server&#34;&gt;Admin Server&lt;/h3&gt;

&lt;p&gt;Envoy 的一大特色是内置的 Admin 服务，如果你在浏览器中访问 &lt;code&gt;http://localhost:8001&lt;/code&gt; ，可以看到 Envoy admin 提供以下管理 API 端点。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;命令&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;/&lt;/td&gt;
&lt;td&gt;Admin 主页&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/certs&lt;/td&gt;
&lt;td&gt;打印机器上的 certs&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/clusters&lt;/td&gt;
&lt;td&gt;upstream cluster 状态&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/config_dump&lt;/td&gt;
&lt;td&gt;输出当前的 Envoy 配置&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/cpuprofiler&lt;/td&gt;
&lt;td&gt;开启/关闭 CPU profiler&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/healthcheck/fail&lt;/td&gt;
&lt;td&gt;导致服务失败健康检查&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/healthcheck/ok&lt;/td&gt;
&lt;td&gt;导致服务通过健康检查&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/help&lt;/td&gt;
&lt;td&gt;打印管理命令的帮助信息&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/hot_restart_version&lt;/td&gt;
&lt;td&gt;打印热重启兼容版本&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/listeners&lt;/td&gt;
&lt;td&gt;打印 listener 地址&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/logging&lt;/td&gt;
&lt;td&gt;查询/更改日志级别&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/quitquitquit&lt;/td&gt;
&lt;td&gt;退出服务&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/reset_counters&lt;/td&gt;
&lt;td&gt;将计数器重置为 1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/runtime&lt;/td&gt;
&lt;td&gt;打印运行时值&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/runtime_modify&lt;/td&gt;
&lt;td&gt;修改运行时值&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/server_info&lt;/td&gt;
&lt;td&gt;打印服务器版本/状态信息&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/stats&lt;/td&gt;
&lt;td&gt;打印服务器状态统计信息&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;/stats/prometheus&lt;/td&gt;
&lt;td&gt;打印 prometheus 格式的服务器状态统计信息&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;通过 API 管理端可以对 Envoy 进行动态配置，参考 &lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/latest/api-v2/api&#34; target=&#34;_blank&#34;&gt;v2 API reference&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-4-进一步探索-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. 进一步探索&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;如果你有兴趣探索 Envoy 的更多其他功能，&lt;a href=&#34;https://github.com/envoyproxy/envoy/tree/master/examples&#34; target=&#34;_blank&#34;&gt;Envoy 官方示例&lt;/a&gt;还有一些更复杂的拓扑结构，但这些示例仍然使用静态类型的服务发现。如果你还想了解有关如何在生产环境中使用 Envoy 的更多信息，请参阅 &lt;a href=&#34;https://www.learnenvoy.io/articles/service-discovery.html&#34; target=&#34;_blank&#34;&gt;Integrating Service Discovery with Envoy&lt;/a&gt; 以了解将 Envoy 与现有环境集成的意义。如果你在测试 Envoy 的过程中遇到问题，请访问 &lt;a href=&#34;https://www.learnenvoy.io/articles/getting-help.html&#34; target=&#34;_blank&#34;&gt;Getting Help&lt;/a&gt; 页面以获取更多的帮助信息。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-5-参考-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;5. 参考&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/posts/envoy-archiecture-and-terminology/&#34; target=&#34;_blank&#34;&gt;Envoy 的架构与基本术语&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/posts/envoy-as-front-proxy/&#34; target=&#34;_blank&#34;&gt;使用 Envoy 作为前端代理&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>使用 envoy-docker-shim 替代 docker-proxy</title>
      <link>https://www.yangcs.net/posts/envoy-docker-shim/</link>
      <pubDate>Fri, 22 Jun 2018 08:22:07 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/envoy-docker-shim/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;在过去一年中，服务网格技术的崛起引发了吃瓜群众对 Istio 的持续关注，而 Istio 的核心组件 &lt;a href=&#34;https://github.com/envoyproxy/envoy&#34; target=&#34;_blank&#34;&gt;Envoy&lt;/a&gt; 是一款由 Lyft 开源的，使用 C++ 编写的 L7 代理和通信总线，目前是 &lt;a href=&#34;https://cncf.io/&#34; target=&#34;_blank&#34;&gt;CNCF&lt;/a&gt; 旗下的开源项目，代码托管在 GitHub 上，它也是 Istio service mesh 中默认的 data plane。&lt;/p&gt;

&lt;p&gt;目前网上关于 Envoy 的文档非常少，能够找到的比较权威的资料只有官方文档，但官方文档的痛点是只有理论概念，没有实际应用指南。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/latest/&#34; target=&#34;_blank&#34;&gt;Envoy 官方文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://servicemesher.github.io/envoy/&#34; target=&#34;_blank&#34;&gt;Envoy 官方文档中文版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;要想真正掌握 Envoy，只有通过实践融入该语境才能真正理解这门技术，而目前能够找到的最佳实践项目就是 &lt;a href=&#34;https://github.com/Nitro/envoy-docker-shim&#34; target=&#34;_blank&#34;&gt;Envoy Docker Shim&lt;/a&gt;。在实践该项目之前，你需要了解 Envoy 中的基本术语和概念，可以参考 Jimmy Song 的文章：&lt;a href=&#34;https://jimmysong.io/posts/envoy-archiecture-and-terminology/&#34; target=&#34;_blank&#34;&gt;Envoy 的架构与基本术语&lt;/a&gt;。下面我就为大家简单地介绍下这个项目。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-envoy-docker-shim-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. Envoy Docker Shim&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;code&gt;Envoy Docker Shim&lt;/code&gt; 是一个预生产项目，它使用 Envoy 来替代 Docker 的 &lt;code&gt;docker-proxy&lt;/code&gt;，这样做的目的是为在 Docker 上运行的服务启用 Envoy 的指标收集和分布式路由跟踪功能。使用了 Envoy Docker Shim，就相当于获得了服务网格的一半功能。该项目由以下四个组件组成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;envoy-docker-server&lt;/code&gt; : 运行在 Docker 主机上的注册中心，用来向 Envoy 提供 服务发现 API。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;envoy-docker-shim&lt;/code&gt; : 命令行应用程序，采用与 &lt;code&gt;docker-proxy&lt;/code&gt; 相同的命令参数，但它的作用是将新的端点（endpoint）注册到注册中心。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Envoy 实例&lt;/code&gt; : 以 host 网络模式运行在 Docker 容器中。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;resync&lt;/code&gt; : shell 脚本，当容器重启时用来恢复注册中心的状态。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过将这些组件结合在一起，就形成了一个通过 Envoy 来代理 HTTP 和 TCP 流量的系统，对 &lt;code&gt;UDP&lt;/code&gt; 流量的处理继续使用 docker-proxy 的代码逻辑，目前暂不支持 &lt;code&gt;SCTP&lt;/code&gt; 协议。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-2-安装步骤-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. 安装步骤&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;要想使用 Envoy Docker Shim，首先需要修改 &lt;code&gt;dockerd&lt;/code&gt; 的启动参数：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--userland-proxy-path=/path/to/envoy-docker-shim&lt;/code&gt; : 使用 envoy-docker-shim 替换 &lt;code&gt;docker-proxy&lt;/code&gt;，用来告诉 Envoy 需要监听的服务以及如何转发。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--iptables=false&lt;/code&gt; : 禁用 Iptables 来转发 Docker 的流量，强制使用 Envoy 来转发 Docker 流量。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一旦设置了 &amp;ndash;iptables=false，Docker 流量就不会再通过内核直接流入桥接网络。通过配置不同的代理模式，所有的流量都会在 4 层或 7 层进行代理。&lt;/p&gt;

&lt;p&gt;修改完启动参数后，就可以重启 Docker 服务了：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl daemon-reload
$ systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;下载二进制文件和脚本&#34;&gt;下载二进制文件和脚本&lt;/h3&gt;

&lt;p&gt;安装 &lt;code&gt;envoy-docker-server&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ go get github.com/Nitro/envoy-docker-shim/cmd/envoy-docker-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装 &lt;code&gt;envoy-docker-shim&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ go get github.com/Nitro/envoy-docker-shim/cmd/envoy-docker-shim
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;克隆 envoy-docker-shim 项目：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/Nitro/envoy-docker-shim
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将脚本 &lt;code&gt;resync&lt;/code&gt; 复制到 /usr/local/bin 目录下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd envoy-docker-shim
$ cp scripts/resync /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;配置-envoy-docker-server-和-resync&#34;&gt;配置 envoy-docker-server 和 resync&lt;/h3&gt;

&lt;p&gt;修改 examples 目录下的 &lt;code&gt;envoy-docker-server.service&lt;/code&gt; 文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;[Unit]
Description=Envoy Docker Shim
PartOf=docker.service

[Service]
ExecStart=$GOPATH/bin/envoy-docker-server
ExecStartPost=/usr/local/bin/resync
ExecReload=/usr/local/bin/resync
KillMode=process
Restart=on-failure
Type=simple

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了防止出现状态不同步的问题，envoy-docker-server 仅将状态存储在内存中，一旦重启了 envoy-docker-server，就需要执行脚本 resync 来同步容器的状态。&lt;/p&gt;

&lt;p&gt;将 envoy-docker-server.service 文件复制到 &lt;code&gt;/etc/systemd/system/&lt;/code&gt; 目录下并启动该服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cp examples/envoy-docker-server.service /etc/systemd/system/
$ systemctl start envoy-docker-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;启动-envoy-实例&#34;&gt;启动 Envoy 实例&lt;/h3&gt;

&lt;p&gt;在 examples 目录中包含了一个 &lt;code&gt;envoy.yaml&lt;/code&gt; 文件，你可以通过该配置文件启动 Envoy 实例（在 1.6 和 1.7 版本上测试通过）。你也可以选择通过 Docker 容器来运行 Envoy 实例，这里我们通过容器的方式来启动 Envoy：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d --name envoyproxy --restart always --net host --cap-add NET_ADMIN -e ENVOY_BASE_ID=100 -e ENVOY_PORT=9902 -e ENVOY_UI_LISTEN_PORT=8081 gonitro/envoyproxy:1.7.0-27960f3-tracing
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该 Envoy 容器还提供了一个 UI 来展示指标和路由，可以通过在浏览器中输入 url：&lt;code&gt;http://host_ip:8081&lt;/code&gt; 来打开 UI 界面：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/envoy.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;至此，Envoy Docker Shim 已经完美地完成了替代 docker-proxy 的工作，接下来就可以不通过 iptables 而使用 Envoy 来实现 Docker 容器的端口映射啦！&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-容器配置-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 容器配置&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;一切准备就绪后，就可以启动一个容器试试了。如果想完美地使用 envoy-docker-shim，你需要在启动容器时指定两个或三个标签，虽然这不是必须的，但可以更方便地跟踪流量。这些 Docker 标签被映射成报告给 &lt;code&gt;Zipkin&lt;/code&gt; 或 &lt;code&gt;Jaeger&lt;/code&gt; 的服务的标签。这三个标签是从 &lt;a href=&#34;https://github.com/Nitro/sidecar&#34; target=&#34;_blank&#34;&gt;Sidecar&lt;/a&gt; 项目集成而来的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;EnvironmentName&lt;/code&gt; : 这个标签可以成某个客户的名字，或者表示这是生产项目还是测试项目，反正只要对你来说有意义就行。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ServiceName&lt;/code&gt; : 设置 Envoy 跟踪的服务名。例如，如果你将这个标签设置为 &lt;code&gt;nginx&lt;/code&gt;，将 EnvironmentName 标签设置为 &lt;code&gt;prod&lt;/code&gt;，那么该服务名为 &lt;code&gt;nginx-prod&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ProxyMode&lt;/code&gt; : 设置 Envoy 的代理模式。默认使用 &lt;code&gt;http&lt;/code&gt; 代理模式，如果想使用 tcp 代理模式，需要指定该标签值为 &lt;code&gt;tcp&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run -d -p 80:80 -p 443:443 -l EnvironmentName=proxy -l ServiceName=nginx -l ProxyMode=tcp nginx:alpine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打开 Envoy UI：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/envoy-nginx.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到 nginx 的服务名为 &lt;code&gt;nginx-proxy&lt;/code&gt;。&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>使用自定义指标进行弹性伸缩</title>
      <link>https://www.yangcs.net/posts/custom-metrics-hpa/</link>
      <pubDate>Tue, 19 Jun 2018 09:02:52 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/custom-metrics-hpa/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;从 Kubernetes 1.8 开始，资源使用指标（如容器 CPU 和内存使用率）可以通过 Metrics API 在 Kubernetes 中获取。 这些指标可以直接被用户访问(例如通过使用 kubectl top 命令)，或由集群中的控制器使用(例如，Horizontal Pod Autoscale 可以使用这些指标作出决策)。&lt;/p&gt;

&lt;p&gt;例如，可以使用 &lt;code&gt;kubectl top node&lt;/code&gt; 和 &lt;code&gt;kubectl top pod&lt;/code&gt; 查看资源使用情况：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl top node

NAME              CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%
192.168.123.248   245m         12%       2687Mi          34%
192.168.123.249   442m         22%       3270Mi          42%
192.168.123.250   455m         22%       4014Mi          52%

$ kubectl top pod

NAME                              CPU(cores)   MEMORY(bytes)
details-v1-64b86cd49-52g82        0m           11Mi
podinfo-6b86c8ccc9-5qr8b          0m           7Mi
podinfo-6b86c8ccc9-hlxm7          0m           12Mi
podinfo-6b86c8ccc9-qxhng          0m           6Mi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-resource-metrics-api-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. Resource Metrics API&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;通过 &lt;code&gt;Metrics API&lt;/code&gt;，您可以获取指定 node 或 pod 当前使用的资源量。这个 API 不存储指标值， 因此想要获取某个指定 node 10分钟前的资源使用量是不可能的。&lt;/p&gt;

&lt;p&gt;Metrics API 和其他的 API 没有什么不同，它可以通过与 &lt;code&gt;/apis/metrics.k8s.io/&lt;/code&gt; 路径下的其他 Kubernetes API 相同的端点来发现，并且提供了相同的安全性、可扩展性和可靠性保证，Metrics API 在 &lt;a href=&#34;https://github.com/kubernetes/metrics/blob/master/pkg/apis/metrics/v1beta1/types.go&#34; target=&#34;_blank&#34;&gt;k8s.io/metrics&lt;/a&gt; 仓库中定义，你可以在这里找到关于 Metrics API 的更多信息。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt; Metrics API 需要在集群中部署 Metrics Server。否则它将不可用。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-2-metrics-server-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. Metrics Server&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;code&gt;Metrics Server&lt;/code&gt; 实现了Resource Metrics API。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/metrics-server&#34; target=&#34;_blank&#34;&gt;Metrics Server&lt;/a&gt; 是集群范围资源使用数据的聚合器。 从 Kubernetes 1.8 开始，它作为一个 Deployment 对象默认部署在由 kube-up.sh 脚本创建的集群中。 如果你使用了其他的 Kubernetes 安装方法，您可以使用 Kubernetes 1.7+ (请参阅下面的详细信息) 中引入的 &lt;a href=&#34;https://github.com/kubernetes-incubator/metrics-server/tree/master/deploy&#34; target=&#34;_blank&#34;&gt;deployment yamls&lt;/a&gt; 文件来部署。&lt;/p&gt;

&lt;p&gt;Metrics Server 从每个节点上的 &lt;code&gt;Kubelet&lt;/code&gt; 公开的 Summary API 中采集指标信息。&lt;/p&gt;

&lt;p&gt;通过在主 API server 中注册的 Metrics Server &lt;a href=&#34;https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation/&#34; target=&#34;_blank&#34;&gt;Kubernetes 聚合器&lt;/a&gt; 来采集指标信息， 这是在 Kubernetes 1.7 中引入的。在 &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/metrics-server.md&#34; target=&#34;_blank&#34;&gt;设计文档&lt;/a&gt; 中可以了解到有关 Metrics Server 的更多信息。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-custom-metrics-api-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. custom metrics api&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;该 API 允许消费者访问通过任意指标描述的 Kubernetes 资源。如果你想实现这个 API Service，请参阅 &lt;a href=&#34;https://github.com/kubernetes-incubator/custom-metrics-apiserver&#34; target=&#34;_blank&#34;&gt;kubernetes-incubator/custom-metrics-apiserver&lt;/a&gt;，这是一个用来实现 Kubernetes 自定义指标的框架。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-4-hpa-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. HPA&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;自动伸缩是一种根据资源使用情况自动伸缩工作负载的方法。自动伸缩在 Kubernetes 中有两个维度：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;span id=&#34;inline-blue&#34;&gt;Cluster Autoscaler&lt;/span&gt; : 用来处理节点扩容。&lt;/li&gt;
&lt;li&gt;&lt;span id=&#34;inline-blue&#34;&gt;Horizontal Pod Autoscaler&lt;/span&gt; : 自动缩放 rs 或 rc 中的 pod。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cluster Autoscaler 和 Horizontal Pod Autoscaler 一起可用于动态调整集群的计算能力。虽然 Cluster Autoscaler 高度依赖于托管集群的云服务商提供的底层功能，但是 HPA 可以独立于你的 IaaS/PaaS 提供商进行操作。&lt;/p&gt;

&lt;p&gt;Kubernetes 自 1.2 版本引入 &lt;code&gt;HPA&lt;/code&gt; 机制，到 1.6 版本之前一直是通过 &lt;code&gt;kubelet&lt;/code&gt; 来获取监控指标来判断是否需要扩缩容，1.6 版本之后必须通过 API server、Heapseter 或者 kube-aggregator 来获取监控指标。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Kubernetes 1.7&lt;/code&gt; 引入了聚合层，允许第三方应用程序通过注册为 API 附加组件来扩展 Kubernetes API。 自定义指标 API 以及聚合层使得像 &lt;code&gt;Prometheus&lt;/code&gt; 这样的监控系统可以向 HPA 控制器公开针对特定应用程序的指标。&lt;/p&gt;

&lt;p&gt;hpa 实现了一个控制环，可以周期性的从 Resource Metrics API 查询特定应用的 CPU 和内存信息。&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;https://github.com/stefanprodan/k8s-prom-hpa/raw/master/diagrams/k8s-hpa.png&#34; alt=&#34;&#34; /&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/hpa.png&#34; alt=&#34;&#34; /&gt;&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-5-实战-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;5. 实战&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;以下是一份为 Kubernetes 1.9 或更高版本配置 HPA v2 的分步指南。首先将会安装提供核心指标的 &lt;code&gt;Metrics Server&lt;/code&gt; 附件组件，然后使用一个 demo 来演示基于 CPU 和内存使用的 Pod 的自动伸缩。在指南的第二部分，将会部署 Prometheus 和一个 &lt;code&gt;custom metrics apiserver&lt;/code&gt;。聚合层会自动注册 custom metrics apiserver，然后通过一个 demo 来演示自定义指标的 HPA。&lt;/p&gt;

&lt;h3 id=&#34;前提&#34;&gt;前提&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.yangcs.net/posts/api-aggregation/&#34; target=&#34;_blank&#34;&gt;开启聚合层 API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;go 1.8+&lt;/li&gt;
&lt;li&gt;克隆 &lt;a href=&#34;https://github.com/stefanprodan/k8s-prom-hpa&#34; target=&#34;_blank&#34;&gt;k8s-prom-hpa&lt;/a&gt; 仓库&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd $GOPATH
$ git clone https://github.com/stefanprodan/k8s-prom-hpa
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;安装-metrics-server&#34;&gt;安装 Metrics Server&lt;/h3&gt;

&lt;p&gt;Kubernetes Metrics Server 是一个集群范围内的资源使用量的聚合器，它是 Heapster 的继承者。Metrics Server 通过汇集来自 &lt;code&gt;kubernetes.summary_api&lt;/code&gt; 的数据来收集 node 和 pod 的 CPU 和内存使用情况。&lt;code&gt;summary API&lt;/code&gt; 是用于将数据从 Kubelet/cAdvisor 传递到 Metrics Server 的高效内存 API。&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;https://github.com/stefanprodan/k8s-prom-hpa/raw/master/diagrams/k8s-hpa-ms.png&#34; alt=&#34;&#34; /&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;在安装 Metrics Server 之前需要先进行如下配置：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;将 kube-controller-manager 的启动参数 &lt;code&gt;--horizontal-pod-autoscaler-use-rest-clients&lt;/code&gt; 的值设置为 true。&lt;/li&gt;
&lt;li&gt;在 kube-controller-manager 的启动参数 &amp;ndash;master 设置为 kube-apiserver 的地址，如：&lt;code&gt;--master=http://172.20.0.113:8080&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 &lt;code&gt;kube-system&lt;/code&gt; 命名空间部署 metrics-server：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl create -f ./metrics-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一分钟后，度量服务器开始报告节点和 Pod 的 CPU 和内存使用情况。 查看 nodes 指标：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get --raw &amp;quot;/apis/metrics.k8s.io/v1beta1/nodes&amp;quot; | jq .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 pods 指标：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get --raw &amp;quot;/apis/metrics.k8s.io/v1beta1/pods&amp;quot; | jq .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;基于-cpu-和内存使用的自动缩放&#34;&gt;基于 CPU 和内存使用的自动缩放&lt;/h3&gt;

&lt;p&gt;下面使用一个基于 golang 的小程序来测试 HPA。&lt;/p&gt;

&lt;p&gt;在 &lt;code&gt;default&lt;/code&gt; 命名空间中部署 podinfo：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl create -f ./podinfo/podinfo-svc.yaml,./podinfo/podinfo-dep.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以通过 &lt;code&gt;http://PODINFO_SVC_IP:9898&lt;/code&gt; 来访问 podinfo。&lt;/p&gt;

&lt;p&gt;接下来定义一个保持最少两个副本的 HPA，如果 CPU 平均使用量超过 80％ 或内存超过 200Mi，则最高可扩展到 10 个副本：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: podinfo
spec:
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: podinfo
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      targetAverageUtilization: 80
  - type: Resource
    resource:
      name: memory
      targetAverageValue: 200Mi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 HPA：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl create -f ./podinfo/podinfo-hpa.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;几秒钟之后，HPA 控制器与 metrics server 进行通信，然后获取 CPU 和内存使用情况。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get hpa

NAME      REFERENCE            TARGETS                      MINPODS   MAXPODS   REPLICAS   AGE
podinfo   Deployment/podinfo   2826240 / 200Mi, 15% / 80%   2         10        2          5m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为了提高 CPU 使用率、运行 &lt;code&gt;rakyll/hey&lt;/code&gt; 进行压力测试：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#install hey
$ go get -u github.com/rakyll/hey

#do 10K requests
hey -n 10000 -q 10 -c 5 http://PODINFO_SVC_IP:9898/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你可以通过以下命令获取 HPA event：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl describe hpa podinfo

Events:
  Type    Reason             Age   From                       Message
  ----    ------             ----  ----                       -------
  Normal  SuccessfulRescale  7m    horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  3m    horizontal-pod-autoscaler  New size: 8; reason: cpu resource utilization (percentage of request) above target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先将 podinfo 删除，稍后将会重新部署：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl delete -f ./podinfo/podinfo-hpa.yaml,./podinfo/podinfo-dep.yaml,./podinfo/podinfo-svc.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;安装-custom-metrics-server&#34;&gt;安装 Custom Metrics Server&lt;/h3&gt;

&lt;p&gt;为了让 HPA 可以根据 custom metrics 进行扩展，你需要有两个组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;span id=&#34;inline-blue&#34;&gt;Prometheus&lt;/span&gt; : 从应用程序中收集指标并将其存储为 Prometheus 时间序列数据库。&lt;/li&gt;
&lt;li&gt;&lt;span id=&#34;inline-blue&#34;&gt;custom-metrics-apiserver&lt;/span&gt; : 使用 &lt;a href=&#34;https://github.com/DirectXMan12/k8s-prometheus-adapter&#34; target=&#34;_blank&#34;&gt;k8s-prometheus-adapter&lt;/a&gt; 提供的 metrics 来扩展 Kubernetes 自定义指标 API。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;https://github.com/stefanprodan/k8s-prom-hpa/raw/master/diagrams/k8s-hpa-prom.png&#34; alt=&#34;&#34; /&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;创建 &lt;code&gt;monitoring&lt;/code&gt; 命名空间：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl create -f ./namespaces.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 Prometheus v2 部署到 monitoring 命名空间：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl create -f ./prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生成 Prometheus adapter 所需的 TLS 证书：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ make certs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;部署 custom-metrics-apiserver：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl create -f ./custom-metrics-api
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;列出由 Prometheus 提供的自定义指标：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get --raw &amp;quot;/apis/custom.metrics.k8s.io/v1beta1&amp;quot; | jq .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;获取 monitoring 命名空间中所有 pod 的 &lt;code&gt;FS&lt;/code&gt; 信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get --raw &amp;quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/monitoring/pods/*/fs_usage_bytes&amp;quot; | jq .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;基于自定义指标的自动扩容&#34;&gt;基于自定义指标的自动扩容&lt;/h3&gt;

&lt;p&gt;在 &lt;code&gt;default&lt;/code&gt; 命名空间中部署 podinfo：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl create -f ./podinfo/podinfo-svc.yaml,./podinfo/podinfo-dep.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;podinfo 应用暴露了一个自定义的度量指标：&lt;code&gt;http_requests_total&lt;/code&gt;。Prometheus adapter（即 custom-metrics-apiserver）删除了 &lt;code&gt;_total&lt;/code&gt; 后缀并将该指标标记为 &lt;code&gt;counter metric&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;从自定义指标 API 获取每秒的总请求数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get --raw &amp;quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests&amp;quot; | jq .
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;kind&amp;quot;: &amp;quot;MetricValueList&amp;quot;,
  &amp;quot;apiVersion&amp;quot;: &amp;quot;custom.metrics.k8s.io/v1beta1&amp;quot;,
  &amp;quot;metadata&amp;quot;: {
    &amp;quot;selfLink&amp;quot;: &amp;quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/http_requests&amp;quot;
  },
  &amp;quot;items&amp;quot;: [
    {
      &amp;quot;describedObject&amp;quot;: {
        &amp;quot;kind&amp;quot;: &amp;quot;Pod&amp;quot;,
        &amp;quot;namespace&amp;quot;: &amp;quot;default&amp;quot;,
        &amp;quot;name&amp;quot;: &amp;quot;podinfo-6b86c8ccc9-kv5g9&amp;quot;,
        &amp;quot;apiVersion&amp;quot;: &amp;quot;/__internal&amp;quot;
      },
      &amp;quot;metricName&amp;quot;: &amp;quot;http_requests&amp;quot;,
      &amp;quot;timestamp&amp;quot;: &amp;quot;2018-01-10T16:49:07Z&amp;quot;,
      &amp;quot;value&amp;quot;: &amp;quot;901m&amp;quot;
    },
    {
      &amp;quot;describedObject&amp;quot;: {
        &amp;quot;kind&amp;quot;: &amp;quot;Pod&amp;quot;,
        &amp;quot;namespace&amp;quot;: &amp;quot;default&amp;quot;,
        &amp;quot;name&amp;quot;: &amp;quot;podinfo-6b86c8ccc9-nm7bl&amp;quot;,
        &amp;quot;apiVersion&amp;quot;: &amp;quot;/__internal&amp;quot;
      },
      &amp;quot;metricName&amp;quot;: &amp;quot;http_requests&amp;quot;,
      &amp;quot;timestamp&amp;quot;: &amp;quot;2018-01-10T16:49:07Z&amp;quot;,
      &amp;quot;value&amp;quot;: &amp;quot;898m&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;m&lt;/code&gt; 表示 &lt;code&gt;毫&lt;/code&gt;，例如，&lt;code&gt;901m&lt;/code&gt; 表示 901 毫次/每秒。&lt;/p&gt;

&lt;p&gt;创建一个 HPA，如果请求数超过每秒 10 次将扩大 podinfo 副本数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: podinfo
spec:
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: podinfo
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Pods
    pods:
      metricName: http_requests
      targetAverageValue: 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 default 命名空间部署 podinfo HPA：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl create -f ./podinfo/podinfo-hpa-custom.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;几秒钟后 HPA 从 metrics API 获取 &lt;code&gt;http_requests&lt;/code&gt; 的值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get hpa

NAME      REFERENCE            TARGETS     MINPODS   MAXPODS   REPLICAS   AGE
podinfo   Deployment/podinfo   899m / 10   2         10        2          1m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以每秒 25 个请求数的速度给 podinfo 加压：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#install hey
$ go get -u github.com/rakyll/hey

#do 10K requests rate limited at 25 QPS
$ hey -n 10000 -q 5 -c 5 http://PODINFO_SVC_IP:9898/healthz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;几分钟后，HPA 开始扩大 podinfo 的副本数：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl describe hpa podinfo

Name:                       podinfo
Namespace:                  default
Reference:                  Deployment/podinfo
Metrics:                    ( current / target )
  &amp;quot;http_requests&amp;quot; on pods:  9059m / 10
Min replicas:               2
Max replicas:               10

Events:
  Type    Reason             Age   From                       Message
  ----    ------             ----  ----                       -------
  Normal  SuccessfulRescale  2m    horizontal-pod-autoscaler  New size: 3; reason: pods metric http_requests above target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以目前的请求速度，podinfo 的副本数永远不会扩展到最大值，三个副本足以让每个 Pod 的请求速度保持在每秒 10 次以下。&lt;/p&gt;

&lt;p&gt;停止加压后，HPA 会将副本数缩减成最小值：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Events:
  Type    Reason             Age   From                       Message
  ----    ------             ----  ----                       -------
  Normal  SuccessfulRescale  5m    horizontal-pod-autoscaler  New size: 3; reason: pods metric http_requests above target
  Normal  SuccessfulRescale  21s   horizontal-pod-autoscaler  New size: 2; reason: All metrics below target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-6-总结-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;6. 总结&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;并非所有的系统都可以仅依靠 CPU 和内存指标来满足 SLA，大多数 Web 应用的后端都需要基于每秒的请求数量进行弹性伸缩来处理突发流量。对于 ETL 应用程序，可以通过设置 Job 队列长度超过某个阈值来触发弹性伸缩。通过 Prometheus 来监控应用程序并暴露出用于弹性伸缩的指标，可以微调应用程序以更好地处理突发事件，从而确保其高可用性。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-7-参考-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;7. 参考&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/stefanprodan/k8s-prom-hpa&#34; target=&#34;_blank&#34;&gt;Kubernetes Horizontal Pod Autoscaler with Prometheus custom metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/DirectXMan12/k8s-prometheus-adapter&#34; target=&#34;_blank&#34;&gt;k8s-prometheus-adapter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes API 扩展</title>
      <link>https://www.yangcs.net/posts/api-aggregation/</link>
      <pubDate>Tue, 19 Jun 2018 06:15:06 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/api-aggregation/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Aggregated（聚合的）API server&lt;/code&gt; 是为了将原来的 API server 这个巨石（monolithic）应用给拆分开，为了方便用户开发自己的 API server 集成进来，而不用直接修改 Kubernetes 官方仓库的代码，这样一来也能将 API server 解耦，方便用户使用实验特性。这些 API server 可以跟 &lt;code&gt;core API server&lt;/code&gt; 无缝衔接，使用 kubectl 也可以管理它们。&lt;/p&gt;

&lt;p&gt;在 &lt;code&gt;1.7+&lt;/code&gt; 版本中，聚合层和 kube-apiserver 一起运行。在扩展资源被注册前，聚合层不执行任何操，要注册其 API,用户必需添加一个 &lt;code&gt;APIService&lt;/code&gt; 对象，该对象需在 Kubernetes API 中声明 URL 路径，聚合层将发送到该 API 路径(e.g. /apis/myextension.mycompany.io/v1/…)的所有对象代理到注册的 APIService。&lt;/p&gt;

&lt;p&gt;通常，通过在集群中的一个 Pod 中运行一个 &lt;code&gt;extension-apiserver&lt;/code&gt; 来实现 APIService。如果已添加的资源需要主动管理，这个 extension-apiserver 通常需要和一个或多个控制器配对。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-创建聚合层-api-证书-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. 创建聚合层 API 证书&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;如果想开启聚合层 API，需要创建几个与聚合层 API 相关的证书。&lt;/p&gt;

&lt;h3 id=&#34;安装-cfssl&#34;&gt;安装 cfssl&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;方式一：直接使用二进制源码包安装&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
$ chmod +x cfssl_linux-amd64
$ mv cfssl_linux-amd64 /usr/local/bin/cfssl

$ wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
$ chmod +x cfssljson_linux-amd64
$ mv cfssljson_linux-amd64 /usr/local/bin/cfssljson

$ wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
$ chmod +x cfssl-certinfo_linux-amd64
$ mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo

$ export PATH=/usr/local/bin:$PATH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;方式二：使用go命令安装&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ go get -u github.com/cloudflare/cfssl/cmd/...
$ls $GOPATH/bin/cfssl*
cfssl cfssl-bundle cfssl-certinfo cfssljson cfssl-newkey cfssl-scan
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;$GOPATH/bin&lt;/code&gt; 目录下得到以 cfssl 开头的几个命令。&lt;/p&gt;

&lt;p&gt;注意：以下文章中出现的 cat 的文件名如果不存在需要手工创建。&lt;/p&gt;

&lt;h3 id=&#34;创建-ca-certificate-authority&#34;&gt;创建 CA (Certificate Authority)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;创建 CA 配置文件&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir /root/ssl
$ cd /root/ssl
$ cfssl print-defaults config &amp;gt; config.json
$ cfssl print-defaults csr &amp;gt; csr.json
# 根据config.json文件的格式创建如下的ca-config.json文件
# 过期时间设置成了 87600h
$ cat &amp;gt; aggregator-ca-config.json &amp;lt;&amp;lt;EOF
{
  &amp;quot;signing&amp;quot;: {
    &amp;quot;default&amp;quot;: {
      &amp;quot;expiry&amp;quot;: &amp;quot;87600h&amp;quot;
    },
    &amp;quot;profiles&amp;quot;: {
      &amp;quot;aggregator&amp;quot;: {
        &amp;quot;usages&amp;quot;: [
            &amp;quot;signing&amp;quot;,
            &amp;quot;key encipherment&amp;quot;,
            &amp;quot;server auth&amp;quot;,
            &amp;quot;client auth&amp;quot;
        ],
        &amp;quot;expiry&amp;quot;: &amp;quot;87600h&amp;quot;
      }
    }
  }
}
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;字段说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;profiles&lt;/code&gt; : 可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;signing&lt;/code&gt; ：表示该证书可用于签名其它证书；生成的 aggregator-ca.pem 证书中 &lt;code&gt;CA=TRUE&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;server auth&lt;/code&gt; ：表示 Client 可以用该 CA 对 Server 提供的证书进行验证。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;client auth&lt;/code&gt; ：表示 Server 可以用该 CA 对 Client 提供的证书进行验证。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;创建 CA 证书签名请求&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;创建 &lt;code&gt;aggregator-ca-csr.json&lt;/code&gt; 文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;CN&amp;quot;: &amp;quot;aggregator&amp;quot;,
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;Shanghai&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;Shanghai&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ],
    &amp;quot;ca&amp;quot;: {
       &amp;quot;expiry&amp;quot;: &amp;quot;87600h&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;字段说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;span id=&#34;inline-blue&#34;&gt;&amp;ldquo;CN&amp;rdquo;&lt;/span&gt; ：&lt;code&gt;Common Name&lt;/code&gt;，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法。&lt;/li&gt;
&lt;li&gt;&lt;span id=&#34;inline-blue&#34;&gt;&amp;ldquo;O&amp;rdquo;&lt;/span&gt; ：&lt;code&gt;Organization&lt;/code&gt;，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;生成 CA 证书和私钥&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cfssl gencert -initca aggregator-ca-csr.json | cfssljson -bare aggregator-ca
$ ls aggregator-ca*
aggregator-ca-config.json  aggregator-ca.csr  aggregator-ca-csr.json  aggregator-ca-key.pem  aggregator-ca.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;创建-kubernetes-证书&#34;&gt;创建 kubernetes 证书&lt;/h3&gt;

&lt;p&gt;创建 aggregator 证书签名请求文件 &lt;code&gt;aggregator-csr.json&lt;/code&gt; ：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;CN&amp;quot;: &amp;quot;aggregator&amp;quot;,
    &amp;quot;hosts&amp;quot;: [
      &amp;quot;127.0.0.1&amp;quot;,
      &amp;quot;192.168.123.250&amp;quot;,
      &amp;quot;192.168.123.248&amp;quot;,
      &amp;quot;192.168.123.249&amp;quot;,
      &amp;quot;10.254.0.1&amp;quot;,
      &amp;quot;kubernetes&amp;quot;,
      &amp;quot;kubernetes.default&amp;quot;,
      &amp;quot;kubernetes.default.svc&amp;quot;,
      &amp;quot;kubernetes.default.svc.cluster&amp;quot;,
      &amp;quot;kubernetes.default.svc.cluster.local&amp;quot;
    ],
    &amp;quot;key&amp;quot;: {
        &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
        &amp;quot;size&amp;quot;: 2048
    },
    &amp;quot;names&amp;quot;: [
        {
            &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
            &amp;quot;ST&amp;quot;: &amp;quot;Shanghai&amp;quot;,
            &amp;quot;L&amp;quot;: &amp;quot;Shanghai&amp;quot;,
            &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
            &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，由于该证书后续被 etcd 集群和 kubernetes master 集群使用，所以上面分别指定了 &lt;code&gt;etcd&lt;/code&gt; 集群、&lt;code&gt;kubernetes master&lt;/code&gt; 集群的主机 IP 和 kubernetes 服务的服务 IP（一般是 kube-apiserver 指定的 &lt;code&gt;service-cluster-ip-range&lt;/code&gt; 网段的第一个 IP，如 10.254.0.1）。&lt;/li&gt;
&lt;li&gt;以上物理节点的 IP 也可以更换为主机名。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;生成 aggregator 证书和私钥&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=aggregator aggregator-csr.json | cfssljson -bare aggregator
$ ls aggregator*
aggregator.csr  aggregator-csr.json  aggregator-key.pem  aggregator.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;分发证书&#34;&gt;分发证书&lt;/h3&gt;

&lt;p&gt;将生成的证书和秘钥文件（后缀名为.pem）拷贝到 Master 节点的 &lt;code&gt;/etc/kubernetes/ssl&lt;/code&gt; 目录下备用。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cp *.pem /etc/kubernetes/ssl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-2-开启聚合层-api-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. 开启聚合层 API&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;code&gt;kube-apiserver&lt;/code&gt; 增加以下配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--requestheader-client-ca-file=/etc/kubernetes/ssl/aggregator-ca.pem
--requestheader-allowed-names=aggregator
--requestheader-extra-headers-prefix=X-Remote-Extra-
--requestheader-group-headers=X-Remote-Group
--requestheader-username-headers=X-Remote-User
--proxy-client-cert-file=/etc/kubernetes/ssl/aggregator.pem
--proxy-client-key-file=/etc/kubernetes/ssl/aggregator-key.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;前面创建的证书的 &lt;code&gt;CN&lt;/code&gt; 字段的值必须和参数 &lt;code&gt;--requestheader-allowed-names&lt;/code&gt; 指定的值 &lt;code&gt;aggregator&lt;/code&gt; 相同。&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;重启 kube-apiserver：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl daemon-reload
$ systemctl restart kube-apiserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 &lt;code&gt;kube-proxy&lt;/code&gt; 没有在 Master 上面运行，&lt;code&gt;kube-proxy&lt;/code&gt; 还需要添加配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--enable-aggregator-routing=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-参考-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 参考&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/&#34; target=&#34;_blank&#34;&gt;Extending the Kubernetes API with the aggregation layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/&#34; target=&#34;_blank&#34;&gt;Configure the aggregation layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/practice/create-tls-and-secret-key.html&#34; target=&#34;_blank&#34;&gt;创建TLS证书和秘钥&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title> 修复 Service Endpoint 更新的延迟</title>
      <link>https://www.yangcs.net/posts/kubernetes-fixing-delayed-service-endpoint-updates/</link>
      <pubDate>Fri, 15 Jun 2018 14:02:11 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/kubernetes-fixing-delayed-service-endpoint-updates/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;几个月前，我在更新 Kubernetes 集群中的 &lt;code&gt;Deployment&lt;/code&gt; 时发现了一个很奇怪的连接超时现象，在更新 Deployment 之后的 30 秒到两分钟左右，所有与以该 Deployment 作为服务后端的 &lt;code&gt;Service&lt;/code&gt; 的连接都会超时或失败。同时我还注意到其他应用在这段时间内也会出现莫名其妙的延迟现象。&lt;/p&gt;

&lt;p&gt;一开始我怀疑是&lt;a href=&#34;https://hackernoon.com/graceful-shutdown-in-kubernetes-435b98794461&#34; target=&#34;_blank&#34;&gt;应用没有优雅删除&lt;/a&gt;导致的，但当我在更新 Deployment 的过程中（删除旧的 Pod，启动新的 Pod）通过 &lt;code&gt;curl&lt;/code&gt; 来测试该应用的健康检查（liveness）和就绪检查（readiness）&lt;code&gt;Endpoints&lt;/code&gt; 时，很快就排除了这个可能性。&lt;/p&gt;

&lt;p&gt;我开始怀疑人生，开始怀疑我的职业选择，几个小时之后我忽然想起来 &lt;code&gt;Service&lt;/code&gt; 并不是直接与 Deployment 关联的，而是按照标签对一组提供相同功能的 Pods 的抽象，并为它们提供一个统一的入口。更重要的是，Service 是由一组 &lt;code&gt;Endpoint&lt;/code&gt; 组成的，只要 Service 中的一组 Pod 发生变更，Endpoint 就会被更新。&lt;/p&gt;

&lt;p&gt;想到这里，就可以继续排查问题了。下面在更新 Deployment 的过程中通过 &lt;code&gt;watch&lt;/code&gt; 命令来观察有问题的 Service 的 Endpoint。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ watch kubectl describe endpoints [endpoint name]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我就发现了罪魁祸首，在旧 Pod 被移除的 30 秒到几分钟左右的时间段内，这些被删除的 Pod 的 &lt;code&gt;IP:Port&lt;/code&gt; 仍然出现在 Endpoint 的就绪列表中，同时新启动的 Pod 的 &lt;code&gt;IP:Port&lt;/code&gt; 也没有被添加到 Endpoint 中。终于发现了连接失败的根源，但是为什么会出现这种状况呢？仍然无解。&lt;/p&gt;

&lt;p&gt;又经历了几天折腾之后，我又有了新点子，那就是调试负责更新 Endpoint 的组件：&lt;code&gt;kube-controller-manager&lt;/code&gt;，最后终于在 kube-controller-manager 的日志输出中发现了如下可疑的信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;I0412 22:59:59.914517       1 request.go:638] Throttling request took 2.489742918s, request: GET:https://10.3.0.1:443/api/v1/namespaces/[some namespace]/endpoints/[some endpoints]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但还是感觉哪里不对劲，明明延迟了几分钟，为什么这里显示的只有两秒？&lt;/p&gt;

&lt;p&gt;在阅读了 kube-controller-manager 的源码后，我发现了问题所在。Kube-controller-manager 的主要职责是通过内部的众多 &lt;code&gt;Controller&lt;/code&gt; 将集群的当前状态调整到期望状态，其中 &lt;code&gt;Endpoint Controller&lt;/code&gt; 用于监控 Pod 的生命周期事件并根据这些事件更新 Endpoint。&lt;/p&gt;

&lt;p&gt;Endpoint Controller 内部运行了一组 &lt;code&gt;workers&lt;/code&gt; 来处理这些事件并更新 Endpoint，如果有足够多的对 Endpoint 发起的请求被阻塞，那么所有的 workers 都会忙于等待被阻塞的请求，这时候新事件只能被添加到队列中排队等待，如果该队列很长，就会花很长时间来更新 Endpoint。&lt;/p&gt;

&lt;p&gt;为了解决这个问题，首先我通过调整 kube-controller-manager 的 参数 &lt;code&gt;--concurrent-endpoints-syncs&lt;/code&gt; 来增加 Endpoint Controller 的 workers，但收效甚微。&lt;/p&gt;

&lt;p&gt;再次仔细阅读源码后，我找到了两个可以可以扭转战局的参数：&lt;code&gt;--kube-api-qps&lt;/code&gt; 和 &lt;code&gt;--kube-api-burst&lt;/code&gt;。kube-controller-manager 可以通过这两个参数来限制任何 Controller（包括 Endpoint Controller）对 kube-apiserver 发起的请求的速率。&lt;/p&gt;

&lt;p&gt;这两个参数的默认值是 20，但当集群中的主机数量非常多时，默认值显然不满足集群运行的工作负载。经过不断调试之后，我将参数 &lt;code&gt;--kube-api-qps&lt;/code&gt; 的值设置为 300，将 &lt;code&gt;--kube-api-burst&lt;/code&gt; 的值设置为 325，上面的日志信息便消失了，同时添加或移除 Pod 时 Endpoint 也能够立即更新。&lt;/p&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;&lt;code&gt;--kube-api-qps&lt;/code&gt; 和 &lt;code&gt;--kube-api-burst&lt;/code&gt; 参数的值越大，kube-apiserver 和 etcd 的负载就越高。在我的集群中，通过适当地增加一些负载来解决这个问题是很值得的。&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&#34;p-id-h2-原文链接-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;原文链接&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://engineering.dollarshaveclub.com/kubernetes-fixing-delayed-service-endpoint-updates-fd4d0a31852c&#34; target=&#34;_blank&#34;&gt;Kubernetes: Fixing Delayed Service Endpoint Updates&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes 的奇技淫巧</title>
      <link>https://www.yangcs.net/posts/kubernetes-fucking-trick/</link>
      <pubDate>Mon, 11 Jun 2018 04:35:48 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/kubernetes-fucking-trick/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes 作为云原生时代的“操作系统”，熟悉和使用它是每名用户（User）的必备技能。如果你正在 Kubernetes 上工作，你需要正确的工具和技巧来确保 Kubernetes 集群的高可用以及工作负载的稳定运行。&lt;/p&gt;

&lt;p&gt;随着 Kubernetes 的发展和演变，人们可以从内部来驯服它的无节制行为。但有些人并不情愿干等 Kubernetes 变得易于使用，并且为已投入生产的 Kubernetes 中遇到的很多常见问题制定了自己的解决方案。&lt;/p&gt;

&lt;p&gt;这里我们将介绍一些提高操作效率的技巧，同时列举几个比较有用的开源 Kubernetes 工具，这些工具以各种方式简化 Kubernetes，包括简化命令行交互，简化应用程序部署语法等。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-kubectl-自动补全-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;kubectl 自动补全&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;kubectl&lt;/code&gt; 这个命令行工具非常重要，与之相关的命令也很多，我们也记不住那么多的命令，而且也会经常写错，所以命令自动补全是很有必要的，kubectl 工具本身就支持自动补全，只需简单设置一下即可。&lt;/p&gt;

&lt;h3 id=&#34;bash-用户&#34;&gt;bash 用户&lt;/h3&gt;

&lt;p&gt;大多数用户的 shell 使用的是 &lt;code&gt;bash&lt;/code&gt;，Linux 系统可以通过下面的命令来设置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;$ echo &amp;quot;source &amp;lt;(kubectl completion bash)&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果发现不能自动补全，可以尝试安装 &lt;code&gt;bash-completion&lt;/code&gt; 然后刷新即可！&lt;/p&gt;

&lt;h3 id=&#34;zsh-用户&#34;&gt;zsh 用户&lt;/h3&gt;

&lt;p&gt;如果你使用的 shell 是 &lt;code&gt;zsh&lt;/code&gt;，可以通过下面的命令来设置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ echo &amp;quot;source &amp;lt;(kubectl completion zsh)&amp;quot; &amp;gt;&amp;gt; ~/.zshrc
$ source ~/.zshrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-自定义-kubectl-get-输出-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;自定义 kubectl get 输出&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;kubectl get&lt;/code&gt; 相关资源，默认输出为 kubectl 内置，一般我们也可以使用 &lt;code&gt;-o json&lt;/code&gt; 或者 &lt;code&gt;-o yaml&lt;/code&gt; 查看其完整的资源信息。但是很多时候，我们需要关心的信息并不全面，因此我们需要自定义输出的列，那么可以使用 &lt;code&gt;go-template&lt;/code&gt; 来进行实现。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;go-template&lt;/code&gt; 是 golang 的一种模板，可以参考 &lt;a href=&#34;https://golang.org/pkg/text/template/&#34; target=&#34;_blank&#34;&gt;template的相关说明&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;比如仅仅想要查看获取的 pods 中的各个 pod 的 &lt;code&gt;uid&lt;/code&gt;，则可以使用以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;$ kubectl get pods --all-namespaces -o go-template=&#39;{{range .items}}{{.metadata.uid}}
{{end}}&#39;

2ea418d4-533e-11e8-b722-005056a1bc83
7178b8bf-4e93-11e8-8175-005056a1bc83
a0341475-5338-11e8-b722-005056a1bc83
...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;$ kubectl get pods -o yaml

apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    name: nginx-deployment-1751389443-26gbm
    namespace: default
    uid: a911e34b-f445-11e7-9cda-40f2e9b98448
  ...
- apiVersion: v1
  kind: Pod
  metadata:
    name: nginx-deployment-1751389443-rsbkc
    namespace: default
    uid: a911d2d2-f445-11e7-9cda-40f2e9b98448
  ...
- apiVersion: v1
  kind: Pod
  metadata:
    name: nginx-deployment-1751389443-sdbkx
    namespace: default
    uid: a911da1a-f445-11e7-9cda-40f2e9b98448
    ...
kind: List
metadata: {}
resourceVersion: &amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为 get pods 的返回结果是 &lt;code&gt;List&lt;/code&gt; 类型，获取的 pods 都在 &lt;code&gt;items&lt;/code&gt; 这个的 value 中，因此需要遍历 items，也就有了 &lt;code&gt;{{range .items}}&lt;/code&gt;。而后通过模板选定需要展示的内容，就是 items 中的每个 &lt;code&gt;{{.metadata.uid}}&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;这里特别注意，要做一个特别的处理，就是要把 &lt;code&gt;{{end}}&lt;/code&gt; 前进行换行，以便在模板中插入换行符。&lt;/p&gt;

&lt;p&gt;当然，如果觉得这样处理不优雅的话，也可以使用 &lt;code&gt;printf&lt;/code&gt; 函数，在其中使用 &lt;code&gt;\n&lt;/code&gt; 即可实现换行符的插入。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get pods --all-namespaces -o go-template --template=&#39;{{range .items}}{{printf &amp;quot;%s\n&amp;quot; .metadata.uid}}{{end}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者可以这样：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get pods --all-namespaces -o go-template --template=&#39;{{range .items}}{{.metadata.uid}}{{&amp;quot;\n&amp;quot;}}{{end}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实有了 &lt;code&gt;printf&lt;/code&gt;，就可以很容易的实现对应字段的输出，且样式可以进行自己控制。比如可以这样&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get pods --all-namespaces -o go-template --template=&#39;{{range .items}}{{printf &amp;quot;|%-20s|%-50s|%-30s|\n&amp;quot; .metadata.namespace .metadata.name .metadata.uid}}{{end}}&#39;

|default             |details-v1-64b86cd49-85vks                        |2e7a2a66-533e-11e8-b722-005056a1bc83|
|default             |productpage-v1-84f77f8747-7tkwb                   |2eb4e840-533e-11e8-b722-005056a1bc83|
|default             |ratings-v1-5f46655b57-qlrxp                       |2e89f981-533e-11e8-b722-005056a1bc83|
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面举两个 go-template 高级用法的例子：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;range 嵌套&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 列出所有容器使用的镜像名
$ kubectl get pods --all-namespaces -o go-template --template=&#39;{{range .items}}{{range .spec.containers}}{{printf &amp;quot;%s\n&amp;quot; .image}}{{end}}{{end}}&#39;

istio/examples-bookinfo-details-v1:1.5.0
istio/examples-bookinfo-productpage-v1:1.5.0
istio/examples-bookinfo-ratings-v1:1.5.0
...
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;条件判断&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 列出所有不可调度节点的节点名与 IP
$ kubectl get no -o go-template=&#39;{{range .items}}{{if .spec.unschedulable}}{{.metadata.name}} {{.spec.externalID}}{{&amp;quot;\n&amp;quot;}}{{end}}{{end}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了使用 &lt;code&gt;go-template&lt;/code&gt; 之外，还可以使用逗号分隔的自定义列列表打印表格：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n kube-system get pods coredns-64b597b598-7547d -o custom-columns=NAME:.metadata.name,hostip:.status.hostIP

NAME                       hostip
coredns-64b597b598-7547d   192.168.123.250
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以使用 &lt;code&gt;go-template-file&lt;/code&gt; 自定义模板列表，模板不用通过参数传进去，而是写成一个文件，然后需要指定 &lt;code&gt;template&lt;/code&gt; 指向该文件即可。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat &amp;gt; test.tmpl &amp;lt;&amp;lt; EOF 
NAME                      HOSTIP
metadata.name       status.hostIP
EOF

$ kubectl -n kube-system get pods coredns-64b597b598-7547d -o custom-columns-file=test.tmpl

NAME                       HOSTIP
coredns-64b597b598-7547d   192.168.123.250
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-kube-prompt-https-github-com-c-bata-kube-prompt-交互式-kubernetes-客户端-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;&lt;a href=&#34;https://github.com/c-bata/kube-prompt&#34; target=&#34;_blank&#34;&gt;Kube-prompt&lt;/a&gt;：交互式 Kubernetes 客户端&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;Kube-prompt&lt;/code&gt; 可以让你在 Kubernetes 客户端输入相当于交互式命令会话的东西，并为每个命令提供自动填充的背景信息，你不必键入 kubectl 来为每个命令添加前缀。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/kube-prompt.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-kubectl-aliases-https-github-com-ahmetb-kubectl-aliases-生成-kubectl-别名-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;&lt;a href=&#34;https://github.com/ahmetb/kubectl-aliases&#34; target=&#34;_blank&#34;&gt;Kubectl Aliases&lt;/a&gt;：生成 kubectl 别名&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;如果你需要频繁地使用 kubectl 和 kubernetes api 进行交互，使用别名将会为你节省大量的时间，开源项目 &lt;a href=&#34;https://github.com/ahmetb/kubectl-aliases&#34; target=&#34;_blank&#34;&gt;kubectl-aliases&lt;/a&gt; 可以通过编程的方式生成 kubectl 别名，别名生成规则如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/kubectl-alias.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;简单别名示例&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;font color=red&gt;kd&lt;/font&gt; → &lt;font color=red&gt;k&lt;/font&gt;ubectl &lt;font color=red&gt;d&lt;/font&gt;escribe&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;高级别名示例&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;font color=red&gt;kgdepallw&lt;/font&gt; → &lt;font color=red&gt;k&lt;/font&gt;ubectl &lt;font color=red&gt;g&lt;/font&gt;et &lt;font color=red&gt;dep&lt;/font&gt;loyment &amp;ndash;&lt;font color=red&gt;all&lt;/font&gt;-namespaces &amp;ndash;&lt;font color=red&gt;w&lt;/font&gt;atch&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-kubeval-https-github-com-garethr-kubeval-校验配置文件-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;&lt;a href=&#34;https://github.com/garethr/kubeval&#34; target=&#34;_blank&#34;&gt;Kubeval&lt;/a&gt;：校验配置文件&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;如果你手动写 Kubernetes manifest 文件，检查 manifest 文件的语法是很困难的，特别是当你有多个不同版本的 Kubernetes 集群时，确认配置文件语法是否正确更是难上加难。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/garethr/kubeval&#34; target=&#34;_blank&#34;&gt;Kubeval&lt;/a&gt; 是一个用于校验Kubernetes YAML或JSON配置文件的工具，支持多个Kubernetes版本，可以帮助我们解决不少的麻烦。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用示例&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubeval nginx.yaml

The document nginx.yaml contains an invalid Deployment
---&amp;gt; spec.replicas: Invalid type. Expected: integer, given: string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-kedge-http-kedgeproject-org-简化-kubernetes-部署定义-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;&lt;a href=&#34;http://kedgeproject.org/&#34; target=&#34;_blank&#34;&gt;Kedge&lt;/a&gt;：简化 Kubernetes 部署定义&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;很多人都抱怨 Kubernetes manifest 文件的定义太复杂和冗长。它们很难写，而且很难维护，如果能够简化部署定义就会极大地降低维护难度。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://kedgeproject.org/&#34; target=&#34;_blank&#34;&gt;Kedge&lt;/a&gt; 提供更简单、更简洁的语法，然后 kedge 将其转换为 Kubernetes manifest 文件。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用示例&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# Web server Kedge example
name: httpd
deployments:
- containers:
  - image: centos/httpd
services:
- name: httpd
  type: LoadBalancer
  portMappings: 
    - 8080:80
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# Converted Kubernetes artifact file(s)
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: httpd
  name: httpd
spec:
  ports:
  - name: httpd-8080
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: httpd
  type: LoadBalancer
status:
  loadBalancer: {}
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: httpd
  name: httpd
spec:
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: httpd
      name: httpd
    spec:
      containers:
      - image: centos/httpd
        name: httpd
        resources: {}
status: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-参考-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;参考&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://segmentfault.com/a/1190000014526263?utm_source=tag-newest&#34; target=&#34;_blank&#34;&gt;为高效 Ops 和 SRE 团队准备的 10 个开源 k8s 工具&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/posts/configuring-efficient-kubernetes-cli-terminal/&#34; target=&#34;_blank&#34;&gt;打造高效的Kubernetes命令行终端&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>kubectl run 背后到底发生了什么？</title>
      <link>https://www.yangcs.net/posts/what-happens-when-k8s/</link>
      <pubDate>Fri, 01 Jun 2018 11:36:45 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/what-happens-when-k8s/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;想象一下，如果我想将 nginx 部署到 Kubernetes 集群，我可能会在终端中输入类似这样的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl run --image=nginx --replicas=3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后回车。几秒钟后，你就会看到三个 nginx pod 分布在所有的工作节点上。这一切就像变魔术一样，但你并不知道这一切的背后究竟发生了什么事情。&lt;/p&gt;

&lt;p&gt;Kubernetes 的神奇之处在于：它可以通过用户友好的 API 来处理跨基础架构的 &lt;code&gt;deployments&lt;/code&gt;，而背后的复杂性被隐藏在简单的抽象中。但为了充分理解它为我们提供的价值，我们需要理解它的内部原理。&lt;/p&gt;

&lt;p&gt;本指南将引导您理解从 client 到 &lt;code&gt;Kubelet&lt;/code&gt; 的请求的完整生命周期，必要时会通过源代码来说明背后发生了什么。&lt;/p&gt;

&lt;p&gt;这是一份可以在线修改的文档，如果你发现有什么可以改进或重写的，欢迎提供帮助！&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-kubectl-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. kubectl&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;验证和生成器&#34;&gt;验证和生成器&lt;/h3&gt;

&lt;p&gt;当敲下回车键以后，&lt;code&gt;kubectl&lt;/code&gt; 首先会执行一些客户端验证操作，以确保不合法的请求（例如，创建不支持的资源或使用格式错误的镜像名称）将会快速失败，也不会发送给 &lt;code&gt;kube-apiserver&lt;/code&gt;。通过减少不必要的负载来提高系统性能。&lt;/p&gt;

&lt;p&gt;验证通过之后， kubectl 开始将发送给 kube-apiserver 的 HTTP 请求进行封装。&lt;code&gt;kube-apiserver&lt;/code&gt; 与 etcd 进行通信，所有尝试访问或更改 Kubernetes 系统状态的请求都会通过 kube-apiserver 进行，kubectl 也不例外。kubectl 使用生成器（&lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl-conventions/#generators&#34; target=&#34;_blank&#34;&gt;generators&lt;/a&gt;）来构造 HTTP 请求。生成器是一个用来处理序列化的抽象概念。&lt;/p&gt;

&lt;p&gt;通过 &lt;code&gt;kubectl run&lt;/code&gt; 不仅可以运行 &lt;code&gt;deployment&lt;/code&gt;，还可以通过指定参数 &lt;code&gt;--generator&lt;/code&gt; 来部署其他多种资源类型。如果没有指定 &lt;code&gt;--generator&lt;/code&gt; 参数的值，kubectl 将会自动判断资源的类型。&lt;/p&gt;

&lt;p&gt;例如，带有参数 &lt;code&gt;--restart-policy=Always&lt;/code&gt; 的资源将被部署为 Deployment，而带有参数 &lt;code&gt;--restart-policy=Never&lt;/code&gt; 的资源将被部署为 Pod。同时 kubectl 也会检查是否需要触发其他操作，例如记录命令（用来进行回滚或审计）。&lt;/p&gt;

&lt;p&gt;在 kubectl 判断出要创建一个 Deployment 后，它将使用 &lt;code&gt;DeploymentV1Beta1&lt;/code&gt; 生成器从我们提供的参数中生成一个&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/7650665059e65b4b22375d1e28da5306536a12fb/pkg/kubectl/run.go#L59&#34; target=&#34;_blank&#34;&gt;运行时对象&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;api-版本协商与-api-组&#34;&gt;API 版本协商与 API 组&lt;/h3&gt;

&lt;p&gt;为了更容易地消除字段或者重新组织资源结构，Kubernetes 支持多个 API 版本，每个版本都在不同的 API 路径下，例如 &lt;code&gt;/api/v1&lt;/code&gt; 或者 &lt;code&gt;/apis/extensions/v1beta1&lt;/code&gt;。不同的 API 版本表明不同的稳定性和支持级别，更详细的描述可以参考 &lt;a href=&#34;https://k8smeetup.github.io/docs/reference/api-overview/&#34; target=&#34;_blank&#34;&gt;Kubernetes API 概述&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;API 组旨在对类似资源进行分类，以便使得 Kubernetes API 更容易扩展。API 的组别在 REST 路径或者序列化对象的 &lt;code&gt;apiVersion&lt;/code&gt; 字段中指定。例如，Deployment 的 API 组名是 &lt;code&gt;apps&lt;/code&gt;，最新的 API 版本是 &lt;code&gt;v1beta2&lt;/code&gt;，这就是为什么你要在 Deployment manifests 顶部输入 &lt;code&gt;apiVersion: apps/v1beta2&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;kubectl 在生成运行时对象后，开始为它&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/cmd/run.go#L580-L597&#34; target=&#34;_blank&#34;&gt;找到适当的 API 组和 API 版本&lt;/a&gt;，然后&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/cmd/run.go#L598&#34; target=&#34;_blank&#34;&gt;组装成一个版本化客户端&lt;/a&gt;，该客户端知道资源的各种 REST 语义。该阶段被称为版本协商，kubectl 会扫描 &lt;code&gt;remote API&lt;/code&gt; 上的 &lt;code&gt;/apis&lt;/code&gt; 路径来检索所有可能的 API 组。由于 kube-apiserver 在 &lt;code&gt;/apis&lt;/code&gt; 路径上公开了 OpenAPI 格式的模式文档， 因此客户端很容易找到合适的 API。&lt;/p&gt;

&lt;p&gt;为了提高性能，kubectl &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/7650665059e65b4b22375d1e28da5306536a12fb/pkg/kubectl/cmd/util/factory_client_access.go#L117&#34; target=&#34;_blank&#34;&gt;将 OpenAPI 模式缓存&lt;/a&gt;到了 &lt;code&gt;~/.kube/cache&lt;/code&gt; 目录。如果你想了解 API 发现的过程，请尝试删除该目录并在运行 kubectl 命令时将 &lt;code&gt;-v&lt;/code&gt; 参数的值设为最大值，然后你将会看到所有试图找到这些 API 版本的HTTP 请求。参考 &lt;a href=&#34;https://k8smeetup.github.io/docs/reference/kubectl/cheatsheet/&#34; target=&#34;_blank&#34;&gt;kubectl 备忘单&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;最后一步才是真正地发送 HTTP 请求。一旦请求发送之后获得成功的响应，kubectl 将会根据所需的输出格式打印 success message。&lt;/p&gt;

&lt;h3 id=&#34;客户端身份认证&#34;&gt;客户端身份认证&lt;/h3&gt;

&lt;p&gt;在发送 HTTP 请求之前还要进行客户端认证，这是之前没有提到的，现在可以来看一下。&lt;/p&gt;

&lt;p&gt;为了能够成功发送请求，kubectl 需要先进行身份认证。用户凭证保存在 &lt;code&gt;kubeconfig&lt;/code&gt; 文件中，kubectl 通过以下顺序来找到 kubeconfig 文件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果提供了 &lt;code&gt;--kubeconfig&lt;/code&gt; 参数， kubectl 就使用 &amp;ndash;kubeconfig 参数提供的 kubeconfig 文件。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;如果没有提供 &amp;ndash;kubeconfig 参数，但设置了环境变量 &lt;code&gt;$KUBECONFIG&lt;/code&gt;，则使用该环境变量提供的 kubeconfig 文件。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;如果 &amp;ndash;kubeconfig 参数和环境变量 &lt;code&gt;$KUBECONFIG&lt;/code&gt; 都没有提供，kubectl 就使用默认的 kubeconfig 文件 &lt;code&gt;$HOME/.kube/config&lt;/code&gt;。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;解析完 kubeconfig 文件后，kubectl 会确定当前要使用的上下文、当前指向的群集以及与当前用户关联的任何认证信息。如果用户提供了额外的参数（例如 &amp;ndash;username），则优先使用这些参数覆盖 kubeconfig 中指定的值。一旦拿到这些信息之后， kubectl 就会把这些信息填充到将要发送的 HTTP 请求头中：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;x509 证书使用 &lt;a href=&#34;https://github.com/kubernetes/client-go/blob/82aa063804cf055e16e8911250f888bc216e8b61/rest/transport.go#L80-L89&#34; target=&#34;_blank&#34;&gt;tls.TLSConfig&lt;/a&gt; 发送（包括 CA 证书）。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bearer tokens&lt;/code&gt; 在 HTTP 请求头 &lt;code&gt;Authorization&lt;/code&gt; 中&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/c6f8cf2c47d21d55fa0df928291b2580544886c8/transport/round_trippers.go#L314&#34; target=&#34;_blank&#34;&gt;发送&lt;/a&gt;。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;用户名和密码通过 HTTP 基本认证&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/c6f8cf2c47d21d55fa0df928291b2580544886c8/transport/round_trippers.go#L223&#34; target=&#34;_blank&#34;&gt;发送&lt;/a&gt;。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;OpenID&lt;/code&gt; 认证过程是由用户事先手动处理的，产生一个像 bearer token 一样被发送的 token。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;p-id-h2-2-kube-apiserver-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. kube-apiserver&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;认证-https-k8smeetup-github-io-docs-admin-authentication&#34;&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/admin/authentication/&#34; target=&#34;_blank&#34;&gt;认证&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;现在我们的请求已经成功发送了，接下来将会发生什么？这时候就该 &lt;code&gt;kube-apiserver&lt;/code&gt; 闪亮登场了！kube-apiserver 是客户端和系统组件用来保存和检索集群状态的主要接口。为了执行相应的功能，kube-apiserver 需要能够验证请求者是合法的，这个过程被称为认证。&lt;/p&gt;

&lt;p&gt;那么 apiserver 如何对请求进行认证呢？当 kube-apiserver 第一次启动时，它会查看用户提供的所有 &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/&#34; target=&#34;_blank&#34;&gt;CLI 参数&lt;/a&gt;，并组合成一个合适的令牌列表。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;举个例子：&lt;/strong&gt;如果提供了 &lt;code&gt;--client-ca-file&lt;/code&gt; 参数，则会将 x509 客户端证书认证添加到令牌列表中；如果提供了 &lt;code&gt;--token-auth-file&lt;/code&gt; 参数，则会将 breaer token 添加到令牌列表中。&lt;/p&gt;

&lt;p&gt;每次收到请求时，apiserver 都会&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/51bebaffa01be9dc28195140da276c2f39a10cd4/pkg/authentication/request/union/union.go#L54&#34; target=&#34;_blank&#34;&gt;通过令牌链进行认证，直到某一个认证成功为止&lt;/a&gt;：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/51bebaffa01be9dc28195140da276c2f39a10cd4/pkg/authentication/request/x509/x509.go#L60&#34; target=&#34;_blank&#34;&gt;x509 处理程序&lt;/a&gt;将验证 HTTP 请求是否是由 CA 根证书签名的 TLS 密钥进行编码的。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/51bebaffa01be9dc28195140da276c2f39a10cd4/pkg/authentication/request/bearertoken/bearertoken.go#L38&#34; target=&#34;_blank&#34;&gt;bearer token 处理程序&lt;/a&gt;将验证 &lt;code&gt;--token-auth-file&lt;/code&gt; 参数提供的 token 文件是否存在。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/51bebaffa01be9dc28195140da276c2f39a10cd4/plugin/pkg/authenticator/request/basicauth/basicauth.go#L37&#34; target=&#34;_blank&#34;&gt;基本认证处理程序&lt;/a&gt;确保 HTTP 请求的基本认证凭证与本地的状态匹配。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/20bfbdf738a0643fe77ffd527b88034dcde1b8e3/pkg/authentication/request/union/union.go#L71&#34; target=&#34;_blank&#34;&gt;认证失败&lt;/a&gt;，则请求失败并返回相应的错误信息；如果验证成功，则将请求中的 &lt;code&gt;Authorization&lt;/code&gt; 请求头删除，并&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/e30df5e70ef9127ea69d607207c894251025e55b/pkg/endpoints/filters/authentication.go#L71-L75&#34; target=&#34;_blank&#34;&gt;将用户信息添加到&lt;/a&gt;其上下文中。这给后续的授权和准入控制器提供了访问之前建立的用户身份的能力。&lt;/p&gt;

&lt;h3 id=&#34;授权-https-k8smeetup-github-io-docs-admin-authorization&#34;&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/admin/authorization/&#34; target=&#34;_blank&#34;&gt;授权&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;OK，现在请求已经发送，并且 kube-apiserver 已经成功验证我们是谁，终于解脱了！&lt;/p&gt;

&lt;p&gt;然而事情并没有结束，虽然我们已经证明了&lt;strong&gt;我们是合法的&lt;/strong&gt;，但我们有权执行此操作吗？毕竟身份和权限不是一回事。为了进行后续的操作，kube-apiserver 还要对用户进行授权。&lt;/p&gt;

&lt;p&gt;kube-apiserver 处理授权的方式与处理身份验证的方式相似：通过 kube-apiserver 的启动参数 &lt;code&gt;--authorization_mode&lt;/code&gt; 参数设置。它将组合一系列授权者，这些授权者将针对每个传入的请求进行授权。如果所有授权者都拒绝该请求，则该请求会被禁止响应并且&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/e30df5e70ef9127ea69d607207c894251025e55b/pkg/endpoints/filters/authorization.go#L60&#34; target=&#34;_blank&#34;&gt;不会再继续响应&lt;/a&gt;。如果某个授权者批准了该请求，则请求继续。&lt;/p&gt;

&lt;p&gt;kube-apiserver 目前支持以下几种授权方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/admin/authorization/webhook/&#34; target=&#34;_blank&#34;&gt;webhook&lt;/a&gt;: 它与集群外的 HTTP(S) 服务交互。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/admin/authorization/abac/&#34; target=&#34;_blank&#34;&gt;ABAC&lt;/a&gt;: 它执行静态文件中定义的策略。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/admin/authorization/rbac/&#34; target=&#34;_blank&#34;&gt;RBAC&lt;/a&gt;: 它使用 &lt;code&gt;rbac.authorization.k8s.io&lt;/code&gt;  API Group实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/admin/authorization/node/&#34; target=&#34;_blank&#34;&gt;Node&lt;/a&gt;: 它确保 kubelet 只能访问自己节点上的资源。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;准入控制-https-k8smeetup-github-io-docs-admin-admission-controllers&#34;&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/admin/admission-controllers/&#34; target=&#34;_blank&#34;&gt;准入控制&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;突破了之前所说的认证和授权两道关口之后，客户端的调用请求就能够得到 API Server 的真正响应了吗？答案是：不能！&lt;/p&gt;

&lt;p&gt;从 kube-apiserver 的角度来看，它已经验证了我们的身份并且赋予了相应的权限允许我们继续，但对于 Kubernetes 而言，其他组件对于应不应该允许发生的事情还是很有意见的。所以这个请求还需要通过 &lt;code&gt;Admission Controller&lt;/code&gt; 所控制的一个 &lt;code&gt;准入控制链&lt;/code&gt; 的层层考验，官方标准的 “关卡” 有近十个之多，而且还能自定义扩展！&lt;/p&gt;

&lt;p&gt;虽然授权的重点是回答用户是否有权限，但准入控制器会拦截请求以确保它符合集群的更广泛的期望和规则。它们是资源对象保存到 &lt;code&gt;etcd&lt;/code&gt; 之前的最后一个堡垒，封装了一系列额外的检查以确保操作不会产生意外或负面结果。不同于授权和认证只关心请求的用户和操作，准入控制还处理请求的内容，并且仅对创建、更新、删除或连接（如代理）等有效，而对读操作无效。&lt;/p&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;准入控制器的工作方式与授权者和验证者的工作方式类似，但有一点区别：与验证链和授权链不同，如果某个准入控制器检查不通过，则整个链会中断，整个请求将立即被拒绝并且返回一个错误给终端用户。&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;准入控制器设计的重点在于提高可扩展性，某个控制器都作为一个插件存储在 &lt;code&gt;plugin/pkg/admission&lt;/code&gt; 目录中，并且与某一个接口相匹配，最后被编译到 kube-apiserver 二进制文件中。&lt;/p&gt;

&lt;p&gt;大部分准入控制器都比较容易理解，接下来着重介绍 &lt;code&gt;SecurityContextDeny&lt;/code&gt;、&lt;code&gt;ResourceQuota&lt;/code&gt; 及 &lt;code&gt;LimitRanger&lt;/code&gt; 这三个准入控制器。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;SecurityContextDeny&lt;/span&gt; 该插件将禁止创建设置了 Security Context 的 Pod。&lt;br /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;ResourceQuota&lt;/span&gt; 不仅能限制某个 Namespace 中创建资源的数量，而且能限制某个 Namespace 中被 Pod 所请求的资源总量。该准入控制器和资源对象 &lt;code&gt;ResourceQuota&lt;/code&gt; 一起实现了资源配额管理。&lt;br /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;LimitRanger&lt;/span&gt; 作用类似于上面的 ResourceQuota 控制器，针对 Namespace 资源的每个个体（Pod 与 Container 等）的资源配额。该插件和资源对象 &lt;code&gt;LimitRange&lt;/code&gt; 一起实现资源配额管理。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;p-id-h2-3-etcd-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. etcd&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;到现在为止，Kubernetes 已经对该客户端的调用请求进行了全面彻底地审查，并且已经验证通过，运行它进入下一个环节。下一步 kube-apiserver 将对 HTTP 请求进行反序列化，然后利用得到的结果构建运行时对象（有点像 kubectl 生成器的逆过程），并保存到 &lt;code&gt;etcd&lt;/code&gt; 中。下面我们将这个过程分解一下。&lt;/p&gt;

&lt;p&gt;当收到请求时，kube-apiserver 是如何知道它该怎么做的呢？事实上，在客户端发送调用请求之前就已经产生了一系列非常复杂的流程。我们就从 kube-apiserver 二进制文件首次运行开始分析吧：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;当运行 kube-apiserver 二进制文件时，它会&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/cmd/kube-apiserver/app/server.go#L119&#34; target=&#34;_blank&#34;&gt;创建一个允许 apiserver 聚合的服务链&lt;/a&gt;。这是一种对 &lt;code&gt;Kubernetes API&lt;/code&gt; 进行扩展的方式。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;同时会创建一个 &lt;code&gt;generic apiserver&lt;/code&gt; 作为默认的 apiserver。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;然后&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/server/config.go#L149&#34; target=&#34;_blank&#34;&gt;生成 OpenAPI 规范的配置&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;然后 kube-apiserver 遍历数据结构中指定的所有 API 组，并将每一个 API 组作为通用的存储抽象保存到 etcd 中。当你访问或变更资源状态时，kube-apiserver 就会调用这些 API 组。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;每个 API 组都会遍历它的所有组版本，并且将每个 HTTP 路由&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/endpoints/groupversion.go#L92&#34; target=&#34;_blank&#34;&gt;映射到 REST 路径中&lt;/a&gt;。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;当请求的 METHOD 是 &lt;code&gt;POST&lt;/code&gt; 时，kube-apiserver 就会将请求转交给 &lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/endpoints/handlers/create.go#L37&#34; target=&#34;_blank&#34;&gt;资源创建处理器&lt;/a&gt;。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;现在 kube-apiserver 已经知道了所有的路由及其对应的 REST 路径，以便在请求匹配时知道调用哪些处理器和键值存储。多么机智的设计！现在假设客户端的 HTTP 请求已经被 kube-apiserver 收到了：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;如果处理链可以将请求与已经注册的路由进行匹配，就会将该请求交给注册到该路由的&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/server/handler.go#L143&#34; target=&#34;_blank&#34;&gt;专用处理器&lt;/a&gt;来处理；如果没有任何一个路由可以匹配该请求，就会将请求转交给&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/server/mux/pathrecorder.go#L248&#34; target=&#34;_blank&#34;&gt;基于路径的处理器&lt;/a&gt;（比如当调用 &lt;code&gt;/apis&lt;/code&gt; 时）；如果没有任何一个基于路径的处理器注册到该路径，请求就会被转交给 &lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/server/mux/pathrecorder.go#L254&#34; target=&#34;_blank&#34;&gt;not found 处理器&lt;/a&gt;，最后返回 &lt;code&gt;404&lt;/code&gt;。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;幸运的是，我们有一个名为 &lt;code&gt;createHandler&lt;/code&gt; 的注册路由！它有什么作用呢？首先它会解码 HTTP 请求并进行基本的验证，例如确保请求提供的 json 与 API 资源的版本相匹配。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;接下来进入&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/endpoints/handlers/create.go#L93-L104&#34; target=&#34;_blank&#34;&gt;审计和准入控制&lt;/a&gt;阶段。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;然后资源将会通过 &lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/19667a1afc13cc13930c40a20f2c12bbdcaaa246/pkg/registry/generic/registry/store.go#L327&#34; target=&#34;_blank&#34;&gt;storage provider&lt;/a&gt; 保存&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/endpoints/handlers/create.go#L111&#34; target=&#34;_blank&#34;&gt;到 etcd&lt;/a&gt; 中。默认情况下保存到 etcd 中的键的格式为 &lt;code&gt;&amp;lt;namespace&amp;gt;/&amp;lt;name&amp;gt;&lt;/code&gt;，你也可以自定义。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;资源创建过程中出现的任何错误都会被捕获，最后 &lt;code&gt;storage provider&lt;/code&gt; 会执行 &lt;code&gt;get&lt;/code&gt; 调用来确认该资源是否被成功创建。如果需要额外的清理工作，就会调用后期创建的处理器和装饰器。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;最后构造 HTTP 响应并返回给客户端。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;原来 apiserver 做了这么多的工作，以前竟然没有发现呢！到目前为止，我们创建的 &lt;code&gt;Deployment&lt;/code&gt; 资源已经保存到了 etcd 中，但 apiserver 仍然看不到它。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-4-初始化-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. 初始化&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;在一个资源对象被持久化到数据存储之后，apiserver 还无法完全看到或调度它，在此之前还要执行一系列&lt;a href=&#34;https://kubernetes.io/docs/admin/extensible-admission-controllers/#initializers&#34; target=&#34;_blank&#34;&gt;初始化器&lt;/a&gt;。初始化器是一种与资源类型相关联的控制器，它会在资源对外可用之前执行某些逻辑。如果某个资源类型没有初始化器，就会跳过此初始化步骤立即使资源对外可见。&lt;/p&gt;

&lt;p&gt;正如&lt;a href=&#34;https://ahmet.im/blog/initializers/&#34; target=&#34;_blank&#34;&gt;大佬的博客&lt;/a&gt;指出的那样，初始化器是一个强大的功能，因为它允许我们执行通用引导操作。例如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;将代理边车容器注入到暴露 80 端口的 Pod 中，或者加上特定的 &lt;code&gt;annotation&lt;/code&gt;。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;将保存着测试证书的 &lt;code&gt;volume&lt;/code&gt; 注入到特定命名空间的所有 Pod 中。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;如果 &lt;code&gt;Secret&lt;/code&gt; 中的密码小于 20 个字符，就组织其创建。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;initializerConfiguration&lt;/code&gt; 资源对象允许你声明某些资源类型应该运行哪些初始化器。如果你想每创建一个 Pod 时就运行一个自定义初始化器，你可以这样做：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: admissionregistration.k8s.io/v1alpha1
kind: InitializerConfiguration
metadata:
  name: custom-pod-initializer
initializers:
  - name: podimage.example.com
    rules:
      - apiGroups:
          - &amp;quot;&amp;quot;
        apiVersions:
          - v1
        resources:
          - pods
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过该配置创建资源对象 &lt;code&gt;InitializerConfiguration&lt;/code&gt; 之后，就会在每个 Pod 的 &lt;code&gt;metadata.initializers.pending&lt;/code&gt; 字段中添加 &lt;code&gt;custom-pod-initializer&lt;/code&gt; 字段。该初始化控制器会定期扫描新的 Pod，一旦在 Pod 的 &lt;code&gt;pending&lt;/code&gt; 字段中检测到自己的名称，就会执行其逻辑，执行完逻辑之后就会将 &lt;code&gt;pending&lt;/code&gt; 字段下的自己的名称删除。&lt;/p&gt;

&lt;p&gt;只有在 &lt;code&gt;pending&lt;/code&gt; 字段下的列表中的第一个初始化器可以对资源进行操作，当所有的初始化器执行完成，并且 &lt;code&gt;pending&lt;/code&gt; 字段为空时，该对象就会被认为初始化成功。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;你可能会注意到一个问题：如果 kube-apiserver 不能显示这些资源，那么用户级控制器是如何处理资源的呢？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;为了解决这个问题，kube-apiserver 暴露了一个 &lt;code&gt;?includeUninitialized&lt;/code&gt; 查询参数，它会返回所有的资源对象（包括未初始化的）。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-5-控制循环-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;5. 控制循环&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;deployments-controller&#34;&gt;Deployments controller&lt;/h3&gt;

&lt;p&gt;到了这个阶段，我们的 Deployment 记录已经保存在 etcd 中，并且所有的初始化逻辑都执行完成，接下来的阶段将会涉及到该资源所依赖的拓扑结构。在 Kubernetes 中，Deployment 实际上只是一系列 &lt;code&gt;Replicaset&lt;/code&gt; 的集合，而 Replicaset 是一系列 &lt;code&gt;Pod&lt;/code&gt; 的集合。那么 Kubernetes 是如何从一个 HTTP 请求按照层级结构依次创建这些资源的呢？其实这些工作都是由 Kubernetes 内置的 &lt;code&gt;Controller&lt;/code&gt;(控制器) 来完成的。&lt;/p&gt;

&lt;p&gt;Kubernetes 在整个系统中使用了大量的 Controller，Controller 是一个用于将系统状态从“当前状态”修正到“期望状态”的异步脚本。所有 Controller 都通过 &lt;code&gt;kube-controller-manager&lt;/code&gt; 组件并行运行，每种 Controller 都负责一种具体的控制流程。首先介绍一下 &lt;code&gt;Deployment Controller&lt;/code&gt;：&lt;/p&gt;

&lt;p&gt;将 Deployment 记录存储到 etcd 并初始化后，就可以通过 kube-apiserver 使其可见，然后 &lt;code&gt;Deployment Controller&lt;/code&gt; 就会检测到它（它的工作就是负责监听 Deployment 记录的更改）。在我们的例子中，控制器通过一个 &lt;code&gt;Informer&lt;/code&gt; &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/deployment/deployment_controller.go#L122&#34; target=&#34;_blank&#34;&gt;注册一个创建事件的特定回调函数&lt;/a&gt;（更多信息参加下文）。&lt;/p&gt;

&lt;p&gt;当 Deployment 第一次对外可见时，该 Controller 就会&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/deployment/deployment_controller.go#L170&#34; target=&#34;_blank&#34;&gt;将该资源对象添加到内部工作队列&lt;/a&gt;，然后开始处理这个资源对象：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;通过使用标签选择器查询 kube-apiserver 来&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/deployment/deployment_controller.go#L633&#34; target=&#34;_blank&#34;&gt;检查&lt;/a&gt;该 Deployment 是否有与其关联的 &lt;code&gt;ReplicaSet&lt;/code&gt; 或 &lt;code&gt;Pod&lt;/code&gt; 记录。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;有趣的是，这个同步过程是状态不可知的，它核对新记录与核对已经存在的记录采用的是相同的方式。&lt;/p&gt;

&lt;p&gt;在意识到没有与其关联的 &lt;code&gt;ReplicaSet&lt;/code&gt; 或 &lt;code&gt;Pod&lt;/code&gt; 记录后，Deployment Controller 就会开始执行&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/deployment/sync.go#L385&#34; target=&#34;_blank&#34;&gt;弹性伸缩流程&lt;/a&gt;：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;创建 ReplicaSet 资源，为其分配一个标签选择器并将其版本号设置为 1。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReplicaSet 的 &lt;code&gt;PodSpec&lt;/code&gt; 字段从 Deployment 的 manifest 以及其他相关元数据中复制而来。有时 Deployment 记录在此之后也需要更新（例如，如果设置了 &lt;code&gt;process deadline&lt;/code&gt;）。&lt;/p&gt;

&lt;p&gt;当完成以上步骤之后，该 Deployment 的 &lt;code&gt;status&lt;/code&gt; 就会被更新，然后重新进入与之前相同的循环，等待 Deployment 与期望的状态相匹配。由于 Deployment Controller 只关心 ReplicaSet，因此需要通过 &lt;code&gt;ReplicaSet Controller&lt;/code&gt; 来继续协调。&lt;/p&gt;

&lt;h3 id=&#34;replicasets-controller&#34;&gt;ReplicaSets controller&lt;/h3&gt;

&lt;p&gt;在前面的步骤中，Deployment Controller 创建了第一个 ReplicaSet，但仍然还是没有 Pod，这时候就该 &lt;code&gt;ReplicaSet Controller&lt;/code&gt; 登场了！ReplicaSet Controller 的工作是监视 ReplicaSets 及其相关资源（Pod）的生命周期。和大多数其他 Controller 一样，它通过触发某些事件的处理器来实现此目的。&lt;/p&gt;

&lt;p&gt;当创建 ReplicaSet 时（由 Deployment Controller 创建），RS Controller &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/replicaset/replica_set.go#L583&#34; target=&#34;_blank&#34;&gt;检查新 ReplicaSet 的状态&lt;/a&gt;，并检查当前状态与期望状态之间存在的偏差，然后通过&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/replicaset/replica_set.go#L460&#34; target=&#34;_blank&#34;&gt;调整 Pod 的副本数&lt;/a&gt;来达到期望的状态。&lt;/p&gt;

&lt;p&gt;Pod 的创建也是批量进行的，从 &lt;code&gt;SlowStartInitialBatchSize&lt;/code&gt; 开始，然后在每次成功的迭代中以一种 &lt;code&gt;slow start&lt;/code&gt; 操作加倍。这样做的目的是在大量 Pod 启动失败时（例如，由于资源配额），可以减轻 kube-apiserver 被大量不必要的 HTTP 请求吞没的风险。如果创建失败，最好能够优雅地失败，并且对其他的系统组件造成的影响最小！&lt;/p&gt;

&lt;p&gt;Kubernetes 通过 &lt;code&gt;Owner References&lt;/code&gt;（在子级资源的某个字段中引用其父级资源的 ID） 来构造严格的资源对象层级结构。这确保了一旦 Controller 管理的资源被删除（级联删除），子资源就会被垃圾收集器删除，同时还为父级资源提供了一种有效的方式来避免他们竞争同一个子级资源（想象两对父母都认为他们拥有同一个孩子的场景）。&lt;/p&gt;

&lt;p&gt;Owner References 的另一个好处是：它是有状态的。如果有任何 Controller 重启了，那么由于资源对象的拓扑关系与 Controller 无关，该操作不会影响到系统的稳定运行。这种对资源隔离的重视也体现在 Controller 本身的设计中：Controller 不能对自己没有明确拥有的资源进行操作，它们应该选择对资源的所有权，互不干涉，互不共享。&lt;/p&gt;

&lt;p&gt;有时系统中也会出现孤儿（orphaned）资源，通常由以下两种途径产生：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;父级资源被删除，但子级资源没有被删除&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;垃圾收集策略禁止删除子级资源&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当发生这种情况时，Controller 将会确保孤儿资源拥有新的 &lt;code&gt;Owner&lt;/code&gt;。多个父级资源可以相互竞争同一个孤儿资源，但只有一个会成功（其他父级资源会收到验证错误）。&lt;/p&gt;

&lt;h3 id=&#34;informers&#34;&gt;Informers&lt;/h3&gt;

&lt;p&gt;你可能已经注意到，某些 Controller（例如 RBAC 授权器或 Deployment Controller）需要先检索集群状态然后才能正常运行。拿 RBAC 授权器举例，当请求进入时，授权器会将用户的初始状态缓存下来，然后用它来检索与 etcd 中的用户关联的所有  角色（&lt;code&gt;Role&lt;/code&gt;）和 角色绑定（&lt;code&gt;RoleBinding&lt;/code&gt;）。那么问题来了，Controller 是如何访问和修改这些资源对象的呢？事实上 Kubernetes 是通过 &lt;code&gt;Informer&lt;/code&gt; 机制来解决这个问题的。&lt;/p&gt;

&lt;p&gt;Infomer 是一种模式，它允许 Controller 查找缓存在本地内存中的数据(这份数据由 Informer 自己维护)并列出它们感兴趣的资源。&lt;/p&gt;

&lt;p&gt;虽然 Informer 的设计很抽象，但它在内部实现了大量的对细节的处理逻辑（例如缓存），缓存很重要，因为它不但可以减少对 Kubenetes API 的直接调用，同时也能减少 Server 和 Controller 的大量重复性工作。通过使用 Informer，不同的 Controller 之间以线程安全（Thread safety）的方式进行交互，而不必担心多个线程访问相同的资源时会产生冲突。&lt;/p&gt;

&lt;p&gt;有关 Informer 的更多详细解析，请参考这篇文章：&lt;a href=&#34;https://borismattijssen.github.io/articles/kubernetes-informers-controllers-reflectors-stores&#34; target=&#34;_blank&#34;&gt;Kubernetes: Controllers, Informers, Reflectors and Stores&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;scheduler&#34;&gt;Scheduler&lt;/h3&gt;

&lt;p&gt;当所有的 Controller 正常运行后，etcd 中就会保存一个 Deployment、一个 ReplicaSet 和 三个 Pod 资源记录，并且可以通过 kube-apiserver 查看。然而，这些 Pod 资源现在还处于 &lt;code&gt;Pending&lt;/code&gt; 状态，因为它们还没有被调度到集群中合适的 Node 上运行。这个问题最终要靠调度器（Scheduler）来解决。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Scheduler&lt;/code&gt; 作为一个独立的组件运行在集群控制平面上，工作方式与其他 Controller 相同：监听实际并将系统状态调整到期望的状态。具体来说，Scheduler 的作用是将待调度的 Pod 按照特定的算法和调度策略绑定（Binding）到集群中某个合适的 Node 上，并将绑定信息写入 etcd 中（它会过滤其 PodSpec 中 &lt;code&gt;NodeName&lt;/code&gt; 字段为空的 Pod），默认的调度算法的工作方式如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;当 Scheduler 启动时，会&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/2d64ce5e8e45e26b02492d2b6c85e5ebfb1e4761/plugin/pkg/scheduler/algorithmprovider/defaults/defaults.go#L65-L81&#34; target=&#34;_blank&#34;&gt;注册一个默认的预选策略链&lt;/a&gt;，这些&lt;code&gt;预选策略&lt;/code&gt;会对备选节点进行评估，判断备选节点是否&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/2d64ce5e8e45e26b02492d2b6c85e5ebfb1e4761/plugin/pkg/scheduler/core/generic_scheduler.go#L117&#34; target=&#34;_blank&#34;&gt;满足备选 Pod 的需求&lt;/a&gt;。例如，如果 PodSpec 字段限制了 CPU 和内存资源，那么当备选节点的资源容量不满足备选 Pod 的需求时，备选 Pod 就不会被调度到该节点上（&lt;strong&gt;资源容量=备选节点资源总量-节点中已存在 Pod 的所有容器的需求资源（CPU 和内存）的总和&lt;/strong&gt;）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;一旦筛选出符合要求的候选节点，就会采用&lt;code&gt;优选策略&lt;/code&gt;计算出每个候选节点的积分，然后对这些候选节点进行排序，积分最高者胜出。例如，为了在整个系统中分摊工作负载，这些优选策略会从备选节点列表中选出资源消耗最小的节点。每个节点通过优选策略时都会算出一个得分，计算各项得分，最终选出分值大的节点作为优选的结果。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;一旦找到了合适的节点，Scheduler 就会创建一个 &lt;code&gt;Binding&lt;/code&gt; 对象，该对象的 &lt;code&gt;Name&lt;/code&gt; 和 &lt;code&gt;Uid&lt;/code&gt; 与 Pod 相匹配，并且其 &lt;code&gt;ObjectReference&lt;/code&gt; 字段包含所选节点的名称，然后通过 &lt;code&gt;POST&lt;/code&gt; 请求&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/2d64ce5e8e45e26b02492d2b6c85e5ebfb1e4761/plugin/pkg/scheduler/factory/factory.go#L1095&#34; target=&#34;_blank&#34;&gt;发送给 apiserver&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;当 kube-apiserver 接收到此 Binding 对象时，注册吧会将该对象&lt;strong&gt;反序列化&lt;/strong&gt;并更新 Pod 资源中的以下字段：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;将 &lt;code&gt;NodeName&lt;/code&gt; 的值设置为 ObjectReference 中的 NodeName。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;添加相关的注释。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;将 &lt;code&gt;PodScheduled&lt;/code&gt; 的 &lt;code&gt;status&lt;/code&gt; 值设置为 True。可以通过 kubectl 来查看：&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get &amp;lt;PODNAME&amp;gt; -o go-template=&#39;{{range .status.conditions}}{{if eq .type &amp;quot;PodScheduled&amp;quot;}}{{.status}}{{end}}{{end}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一旦 Scheduler 将 Pod 调度到某个节点上，该节点的 &lt;code&gt;Kubelet&lt;/code&gt; 就会接管该 Pod 并开始部署。&lt;/p&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;预选策略和优选策略都可以通过 &lt;code&gt;--policy-config-file&lt;/code&gt; 参数来扩展，如果默认的调度器不满足要求，还可以部署自定义的调度器。如果  &lt;code&gt;podSpec.schedulerName&lt;/code&gt; 的值设置为其他的调度器，则 Kubernetes 会将该 Pod 的调度转交给那个调度器。&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&#34;p-id-h2-6-kubelet-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;6. Kubelet&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;pod-同步&#34;&gt;Pod 同步&lt;/h3&gt;

&lt;p&gt;现在，所有的 Controller 都完成了工作，我们来总结一下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;HTTP 请求通过了认证、授权和准入控制阶段。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;一个 Deployment、ReplicaSet 和三个 Pod 资源被持久化到 etcd 存储中。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;然后运行了一系列初始化器。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;最后每个 Pod 都被调度到合适的节点。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然而到目前为止，所有的状态变化仅仅只是针对保存在 etcd 中的资源记录，接下来的步骤涉及到运行在工作节点之间的 Pod 的分布状况，这是分布式系统（比如 Kubernetes）的关键因素。这些任务都是由 &lt;code&gt;Kubelet&lt;/code&gt; 组件完成的，让我们开始吧！&lt;/p&gt;

&lt;p&gt;在 Kubernetes 集群中，每个 Node 节点上都会启动一个 Kubelet 服务进程，该进程用于处理 Scheduler 下发到本节点的任务，管理 Pod 的生命周期，包括挂载卷、容器日志记录、垃圾回收以及其他与 Pod 相关的事件。&lt;/p&gt;

&lt;p&gt;如果换一种思维模式，你可以把 Kubelet 当成一种特殊的 Controller，它每隔 20 秒（可以自定义）向 kube-apiserver 通过 &lt;code&gt;NodeName&lt;/code&gt; 获取自身 Node 上所要运行的 Pod 清单。一旦获取到了这个清单，它就会通过与自己的内部缓存进行比较来检测新增加的 Pod，如果有差异，就开始同步 Pod 列表。我们来详细分析一下同步过程：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;如果 Pod 正在创建， Kubelet 就会&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/fc8bfe2d8929e11a898c4557f9323c482b5e8842/pkg/kubelet/kubelet.go#L1519&#34; target=&#34;_blank&#34;&gt;记录一些在 &lt;code&gt;Prometheus&lt;/code&gt; 中用于追踪 Pod 启动延时的指标&lt;/a&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;然后生成一个 &lt;code&gt;PodStatus&lt;/code&gt; 对象，它表示 Pod 当前阶段的状态。Pod 的状态(&lt;code&gt;Phase&lt;/code&gt;) 是 Pod 在其生命周期中的最精简的概要，包括 &lt;code&gt;Pending&lt;/code&gt;，&lt;code&gt;Running&lt;/code&gt;，&lt;code&gt;Succeeded&lt;/code&gt;，&lt;code&gt;Failed&lt;/code&gt; 和 &lt;code&gt;Unkown&lt;/code&gt; 这几个值。状态的产生过程非常过程，所以很有必要深入了解一下背后的原理：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;首先串行执行一系列 Pod 同步处理器（&lt;code&gt;PodSyncHandlers&lt;/code&gt;），每个处理器检查检查 Pod 是否应该运行在该节点上。当所有的处理器都认为该 Pod 不应该运行在该节点上，则 Pod 的 &lt;code&gt;Phase&lt;/code&gt; 值就会变成 &lt;code&gt;PodFailed&lt;/code&gt;，并且将该 Pod 从该节点上驱逐出去。例如当你创建一个 &lt;code&gt;Job&lt;/code&gt; 时，如果 Pod 失败重试的时间超过了 &lt;code&gt;spec.activeDeadlineSeconds&lt;/code&gt; 设置的值，就会将 Pod 从该节点驱逐出去。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;接下来，Pod 的 Phase 值由 &lt;code&gt;init 容器&lt;/code&gt; 和应用容器的状态共同来决定。因为目前容器还没有启动，容器被视为&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/fc8bfe2d8929e11a898c4557f9323c482b5e8842/pkg/kubelet/kubelet_pods.go#L1244&#34; target=&#34;_blank&#34;&gt;处于等待阶段&lt;/a&gt;，如果 Pod 中至少有一个容器处于等待阶段，则其 &lt;code&gt;Phase&lt;/code&gt; 值为 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/fc8bfe2d8929e11a898c4557f9323c482b5e8842/pkg/kubelet/kubelet_pods.go#L1258-L1261&#34; target=&#34;_blank&#34;&gt;Pending&lt;/a&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;最后，Pod 的 &lt;code&gt;Condition&lt;/code&gt; 字段由 Pod 内所有容器的状态决定。现在我们的容器还没有被容器运行时创建，所以 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/fc8bfe2d8929e11a898c4557f9323c482b5e8842/pkg/kubelet/status/generate.go#L70-L81&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;PodReady&lt;/code&gt; 的状态被设置为 &lt;code&gt;False&lt;/code&gt;&lt;/a&gt;。可以通过 kubectl 查看：&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get &amp;lt;PODNAME&amp;gt; -o go-template=&#39;{{range .status.conditions}}{{if eq .type &amp;quot;Ready&amp;quot;}}{{.status}}{{end}}{{end}}&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;生成 PodStatus 之后（Pod 中的 &lt;code&gt;status&lt;/code&gt; 字段），Kubelet 就会将它发送到 Pod 的状态管理器，该管理器的任务是通过 apiserver 异步更新 etcd 中的记录。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;接下来运行一系列&lt;strong&gt;准入处理器&lt;/strong&gt;来确保该 Pod 是否具有相应的权限（包括强制执行 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/fc8bfe2d8929e11a898c4557f9323c482b5e8842/pkg/kubelet/kubelet.go#L883-L884&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;AppArmor&lt;/code&gt; 配置文件和 &lt;code&gt;NO_NEW_PRIVS&lt;/code&gt;&lt;/a&gt;），被准入控制器拒绝的 Pod 将一直保持 &lt;code&gt;Pending&lt;/code&gt; 状态。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果 Kubelet 启动时指定了 &lt;code&gt;cgroups-per-qos&lt;/code&gt; 参数，Kubelet 就会为该 Pod 创建 &lt;code&gt;cgroup&lt;/code&gt; 并进行相应的资源限制。这是为了更方便地对 Pod 进行服务质量（QoS）管理。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;然后为 Pod 创建相应的目录，包括 Pod 的目录（&lt;code&gt;/var/run/kubelet/pods/&amp;lt;podID&amp;gt;&lt;/code&gt;），该 Pod 的卷目录（&lt;code&gt;&amp;lt;podDir&amp;gt;/volumes&lt;/code&gt;）和该 Pod 的插件目录（&lt;code&gt;&amp;lt;podDir&amp;gt;/plugins&lt;/code&gt;）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;卷管理器&lt;/strong&gt;会&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/2723e06a251a4ec3ef241397217e73fa782b0b98/pkg/kubelet/volumemanager/volume_manager.go#L330&#34; target=&#34;_blank&#34;&gt;挂载 &lt;code&gt;Spec.Volumes&lt;/code&gt; 中定义的相关数据卷，然后等待是否挂载成功&lt;/a&gt;。根据挂载卷类型的不同，某些 Pod 可能需要等待更长的时间（比如 NFS 卷）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/dd9981d038012c120525c9e6df98b3beb3ef19e1/pkg/kubelet/kubelet_pods.go#L788&#34; target=&#34;_blank&#34;&gt;从 apiserver 中检索&lt;/a&gt; &lt;code&gt;Spec.ImagePullSecrets&lt;/code&gt; 中定义的所有 &lt;code&gt;Secret&lt;/code&gt;，然后将其注入到容器中。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;最后通过容器运行时接口（&lt;code&gt;Container Runtime Interface（CRI）&lt;/code&gt;）开始启动容器（下面会详细描述）。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;cri-与-pause-容器&#34;&gt;CRI 与 pause 容器&lt;/h3&gt;

&lt;p&gt;到了这个阶段，大量的初始化工作都已经完成，容器已经准备好开始启动了，而容器是由&lt;strong&gt;容器运行时&lt;/strong&gt;（例如 &lt;code&gt;Docker&lt;/code&gt; 和 &lt;code&gt;Rkt&lt;/code&gt;）启动的。&lt;/p&gt;

&lt;p&gt;为了更容易扩展，Kubelet 从 1.5.0 开始通过&lt;strong&gt;容器运行时接口&lt;/strong&gt;与容器运行时（Container Runtime）交互。简而言之，CRI 提供了 Kubelet 和特定的运行时之间的抽象接口，它们之间通过&lt;a href=&#34;https://github.com/google/protobuf&#34; target=&#34;_blank&#34;&gt;协议缓冲区&lt;/a&gt;（它像一个更快的 JSON）和 &lt;a href=&#34;https://grpc.io/&#34; target=&#34;_blank&#34;&gt;gRPC API&lt;/a&gt;（一种非常适合执行 Kubernetes 操作的 API）。这是一个非常酷的想法，通过使用 Kubelet 和运行时之间定义的契约关系，容器如何编排的具体实现细节已经变得无关紧要。由于不需要修改 Kubernetes 的核心代码，开发者可以以最小的开销添加新的运行时。&lt;/p&gt;

&lt;p&gt;不好意思有点跑题了，让我们继续回到容器启动的阶段。第一次启动 Pod 时，Kubelet 会通过 &lt;code&gt;Remote Procedure Command&lt;/code&gt;(RPC) 协议调用 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/2d64ce5e8e45e26b02492d2b6c85e5ebfb1e4761/pkg/kubelet/kuberuntime/kuberuntime_sandbox.go#L51&#34; target=&#34;_blank&#34;&gt;RunPodSandbox&lt;/a&gt;。&lt;code&gt;sandbox&lt;/code&gt; 用于描述一组容器，例如在 Kubernetes 中它表示的是 Pod。&lt;code&gt;sandbox&lt;/code&gt; 是一个很宽泛的概念，所以对于其他没有使用容器的运行时仍然是有意义的（比如在一个基于 &lt;code&gt;hypervisor&lt;/code&gt; 的运行时中，sandbox 可能指的就是虚拟机）。&lt;/p&gt;

&lt;p&gt;我们的例子中使用的容器运行时是 Docker，创建 sandbox 时首先创建的是 &lt;code&gt;pause&lt;/code&gt; 容器。pause 容器作为同一个 Pod 中所有其他容器的基础容器，它为 Pod 中的每个业务容器提供了大量的 Pod 级别资源，这些资源都是 Linux 命名空间（包括网络命名空间，IPC 命名空间和 PID 命名空间）。&lt;/p&gt;

&lt;p&gt;pause 容器提供了一种方法来管理所有这些命名空间并允许业务容器共享它们，在同一个网络命名空间中的好处是：同一个 Pod 中的容器可以使用 &lt;code&gt;localhost&lt;/code&gt; 来相互通信。pause 容器的第二个功能与 PID 命名空间的工作方式相关，在 PID 命名空间中，进程之间形成一个树状结构，一旦某个子进程由于父进程的错误而变成了“孤儿进程”，其便会被 &lt;code&gt;init&lt;/code&gt; 进程进行收养并最终回收资源。关于 pause 工作方式的详细信息可以参考：&lt;a href=&#34;https://www.ianlewis.org/en/almighty-pause-container&#34; target=&#34;_blank&#34;&gt;The Almighty Pause Container&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;一旦创建好了 pause 容器，下面就会开始检查磁盘状态然后开始启动业务容器。&lt;/p&gt;

&lt;h3 id=&#34;cni-和-pod-网络&#34;&gt;CNI 和 Pod 网络&lt;/h3&gt;

&lt;p&gt;现在我们的 Pod 已经有了基本的骨架：一个共享所有命名空间以允许业务容器在同一个 Pod 里进行通信的 pause 容器。但现在还有一个问题，那就是容器的网络是如何建立的？&lt;/p&gt;

&lt;p&gt;当 Kubelet 为 Pod 创建网络时，它会将创建网络的任务交给 &lt;code&gt;CNI&lt;/code&gt; 插件。CNI 表示容器网络接口（Container Network Interface），和容器运行时的运行方式类似，它也是一种抽象，允许不同的网络提供商为容器提供不同的网络实现。通过将 json 配置文件（默认在 &lt;code&gt;/etc/cni/net.d&lt;/code&gt; 路径下）中的数据传送到相关的 CNI 二进制文件（默认在 &lt;code&gt;/opt/cni/bin&lt;/code&gt; 路径下）中，cni 插件可以给 pause 容器配置相关的网络，然后 Pod 中其他的容器都使用 pause 容器的网络。下面是一个简单的示例配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;cniVersion&amp;quot;: &amp;quot;0.3.1&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;bridge&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;bridge&amp;quot;,
    &amp;quot;bridge&amp;quot;: &amp;quot;cnio0&amp;quot;,
    &amp;quot;isGateway&amp;quot;: true,
    &amp;quot;ipMasq&amp;quot;: true,
    &amp;quot;ipam&amp;quot;: {
        &amp;quot;type&amp;quot;: &amp;quot;host-local&amp;quot;,
        &amp;quot;ranges&amp;quot;: [
          [{&amp;quot;subnet&amp;quot;: &amp;quot;${POD_CIDR}&amp;quot;}]
        ],
        &amp;quot;routes&amp;quot;: [{&amp;quot;dst&amp;quot;: &amp;quot;0.0.0.0/0&amp;quot;}]
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CNI 插件还会通过 &lt;code&gt;CNI_ARGS&lt;/code&gt; 环境变量为 Pod 指定其他的元数据，包括 Pod 名称和命名空间。&lt;/p&gt;

&lt;p&gt;下面的步骤因 CNI 插件而异，我们以 &lt;code&gt;bridge&lt;/code&gt; 插件举例：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;该插件首先会在根网络命名空间（也就是宿主机的网络命名空间）中设置本地 Linux 网桥，以便为该主机上的所有容器提供网络服务。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;然后它会将一个网络接口（&lt;code&gt;veth&lt;/code&gt; 设备对的一端）插入到 pause 容器的网络命名空间中，并将另一端连接到网桥上。你可以这样来理解 veth 设备对：它就像一根很长的管道，一端连接到容器，一端连接到根网络命名空间中，数据包就在管道中进行传播。&lt;br /&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;接下来 json 文件中指定的 &lt;code&gt;IPAM&lt;/code&gt; Plugin 会为 pause 容器的网络接口分配一个 IP 并设置相应的路由，现在 Pod 就有了自己的 IP。&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;IPAM Plugin 的工作方式和 CNI Plugin 类似：通过二进制文件调用并具有标准化的接口，每一个 IPAM Plugin 都必须要确定容器网络接口的 IP、子网以及网关和路由，并将信息返回给 CNI 插件。最常见的 IPAM Plugin 是 &lt;code&gt;host-local&lt;/code&gt;，它从预定义的一组地址池中为容器分配 IP 地址。它将地址池的信息以及分配信息保存在主机的文件系统中，从而确保了同一主机上每个容器的 IP 地址的唯一性。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;最后 Kubelet 会将集群内部的 &lt;code&gt;DNS&lt;/code&gt; 服务器的 &lt;code&gt;Cluster IP&lt;/code&gt; 地址传给 CNI 插件，然后 CNI 插件将它们写到容器的 &lt;code&gt;/etc/resolv.conf&lt;/code&gt; 文件中。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一旦完成了上面的步骤，CNI 插件就会将操作的结果以 json 的格式返回给 Kubelet。&lt;/p&gt;

&lt;h3 id=&#34;跨主机容器网络&#34;&gt;跨主机容器网络&lt;/h3&gt;

&lt;p&gt;到目前为止，我们已经描述了容器如何与宿主机进行通信，但跨主机之间的容器如何通信呢？&lt;/p&gt;

&lt;p&gt;通常情况下使用 &lt;code&gt;overlay&lt;/code&gt; 网络来进行跨主机容器通信，这是一种动态同步多个主机间路由的方法。 其中最常用的 overlay 网络插件是 &lt;code&gt;flannel&lt;/code&gt;，flannel 具体的工作方式可以参考 &lt;a href=&#34;https://github.com/coreos/flannel&#34; target=&#34;_blank&#34;&gt;CoreOS 的文档&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;容器启动&#34;&gt;容器启动&lt;/h3&gt;

&lt;p&gt;所有网络都配置完成后，接下来就开始真正启动业务容器了！&lt;/p&gt;

&lt;p&gt;一旦 sanbox 完成初始化并处于 &lt;code&gt;active&lt;/code&gt; 状态，Kubelet 就可以开始为其创建容器了。首先&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/5adfb24f8f25a0d57eb9a7b158db46f9f46f0d80/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L690&#34; target=&#34;_blank&#34;&gt;启动 PodSpec 中定义的 init 容器&lt;/a&gt;，然后再启动业务容器。具体过程如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;首先拉取容器的镜像。如果是私有仓库的镜像，就会利用 PodSpec 中指定的 Secret 来拉取该镜像。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;然后通过 CRI 接口创建容器。Kubelet 向 PodSpec 中填充了一个 &lt;code&gt;ContainerConfig&lt;/code&gt; 数据结构（在其中定义了命令，镜像，标签，挂载卷，设备，环境变量等待），然后通过 &lt;code&gt;protobufs&lt;/code&gt; 发送给 CRI 接口。对于 Docker 来说，它会将这些信息反序列化并填充到自己的配置信息中，然后再发送给 &lt;code&gt;Dockerd&lt;/code&gt; 守护进程。在这个过程中，它会将一些元数据标签（例如容器类型，日志路径，dandbox ID 等待）添加到容器中。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;接下来会使用 CPU 管理器来约束容器，这是 Kubelet 1.8 中新添加的 alpha 特性，它使用 &lt;code&gt;UpdateContainerResources&lt;/code&gt; CRI 方法将容器分配给本节点上的 CPU 资源池。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;最后容器开始真正&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/5f9f4a1c5939436fa320e9bc5973a55d6446e59f/pkg/kubelet/kuberuntime/kuberuntime_container.go#L135&#34; target=&#34;_blank&#34;&gt;启动&lt;/a&gt;。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;如果 Pod 中配置了容器生命周期钩子（Hook），容器启动之后就会运行这些 &lt;code&gt;Hook&lt;/code&gt;。Hook 的类型包括两种：&lt;code&gt;Exec&lt;/code&gt;（执行一段命令） 和 &lt;code&gt;HTTP&lt;/code&gt;（发送HTTP请求）。如果 PostStart Hook 启动的时间过长、挂起或者失败，容器将永远不会变成 &lt;code&gt;running&lt;/code&gt; 状态。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;p-id-h2-7-总结-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;7. 总结&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;如果上面一切顺利，现在你的集群上应该会运行三个容器，所有的网络，数据卷和秘钥都被通过 CRI 接口添加到容器中并配置成功。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-8-原文链接-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;8. 原文链接&lt;/p&gt;&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jamiehannaford/what-happens-when-k8s&#34; target=&#34;_blank&#34;&gt;What happens when &amp;hellip; Kubernetes edition!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>英语学习终极秘诀</title>
      <link>https://www.yangcs.net/learn-english/</link>
      <pubDate>Fri, 25 May 2018 06:46:19 +0000</pubDate>
      
      <guid>https://www.yangcs.net/learn-english/</guid>
      <description>

&lt;p&gt;很久很久以前，当网易还在做梦幻西游和大话西游的时候，网易有道教研部诞生了一位大牛，他是&lt;strong&gt;国内最知名，最有影响力的英语学习研究者，新东方口语大赛评委、网易有道教研总监&lt;span id=&#34;inline-purple&#34;&gt;@恶魔奶爸Sam&lt;/span&gt;&lt;/strong&gt;。不过他现在已经不在网易了，可能是不想陪丁磊一起养猪 😄&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/naiba.png?imageView/2/w/300/q/100&#34; alt=&#34;&#34; /&gt;&lt;/center&gt;
&lt;center&gt;- 恶魔奶爸Sam -&lt;/center&gt;
&lt;center&gt;前网易有道教研总监&lt;/center&gt;
&lt;center&gt;新东方全国口语大赛评委&lt;/center&gt;
&lt;center&gt;知乎英语学习领域百万赞答主&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;辜鸿铭曾说过，“今人学英文十年，张目仅能读报，伸手仅能修函。”，那是一百多年前的事情，如今呢？“今人学英文十年，张目不能读报，伸手不能修函。”&lt;/p&gt;

&lt;p&gt;实用能力，可谓一塌糊涂。&lt;/p&gt;

&lt;p&gt;在此之前，我还在英语学习的艰苦道路上徘徊不前，我自己个人在大学时期苦修英文，同样不得其法，非常苦闷，不论怎么学习，英语水平就是局限在某个瓶颈，就是突破不了。&lt;/p&gt;

&lt;p&gt;因为现在的英语材料和英语方法是在太多了，美剧，新闻，新概念，听写，跟读，模仿，仿写，复述，这个名师讲课，那个名师课堂，等等等等，让人眼花缭乱，目不衔接，根本不知道如何选择，所以更加感觉无从下手。&lt;/p&gt;

&lt;p&gt;我以为我的英语学习生涯到此应该就止步了，可就在我将要丧失斗志之际，我读到了奶爸的文章，他那高屋建瓴，纵览全局的英语学习方法论让我感到醍醐灌顶，跳出之前二十年英语学习的误区，重新燃起了斗志！&lt;/p&gt;

&lt;p&gt;至于具体的方法论，由于篇幅有限，再此不多作介绍，本文末尾会放出详细的链接。&lt;/p&gt;

&lt;p&gt;大家可以通过以下几个途径关注他：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://site.douban.com/195274/&#34; target=&#34;_blank&#34;&gt;豆瓣学习小组：奶爸的英语教室&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;微信公众号：恶魔奶爸Sam&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;奶爸的方法论大多借鉴了 &lt;a href=&#34;http://blog.sina.com.cn/u/1264366955&#34; target=&#34;_blank&#34;&gt;伍君仪&lt;/a&gt; 的方法论，并加以改进。而伍君仪的主要方法论都集中在他和奶爸合作写的一本教材《把你的英语用起来》中。本文节选出该教材的前言部分，大家感受一下：&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-奶爸-这是一本游戏指南-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;奶爸：这是一本游戏指南&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;这是一本游戏指南。没错，你没有看错，这就是一本游戏指南。当然，这本指南针对的只是名为“英文”的游戏。&lt;/p&gt;

&lt;p&gt;把英文和电子游戏比较一下，我们会发现，这两者有惊人的相似之处。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第一&lt;/strong&gt;，它们都需要你耗费大量的时间、投入很多精力去练级，磨炼你的技能，唯一不同的是电脑游戏要你练的是魔法和剑术技能，而英文要你练的是听、说、读、写的技能。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第二&lt;/strong&gt;，它们都要求你积累大量的经验值，游戏要求你打怪和做各种任务，而英语要求你完成大量的听力、跟读、阅读、写作等练习。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第三&lt;/strong&gt;，这两者都有严格的等级分别，不同的等级从某种程度上说，代表着你可以在这个世界上“为非作歹”、“为所欲为”的程度。游戏中的等级高，你可以在 PK 时处处取胜，带着美眉做任务，博得她们的芳心。英文等级高了，你可以出国，找份不错的工作，去英语角博得美眉的芳心或者干脆直接去泡外国妞，这两者在这方面真是有异曲同工之妙。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第四&lt;/strong&gt;，不管是打游戏还是学英语，你都必须闯过多重关卡，而每个关卡都有你不得不面对的 boss。不管这个 boss 是名为暗黑破坏神巫妖王，还是四六级托福雅思 GRE，每个关卡你是非过不可，要么你打死 boss，要么被 boss 打死。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第五&lt;/strong&gt;，在游戏级数升到一定级别之后，你都会遇到很严重的瓶颈，级数再往上升会很困难，因为它需要更多的经验值、更多的时间投入。英文同样如此，很多人学到一定层次之后都很容易陷入平台期，不知道如何进一步提高，耗费很多时间却没多大进展，甚至干脆放弃。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第六&lt;/strong&gt;，在打游戏和学英语的过程中，你都有可能发生意外。游戏中，你可能会膝盖中了一箭，而如果英语学得不好，你可能学位证都拿不到，升职机会就抓不住。为了避免这些意外发生，我们都应该好好想一想，用什么样的攻略和方法去达到自己的目标。&lt;/p&gt;

&lt;p&gt;这两者的相似程度如此之高，以至于我们有理由相信：学英语，也可以像玩游戏一样充满乐趣，越玩越想玩！&lt;/p&gt;

&lt;p&gt;因此，本书的目的有四：&lt;/p&gt;

&lt;p&gt;&lt;font color=#0099ff&gt;第一，给出一个完整的英文学习攻略，涵盖了从ABC初级入门，到轻松看懂无字幕影视、原版专业教材的各个等级和阶段。&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;不管你是一个新人还是一个小有名气的高手，我们都会提供循序渐进的、详细且完整的步骤供你参考，而不像任何一本其他攻略那样只告诉你某一个任务关的攻略（比如只偏重听说而没有涵盖读写之类的学习方法指南）。我们涵盖的是全部流程，每一步都稳扎稳打，让你从一名 newbie（菜鸟）变成一个真正的骨灰级高手。&lt;/p&gt;

&lt;p&gt;&lt;font color=#0099ff&gt;第二，教你如何迅速突破英文学习的平台期和瓶颈——这也是本书的核心内容。&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;如果你英文水平还算不错的话，肯定会明白平台期的痛苦——苦学三个月毫无进步，那这本攻略就正是你所需要的。它可以帮你扫清一切学习上的迷思和痛苦，迅速把你带到真正高水平的世界。事实上，这本攻略是如此有效，以至于你在用它来指导自己学习的时候，进步的速度会快得让你自己都感到吃惊。&lt;/p&gt;

&lt;p&gt;&lt;font color=#0099ff&gt;第三，让大家能够坚持下去学好英文——这也是本书最大的特色。&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;讲英文学习的方法有很多，我们在这里不仅仅是告诉大家如何学习，更重要的是，我们还会告诉大家如何去“坚持学习”。换言之，我们会手把手地教会大家正确的陶冶心智的方法，杜绝不当或者错误的自我认知，一步步地通过自我管理和心智培养，来实现人生满足。&lt;/p&gt;

&lt;p&gt;&lt;font color=#0099ff&gt;第四，让大家学英文像打电子游戏一样，充满乐趣，不再为英文头痛，快速通关！&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;我们在书中设置了中级、中高级、听说进阶级、高级四大关卡，让大家好像打游戏一样来闯关得分，提高自己的英文水准！这四大关卡分别涵盖了英文的听力、口语交际、原版书阅读、语法、词汇、写作等各个部分，大家在学习自己喜欢和感兴趣的材料时，不知不觉中这些能力都能够得到高效的提高&lt;/p&gt;

&lt;p&gt;正是抱着这样的目的，伍君仪老师和不才二人汇总了各自的一些粗浅经验，写出了这一本英文游戏攻略。我们虽不算是功力震古烁今的绝顶高手，但作为浸淫英文游戏多年的过来人，也算是略有心得。在这本书中，我们竭尽所能，将这个名为“英文”的游戏中的各种法门、陷阱、弯路一一道来，让诸君的英文能力升级像开外挂那么迅速，一路闯过各种关卡，灭掉名为“英文听、说、读、写”的四大魔王也好，打败各类名为“四六级”的boss也好，击退各种名为“英文会议”或是“英文面试”的小卒也好，请拿出你的汗水、勇气、智慧去努力奋斗，直至你有能力去挑战名为“流利英语，母语水平”的超级大魔王，让“美女剑豪带着酒来了”的热血故事广为流传！&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-福利-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;福利&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;本书完整版可以到当当或亚马逊购买，我这里也提供了免费的电子版：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://default-1252251317.cossh.myqcloud.com/%E6%8A%8A%E4%BD%A0%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%94%A8%E8%B5%B7%E6%9D%A5.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://default-1252251317.cossh.myqcloud.com/%E6%8A%8A%E4%BD%A0%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%94%A8%E8%B5%B7%E6%9D%A5.epub&#34; target=&#34;_blank&#34;&gt;epub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;关于本书中推荐的 Podcast: 《A Day in the life of Jeff》，我已经将其录音材料免费分享到互联网上，同时也将录音内容整理成电子书，大家可以免费下载：&lt;/p&gt;

&lt;h4 id=&#34;录音材料&#34;&gt;录音材料&lt;/h4&gt;

&lt;p&gt;&lt;a id=&#34;download&#34; href=&#34;https://pan.baidu.com/s/1K0v274ULsghYmjjyWe93Bw&#34;&gt;&lt;i class=&#34;fa fa-download&#34;&gt;&lt;/i&gt;&lt;span&gt; Download Now&lt;/span&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;电子书&#34;&gt;电子书&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.yangcs.net/a-day-in-the-life-of-jeff/&#34; target=&#34;_blank&#34;&gt;在线阅读&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://default-1252251317.cossh.myqcloud.com/a-day-in-the-life-of-jeff.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://default-1252251317.cossh.myqcloud.com/a-day-in-the-life-of-jeff.epub&#34; target=&#34;_blank&#34;&gt;epub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://default-1252251317.cossh.myqcloud.com/a-day-in-the-life-of-jeff.mobi&#34; target=&#34;_blank&#34;&gt;mobi&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果后续还有整理好的福利，将会在这里补充 😊&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;
</description>
    </item>
    
    <item>
      <title>走进 Descheduler</title>
      <link>https://www.yangcs.net/posts/introduce-kubernetes-descheduler/</link>
      <pubDate>Wed, 23 May 2018 10:23:29 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/introduce-kubernetes-descheduler/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kube-scheduler&lt;/code&gt; 是 Kubernetes 中负责调度的组件，它本身的调度功能已经很强大了。但由于 Kubernetes 集群非常活跃，它的状态会随时间而改变，由于各种原因，你可能需要将已经运行的 Pod 移动到其他节点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;某些节点负载过高&lt;/li&gt;
&lt;li&gt;某些资源对象被添加了 &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature&#34; target=&#34;_blank&#34;&gt;node 亲和性&lt;/a&gt; 或 &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature&#34; target=&#34;_blank&#34;&gt;pod （反）亲和性&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;集群中加入了新节点&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一旦 Pod 启动之后 &lt;code&gt;kube-scheduler&lt;/code&gt; 便不会再尝试重新调度它。根据环境的不同，你可能会有很多需要手动调整 Pod 的分布，例如：如果集群中新加入了一个节点，那么已经运行的 Pod 并不会被分摊到这台节点上，这台节点可能只运行了少量的几个 Pod，这并不理想，对吧？&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-descheduler-如何工作-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. Descheduler 如何工作？&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/descheduler&#34; target=&#34;_blank&#34;&gt;Descheduler&lt;/a&gt; 会检查 Pod 的状态，并根据自定义的策略将不满足要求的 Pod 从该节点上驱逐出去。Descheduler 并不是 &lt;code&gt;kube-scheduler&lt;/code&gt; 的替代品，而是要依赖于它。该项目目前放在 Kubernetes 的孵化项目中，还没准备投入生产，但经过我实验发现它的运行效果很好，而且非常稳定。那么该如何安装呢？&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-2-部署方法-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. 部署方法&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;你可以通过 &lt;code&gt;Job&lt;/code&gt; 或 &lt;code&gt;CronJob&lt;/code&gt; 来运行 descheduler。我已经创建了一个镜像 &lt;code&gt;komljen/descheduler:v0.5.0-4-ga7ceb671&lt;/code&gt;（包含在下面的 yaml 文件中），但由于这个项目的更新速度很快，你可以通过以下的命令创建你自己的镜像：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/kubernetes-incubator/descheduler
$ cd descheduler &amp;amp;&amp;amp; make image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后打好标签 push 到自己的镜像仓库中。&lt;/p&gt;

&lt;p&gt;通过我创建的 chart 模板，你可以用 &lt;code&gt;Helm&lt;/code&gt; 来部署 descheduler，该模板支持 RBAC 并且已经在 Kubernetes v1.9 上测试通过。&lt;/p&gt;

&lt;p&gt;添加我的 helm 私有仓库，然后部署 descheduler：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm repo add akomljen-charts \
  https://raw.githubusercontent.com/komljen/helm-charts/master/charts/
  
$ helm install --name ds \
  --namespace kube-system \
  akomljen-charts/descheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以不使用 helm，通过手动部署。首先创建 serviceaccount 和 clusterrolebinding：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create a cluster role
$ cat &amp;lt;&amp;lt; EOF| kubectl create -n kube-system -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: descheduler
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  resources: [&amp;quot;nodes&amp;quot;]
  verbs: [&amp;quot;get&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;list&amp;quot;]
- apiGroups: [&amp;quot;&amp;quot;]
  resources: [&amp;quot;pods&amp;quot;]
  verbs: [&amp;quot;get&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;delete&amp;quot;]
- apiGroups: [&amp;quot;&amp;quot;]
  resources: [&amp;quot;pods/eviction&amp;quot;]
  verbs: [&amp;quot;create&amp;quot;]
EOF

# Create a service account
$ kubectl create sa descheduler -n kube-system

# Bind the cluster role to the service account
$ kubectl create clusterrolebinding descheduler \
    -n kube-system \
    --clusterrole=descheduler \
    --serviceaccount=kube-system:descheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后通过 &lt;code&gt;configmap&lt;/code&gt; 创建 descheduler 策略。目前只支持四种策略：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/descheduler#removeduplicates&#34; target=&#34;_blank&#34;&gt;RemoveDuplicates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/descheduler#lownodeutilization&#34; target=&#34;_blank&#34;&gt;LowNodeUtilization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/descheduler#removepodsviolatinginterpodantiaffinity&#34; target=&#34;_blank&#34;&gt;RemovePodsViolatingInterPodAntiAffinity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/descheduler#removepodsviolatingnodeaffinity&#34; target=&#34;_blank&#34;&gt;RemovePodsViolatingNodeAffinity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;默认这四种策略全部开启，你可以根据需要关闭它们。下面在 &lt;code&gt;kube-suystem&lt;/code&gt; 命名空间中创建一个 configmap：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;$ cat &amp;lt;&amp;lt; EOF| kubectl create -n kube-system -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler
data:
  policy.yaml: |-  
    apiVersion: descheduler/v1alpha1
    kind: DeschedulerPolicy
    strategies:
      RemoveDuplicates:
         enabled: false
      LowNodeUtilization:
         enabled: true
         params:
           nodeResourceUtilizationThresholds:
             thresholds:
               cpu: 20
               memory: 20
               pods: 20
             targetThresholds:
               cpu: 50
               memory: 50
               pods: 50
      RemovePodsViolatingInterPodAntiAffinity:
        enabled: true
      RemovePodsViolatingNodeAffinity:
        enabled: true
        params:
          nodeAffinityType:
          - requiredDuringSchedulingIgnoredDuringExecution
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;kube-system&lt;/code&gt; 命名空间中创建一个 CronJob：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;$ cat &amp;lt;&amp;lt; EOF| kubectl create -n kube-system -f -
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: descheduler
spec:
  schedule: &amp;quot;*/30 * * * *&amp;quot;
  jobTemplate:
    metadata:
      name: descheduler
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: &amp;quot;true&amp;quot;
    spec:
      template:
        spec:
          serviceAccountName: descheduler
          containers:
          - name: descheduler
            image: komljen/descheduler:v0.5.0-4-ga7ceb671
            volumeMounts:
            - mountPath: /policy-dir
              name: policy-volume
            command:
            - /bin/descheduler
            - --v=4
            - --max-pods-to-evict-per-node=10
            - --policy-config-file=/policy-dir/policy.yaml
          restartPolicy: &amp;quot;OnFailure&amp;quot;
          volumes:
          - name: policy-volume
            configMap:
              name: descheduler
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get cronjobs -n kube-system

NAME             SCHEDULE       SUSPEND   ACTIVE    LAST SCHEDULE   AGE
descheduler      */30 * * * *   False     0         2m              32m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该 CroJob 每 30 分钟运行一次，当 CronJob 开始工作后，可以通过以下命令查看已经成功结束的 Pod：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get pods -n kube-system -a | grep Completed

descheduler-1525520700-297pq          0/1       Completed   0          1h
descheduler-1525521000-tz2ch          0/1       Completed   0          32m
descheduler-1525521300-mrw4t          0/1       Completed   0          2m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以查看这些 Pod 的日志，然后根据需要调整 descheduler 策略：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl logs descheduler-1525521300-mrw4t -n kube-system

I0505 11:55:07.554195       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from github.com/kubernetes-incubator/descheduler/pkg/descheduler/node/node.go:84
I0505 11:55:07.554255       1 reflector.go:240] Listing and watching *v1.Node from github.com/kubernetes-incubator/descheduler/pkg/descheduler/node/node.go:84
I0505 11:55:07.767903       1 lownodeutilization.go:147] Node &amp;quot;ip-10-4-63-172.eu-west-1.compute.internal&amp;quot; is appropriately utilized with usage: api.ResourceThresholds{&amp;quot;cpu&amp;quot;:41.5, &amp;quot;memory&amp;quot;:1.3635487207675927, &amp;quot;pods&amp;quot;:8.181818181818182}
I0505 11:55:07.767942       1 lownodeutilization.go:149] allPods:9, nonRemovablePods:9, bePods:0, bPods:0, gPods:0
I0505 11:55:07.768141       1 lownodeutilization.go:144] Node &amp;quot;ip-10-4-36-223.eu-west-1.compute.internal&amp;quot; is over utilized with usage: api.ResourceThresholds{&amp;quot;cpu&amp;quot;:48.75, &amp;quot;memory&amp;quot;:61.05259502942694, &amp;quot;pods&amp;quot;:30}
I0505 11:55:07.768156       1 lownodeutilization.go:149] allPods:33, nonRemovablePods:12, bePods:1, bPods:19, gPods:1
I0505 11:55:07.768376       1 lownodeutilization.go:144] Node &amp;quot;ip-10-4-41-14.eu-west-1.compute.internal&amp;quot; is over utilized with usage: api.ResourceThresholds{&amp;quot;cpu&amp;quot;:39.125, &amp;quot;memory&amp;quot;:98.19259268881142, &amp;quot;pods&amp;quot;:33.63636363636363}
I0505 11:55:07.768390       1 lownodeutilization.go:149] allPods:37, nonRemovablePods:8, bePods:0, bPods:29, gPods:0
I0505 11:55:07.768538       1 lownodeutilization.go:147] Node &amp;quot;ip-10-4-34-29.eu-west-1.compute.internal&amp;quot; is appropriately utilized with usage: api.ResourceThresholds{&amp;quot;memory&amp;quot;:43.19826999287199, &amp;quot;pods&amp;quot;:30.90909090909091, &amp;quot;cpu&amp;quot;:35.25}
I0505 11:55:07.768552       1 lownodeutilization.go:149] allPods:34, nonRemovablePods:11, bePods:8, bPods:15, gPods:0
I0505 11:55:07.768556       1 lownodeutilization.go:65] Criteria for a node under utilization: CPU: 20, Mem: 20, Pods: 20
I0505 11:55:07.768571       1 lownodeutilization.go:69] No node is underutilized, nothing to do here, you might tune your thersholds further
I0505 11:55:07.768576       1 pod_antiaffinity.go:45] Processing node: &amp;quot;ip-10-4-63-172.eu-west-1.compute.internal&amp;quot;
I0505 11:55:07.779313       1 pod_antiaffinity.go:45] Processing node: &amp;quot;ip-10-4-36-223.eu-west-1.compute.internal&amp;quot;
I0505 11:55:07.796766       1 pod_antiaffinity.go:45] Processing node: &amp;quot;ip-10-4-41-14.eu-west-1.compute.internal&amp;quot;
I0505 11:55:07.813303       1 pod_antiaffinity.go:45] Processing node: &amp;quot;ip-10-4-34-29.eu-west-1.compute.internal&amp;quot;
I0505 11:55:07.829109       1 node_affinity.go:40] Executing for nodeAffinityType: requiredDuringSchedulingIgnoredDuringExecution
I0505 11:55:07.829133       1 node_affinity.go:45] Processing node: &amp;quot;ip-10-4-63-172.eu-west-1.compute.internal&amp;quot;
I0505 11:55:07.840416       1 node_affinity.go:45] Processing node: &amp;quot;ip-10-4-36-223.eu-west-1.compute.internal&amp;quot;
I0505 11:55:07.856735       1 node_affinity.go:45] Processing node: &amp;quot;ip-10-4-41-14.eu-west-1.compute.internal&amp;quot;
I0505 11:55:07.945566       1 request.go:480] Throttling request took 88.738917ms, request: GET:https://100.64.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-4-41-14.eu-west-1.compute.internal%2Cstatus.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded
I0505 11:55:07.972702       1 node_affinity.go:45] Processing node: &amp;quot;ip-10-4-34-29.eu-west-1.compute.internal&amp;quot;
I0505 11:55:08.145559       1 request.go:480] Throttling request took 172.751657ms, request: GET:https://100.64.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-4-34-29.eu-west-1.compute.internal%2Cstatus.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded
I0505 11:55:08.160964       1 node_affinity.go:72] Evicted 0 pods
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;哇哦，现在你的集群中已经运行了一个 descheduler！&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-总结-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 总结&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;Kubernetes 的默认调度器已经做的很好，但由于集群处于不断变化的状态中，某些 Pod 可能运行在错误的节点上，或者你想要均衡集群资源的分配，这时候就需要 descheduler 来帮助你将某些节点上的 Pod 驱逐到正确的节点上去。我很期待正式版的发布！&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-4-原文链接-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. 原文链接&lt;/p&gt;&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://akomljen.com/meet-a-kubernetes-descheduler/&#34; target=&#34;_blank&#34;&gt;Meet a Kubernetes Descheduler&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>Pod 的生命周期管理</title>
      <link>https://www.yangcs.net/posts/pods-life/</link>
      <pubDate>Thu, 03 May 2018 12:08:01 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/pods-life/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;本文我们将从实践者的角度仔细研究整个pod生命周期，包括如何影响启动和关闭行为，并通过实践来理解对应用程序健康状况的检查。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-pod-的生命周期-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. Pod 的生命周期&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;pod-phase&#34;&gt;Pod phase&lt;/h3&gt;

&lt;p&gt;Pod 的 status 在信息保存在 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L2471&#34; target=&#34;_blank&#34;&gt;PodStatus&lt;/a&gt; 中定义，其中有一个 phase 字段。&lt;/p&gt;

&lt;p&gt;Pod 的相位（phase）是 Pod 在其生命周期中的简单宏观概述。该阶段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机。&lt;/p&gt;

&lt;p&gt;Pod 相位的数量和含义是严格指定的。除了本文档中列举的状态外，不应该再假定 Pod 有其他的 phase 值。&lt;/p&gt;

&lt;p&gt;无论你是手动创建 Pod，还是通过 &lt;code&gt;deployment&lt;/code&gt;、&lt;code&gt;daemonset&lt;/code&gt; 或 &lt;code&gt;statefulset&lt;/code&gt;来创建，Pod 的 phase 都有以下几个可能的值：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;挂起（Pending）&lt;/strong&gt;：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;运行中（Running）&lt;/strong&gt;：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;成功（Successed）&lt;/strong&gt;：Pod 中的所有容器都被成功终止，并且不会再重启。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;失败（Failed）&lt;/strong&gt;：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;未知（Unkonwn）&lt;/strong&gt;：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下图是 Pod 的生命周期示意图，从图中可以看到 Pod 状态的变化。&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/kubernetes-pod-life-cycle.jpg&#34; alt=&#34;&#34; /&gt;&lt;/center&gt;
&lt;center&gt;图片 - Pod的生命周期示意图&lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&#34;pod-状态&#34;&gt;Pod 状态&lt;/h3&gt;

&lt;p&gt;Pod 有一个 PodStatus 对象，其中包含一个 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L1964&#34; target=&#34;_blank&#34;&gt;PodCondition&lt;/a&gt; 数组。 PodCondition 数组的每个元素都有一个 type 字段和一个 status 字段。type 字段是字符串，可能的值有 &lt;code&gt;PodScheduled&lt;/code&gt;、&lt;code&gt;Ready&lt;/code&gt;、&lt;code&gt;Initialized&lt;/code&gt; 和 &lt;code&gt;Unschedulable&lt;/code&gt;。status 字段是一个字符串，可能的值有 &lt;code&gt;True&lt;/code&gt;、&lt;code&gt;False&lt;/code&gt; 和 &lt;code&gt;Unknown&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;当你通过 &lt;code&gt;kubectl get pod&lt;/code&gt; 查看 Pod 时，&lt;code&gt;STATUS&lt;/code&gt; 这一列可能会显示与上述5个状态不同的值，例如 &lt;code&gt;Init:0/1&lt;/code&gt; 和 &lt;code&gt;CrashLoopBackOff&lt;/code&gt;。这是因为 Pod 状态的定义除了包含 phase 之外，还有 &lt;code&gt;InitContainerStatuses&lt;/code&gt; 和 &lt;code&gt;containerStatuses&lt;/code&gt; 等其他字段，具体代码参考 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L2471&#34; target=&#34;_blank&#34;&gt;overall status of a pod&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;如果想知道究竟发生了什么，可以通过命令 &lt;code&gt;kubectl describe pod/$PODNAME&lt;/code&gt; 查看输出信息的 &lt;code&gt;Events&lt;/code&gt; 条目。通过 Events 条目可以看到一些具体的信息，比如正在拉取容器镜像，Pod 已经被调度，或者某个 container 处于 unhealthy 状态。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-2-pod-的启动关闭流程-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. Pod 的启动关闭流程&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;下面通过一个具体的示例来探究一下 Pod 的整个生命周期流程。为了确定事情发生的顺序，通过下面的 manifest 来部署一个 deployment。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kind:                   Deployment
apiVersion:             apps/v1beta1
metadata:
  name:                 loap
spec:
  replicas:             1
  template:
    metadata:
      labels:
        app:            loap
    spec:
      initContainers:
      - name:           init
        image:          busybox
        command:       [&#39;sh&#39;, &#39;-c&#39;, &#39;echo $(date +%s): INIT &amp;gt;&amp;gt; /loap/timing&#39;]
        volumeMounts:
        - mountPath:    /loap
          name:         timing
      containers:
      - name:           main
        image:          busybox
        command:       [&#39;sh&#39;, &#39;-c&#39;, &#39;echo $(date +%s): START &amp;gt;&amp;gt; /loap/timing;
sleep 10; echo $(date +%s): END &amp;gt;&amp;gt; /loap/timing;&#39;]
        volumeMounts:
        - mountPath:    /loap
          name:         timing
        livenessProbe:
          exec:
            command:   [&#39;sh&#39;, &#39;-c&#39;, &#39;echo $(date +%s): LIVENESS &amp;gt;&amp;gt; /loap/timing&#39;]
        readinessProbe:
          exec:
            command:   [&#39;sh&#39;, &#39;-c&#39;, &#39;echo $(date +%s): READINESS &amp;gt;&amp;gt; /loap/timing&#39;]
        lifecycle:
          postStart:
            exec:
              command:   [&#39;sh&#39;, &#39;-c&#39;, &#39;echo $(date +%s): POST-START &amp;gt;&amp;gt; /loap/timing&#39;]
          preStop:
            exec:
              command:  [&#39;sh&#39;, &#39;-c&#39;, &#39;echo $(date +%s): PRE-HOOK &amp;gt;&amp;gt; /loap/timing&#39;]
      volumes:
      - name:           timing
        hostPath:
          path:         /tmp/loap
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等待 Pod 状态变为 &lt;code&gt;Running&lt;/code&gt; 之后，通过以下命令来强制停止 Pod：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl scale deployment loap --replicas=0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 &lt;code&gt;/tmp/loap/timing&lt;/code&gt; 文件的内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ cat /tmp/loap/timing

1525334577: INIT
1525334581: START
1525334581: POST-START
1525334584: READINESS
1525334584: LIVENESS
1525334588: PRE-HOOK
1525334589: END
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;/tmp/loap/timing&lt;/code&gt; 文件的内容很好地体现了 Pod 的启动和关闭流程，具体过程如下：&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/loap.png&#34; alt=&#34;&#34; /&gt;&lt;/center&gt;
&lt;center&gt;图片 - Pod 的启动和关闭流程&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;首先启动一个 Infra 容器（又叫 Pause 容器），用来和 Pod 中的其他容器共享 linux 命名空间，并开启 init 进程。（上图中忽略了这一步）&lt;/li&gt;
&lt;li&gt;然后启动 Init 容器，它是一种专用的容器，在应用程序容器启动之前运行，用来对 Pod 进行一些初始化操作，并包括一些应用镜像中不存在的实用工具和安装脚本。&lt;/li&gt;
&lt;li&gt;4 秒之后，应用程序容器和 &lt;code&gt;post-start hook&lt;/code&gt; 同时启动。&lt;/li&gt;
&lt;li&gt;7 秒之后开始启动 &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/&#34; target=&#34;_blank&#34;&gt;liveness 和 readiness 探针&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;11 秒之后，通过手动杀掉 Pod，&lt;code&gt;pre-stop hook&lt;/code&gt; 执行，优雅删除期限过期后（默认是 30 秒），应用程序容器停止。实际的 Pod 终止过程要更复杂，具体参考 &lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/concepts/pod.html&#34; target=&#34;_blank&#34;&gt;Pod 的终止&lt;/a&gt;。&lt;/li&gt;
&lt;/ol&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;必须主动杀掉 Pod 才会触发 &lt;code&gt;pre-stop hook&lt;/code&gt;，如果是 Pod 自己 Down 掉，则不会执行 &lt;code&gt;pre-stop hook&lt;/code&gt;。&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-如何快速-debug-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 如何快速 DEBUG&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;当 Pod 出现致命的错误时，如果能够快速 DEBUG，将会帮助我们快速定位问题。为了实现这个目的，可以把把致命事件的信息通过 &lt;code&gt;.spec.terminationMessagePath&lt;/code&gt; 配置写入指定位置的文件，就像打印错误、异常和堆栈信息一样。该位置的内容可以很方便的通过 dashboards、监控软件等工具检索和展示，默认路径为 &lt;code&gt;/dev/termination-log&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;以下是一个小例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# termination-demo.yaml
apiVersion: v1
kind: Pod
metadata:
  name: termination-demo
spec:
  containers:
  - name: termination-demo-container
    image: alpine
    command: [&amp;quot;/bin/sh&amp;quot;]
    args: [&amp;quot;-c&amp;quot;, &amp;quot;sleep 10 &amp;amp;&amp;amp; echo Sleep expired &amp;gt; /dev/termination-log&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些消息的最后部分会使用其他的规定来单独存储：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl create -f termination-demo.yaml

$ sleep 20

$ kubectl get pod termination-demo -o go-template=&#39;{{range .status.containerStatuses}}{{.lastState.terminated.message}}{{end}}&#39;

Sleep expired

$ kubectl get pod termination-demo -o go-template=&#39;{{range .status.containerStatuses}}{{.lastState.terminated.exitCode}}{{end}}&#39;

0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-4-参考-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. 参考&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/concepts/pod-hook.html&#34; target=&#34;_blank&#34;&gt;Pod hook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.openshift.com/kubernetes-pods-life/&#34; target=&#34;_blank&#34;&gt;Kubernetes: A Pod’s Life&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/&#34; target=&#34;_blank&#34;&gt;确定 Pod 失败的原因&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>Kube-router 实战</title>
      <link>https://www.yangcs.net/posts/kube-router/</link>
      <pubDate>Fri, 20 Apr 2018 04:36:40 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/kube-router/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cloudnativelabs/kube-router&#34; target=&#34;_blank&#34;&gt;Kube-router&lt;/a&gt; 是一个挺有想法的项目，兼备了 &lt;code&gt;calico&lt;/code&gt; 和 &lt;code&gt;kube-proxy&lt;/code&gt; 的功能，是基于 Kubernetes 网络设计的一个集负载均衡器、防火墙和容器网络的综合方案。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-体系架构-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. 体系架构&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;Kube-router 是围绕 &lt;span id=&#34;inline-blue&#34;&gt;观察者&lt;/span&gt; 和 &lt;span id=&#34;inline-blue&#34;&gt;控制器&lt;/span&gt; 的概念而建立的。&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;观察者&lt;/span&gt; 使用 &lt;code&gt;Kubernetes watch API&lt;/code&gt; 来获取与创建，更新和删除 Kubernetes 对象有关的事件的通知。 每个观察者获取与特定 API 对象相关的通知。 在从 API 服务器接收事件时，观察者广播事件。&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;控制器&lt;/span&gt; 注册以获取观察者的事件更新，并处理事件。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Kube-router&lt;/code&gt; 由3个核心控制器和多个观察者组成，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/kube-router-arch.png&#34; alt=&#34;&#34; /&gt;&lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&#34;流程分析&#34;&gt;流程分析&lt;/h3&gt;

&lt;p&gt;Kube-router 启动之后，首先创建 &lt;code&gt;wathcer&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (kr *KubeRouter) Run() error {
	...
	err = kr.startApiWatchers()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;startApiWatchers&lt;/code&gt; 中，会启动 endpoint、namespace、pod、node、networkpolicy、service 这六个 wather。&lt;/p&gt;

&lt;p&gt;这六个 wathcer 将监听的变化发送到 &lt;code&gt;Broadcaster&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func NewBroadcaster() *Broadcaster {
	return &amp;amp;Broadcaster{}
}

func (b *Broadcaster) Add(listener Listener) {
	b.listenerLock.Lock()
	defer b.listenerLock.Unlock()
	b.listeners = append(b.listeners, listener)
}

func (b *Broadcaster) Notify(instance interface{}) {
	b.listenerLock.RLock()
	listeners := b.listeners
	b.listenerLock.RUnlock()
	for _, listener := range listeners {
		go listener.OnUpdate(instance)
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后创建三个 controller：&lt;code&gt;NetworkPolicyController&lt;/code&gt;、&lt;code&gt;NetworkRoutingController&lt;/code&gt;、&lt;code&gt;NetworkServicesControllers&lt;/code&gt;。 每个 controller 会监听所关心的资源的变化。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func NewNetworkServicesController(clientset *kubernetes.Clientset,\
	config *options.KubeRouterConfig) (*NetworkServicesController, error) {
	...
	nsc := NetworkServicesController{}
	...
	watchers.EndpointsWatcher.RegisterHandler(&amp;amp;nsc)
	watchers.ServiceWatcher.RegisterHandler(&amp;amp;nsc)
	...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个 &lt;a href=&#34;https://github.com/cloudnativelabs/kube-router/tree/master/pkg/controllers&#34; target=&#34;_blank&#34;&gt;controller&lt;/a&gt; 遵循以下结构。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func Run() {
    for {
        Sync() // control loop that runs for ever and perfom sync at periodic interval
    }
}

func OnUpdate() {
    Sync() // on receiving update of a watched API object (namespace, node, pod, network policy etc)
}

Sync() {
    //re-concile any state changes
}

Cleanup() {
    // cleanup any changes (to iptables, ipvs, network etc) done to the system
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-id-h2-2-主要功能-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. 主要功能&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;基于-ipvs-lvs-的负载均衡器-run-service-proxy&#34;&gt;基于 IPVS/LVS 的负载均衡器 | &lt;code&gt;--run-service-proxy&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;Kube-router&lt;/code&gt; 采用 Linux 内核的 &lt;code&gt;IPVS&lt;/code&gt; 模块为 K8s 提供 &lt;code&gt;Service&lt;/code&gt; 的代理。&lt;/p&gt;

&lt;p&gt;Kube-router 的负载均衡器功能，会在物理机上创建一个虚拟的 &lt;code&gt;kube-dummy-if&lt;/code&gt; 网卡，然后利用 k8s 的 watch APi 实时更新 &lt;code&gt;svc&lt;/code&gt; 和 &lt;code&gt;ep&lt;/code&gt; 的信息。svc 的 &lt;code&gt;cluster_ip&lt;/code&gt; 会绑定在 kube-dummy-if 网卡上，作为 lvs 的 &lt;code&gt;virtual server&lt;/code&gt; 的地址。&lt;code&gt;realserver&lt;/code&gt; 的 ip 则通过 ep 获取到容器的IP地址。&lt;/p&gt;

&lt;p&gt;基于 Kubernetes 网络服务代理的 Kube-router IPVS 演示&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://asciinema.org/a/120312&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://asciinema.org/a/120312.png&#34; alt=&#34;asciicast&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;特征：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;轮询负载均衡&lt;/li&gt;
&lt;li&gt;基于客户端IP的会话保持&lt;/li&gt;
&lt;li&gt;如果服务控制器与网络路由控制器（带有 &lt;code&gt;–-run-router&lt;/code&gt; 标志的 kube-router）一起使用，源IP将被保留&lt;/li&gt;
&lt;li&gt;用 &lt;code&gt;–-masquerade-all&lt;/code&gt; 参数明确标记伪装(SNAT)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更多详情可以参考：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://link.jianshu.com/?t=https://cloudnativelabs.github.io/post/2017-05-10-kube-network-service-proxy/&#34; target=&#34;_blank&#34;&gt;Kubernetes network services prox with IPVS/LVS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://link.jianshu.com/?t=https://blog.codeship.com/kernel-load-balancing-for-docker-containers-using-ipvs/&#34; target=&#34;_blank&#34;&gt;Kernel Load-Balancing for Docker Containers Using IPVS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.yangcs.net/posts/lvs-persistent-connection/&#34; target=&#34;_blank&#34;&gt;LVS负载均衡之持久性连接介绍&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;容器网络-run-router&#34;&gt;容器网络 | &lt;code&gt;--run-router&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;Kube-router 利用 BGP 协议和 Go 的 &lt;code&gt;GoBGP&lt;/code&gt; 库和为容器网络提供直连的方案。因为用了原生的 Kubernetes API 去构建容器网络，意味着在使用 kube-router 时，不需要在你的集群里面引入其他依赖。&lt;/p&gt;

&lt;p&gt;同样的，kube-router 在引入容器 CNI 时也没有其它的依赖，官方的 &lt;code&gt;bridge&lt;/code&gt; 插件就能满足 kube-rouetr 的需求。&lt;/p&gt;

&lt;p&gt;更多关于 BGP 协议在 Kubernetes 中的使用可以参考：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://link.jianshu.com/?t=https://cloudnativelabs.github.io/post/2017-05-22-kube-pod-networking/&#34; target=&#34;_blank&#34;&gt;Kubernetes pod networking and beyond with BGP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;网络策略管理-run-firewall&#34;&gt;网络策略管理 | &lt;code&gt;--run-firewall&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;网络策略控制器负责从 Kubernetes API 服务器读取命名空间、网络策略和 pod 信息，并相应地使用 &lt;code&gt;ipset&lt;/code&gt; 配置 iptables 以向 pod 提供入口过滤，保证防火墙的规则对系统性能有较低的影响。&lt;/p&gt;

&lt;p&gt;Kube-router 支持 &lt;code&gt;networking.k8s.io/NetworkPolicy&lt;/code&gt; 接口或网络策略 V1/GA &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/39164#issue-197243974&#34; target=&#34;_blank&#34;&gt;semantics&lt;/a&gt; 以及网络策略的 beta 语义。&lt;/p&gt;

&lt;p&gt;更多关于 kube-router 防火墙的功能可以参考：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://link.jianshu.com/?t=https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/&#34; target=&#34;_blank&#34;&gt;Enforcing Kubernetes network policies with iptables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;p-id-h2-3-使用-kube-router-替代-kube-proxy-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 使用 kube-router 替代 kube-proxy&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;下面进入实战阶段，本方案只使用 kube-router 的 &lt;code&gt;service-proxy&lt;/code&gt; 功能，网络插件仍然使用 &lt;code&gt;calico&lt;/code&gt;（估计只有我能想到这么奇葩的组合了 ✌️）&lt;/p&gt;

&lt;h3 id=&#34;前提&#34;&gt;前提&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;已有一个 k8s 集群&lt;/li&gt;
&lt;li&gt;kube-router 能够连接 &lt;code&gt;apiserver&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;如果您选择以 &lt;code&gt;daemonset&lt;/code&gt; 运行 kube-router，那么 kube-apiserver 和 kubelet 必须以 &lt;code&gt;–allow-privileged=true&lt;/code&gt; 选项运行&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;集群环境&#34;&gt;集群环境&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;角色&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;IP 地址&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;主机名&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;k8s master&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;192.168.123.250&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;node1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;k8s node&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;192.168.123.248&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;node2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;k8s node&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;192.168.123.249&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;node3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;安装步骤&#34;&gt;安装步骤&lt;/h3&gt;

&lt;p&gt;如果你正在使用 &lt;code&gt;kube-proxy&lt;/code&gt;，需要先停止 kube-proxy 服务，并且删除相关 iptables 规则。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl stop kube-proxy
$ kube-proxy --cleanup-iptables
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来以 &lt;code&gt;daemonset&lt;/code&gt; 运行 kube-router，这里我们使用 DR 模式。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl --namespace=kube-system create configmap kube-proxy  --from-file=kubeconfig.conf=/root/.kube/config
$ wget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features-dsr.yaml

# 将 kubeadm-kuberouter-all-features-dsr.yaml 里的 --run-router 参数和 --run-firewall 参数的值改为 false
$ kubectl create -f kubeadm-kuberouter-all-features-dsr.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在每台机器上查看 lvs 条目&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ipvsadm -Ln

IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&amp;gt; 192.168.123.250:6443         Masq    1      0          0
  
$ ipvsadm -S -n

-A -t 10.254.0.1:443 -s rr -p 10800
-a -t 10.254.0.1:443 -r 192.168.123.250:6443 -m -w 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出，kube-router 使用的是 lvs 的 nat 模式。&lt;/p&gt;

&lt;h3 id=&#34;创建一个应用测试-kube-router&#34;&gt;创建一个应用测试 kube-router&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl run whats-my-ip --image=cloudnativelabs/whats-my-ip --replicas=3

# 暴露服务
$ kubectl expose deploy whats-my-ip --target-port=8080 --port=8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看创建好的服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get pods -owide

NAME                           READY     STATUS    RESTARTS   AGE       IP               NODE
whats-my-ip-845d4ff4f6-d2ptz   1/1       Running   0          23h       172.20.135.8     192.168.123.249
whats-my-ip-845d4ff4f6-jxzzn   1/1       Running   0          23h       172.20.166.130   192.168.123.250
whats-my-ip-845d4ff4f6-szhhd   1/1       Running   0          34s       172.20.104.9     192.168.123.248

$ kubectl get svc

NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
kubernetes    ClusterIP   10.254.0.1       &amp;lt;none&amp;gt;        443/TCP    45d
whats-my-ip   ClusterIP   10.254.108.117   &amp;lt;none&amp;gt;        8080/TCP   16s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 lvs 规则条目&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ipvsadm -Ln

IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&amp;gt; 192.168.123.250:6443         Masq    1      0          0
TCP  10.254.175.147:8080 rr
  -&amp;gt; 172.20.104.9:8080            Masq    1      0          0
  -&amp;gt; 172.20.135.8:8080            Masq    1      0          0
  -&amp;gt; 172.20.166.130:8080          Masq    1      0          0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以发现本机的 &lt;code&gt;Cluster IP&lt;/code&gt; 代理后端真实 &lt;code&gt;Pod IP&lt;/code&gt;，使用 rr 算法。&lt;/p&gt;

&lt;p&gt;通过 &lt;code&gt;ip a&lt;/code&gt; 可以看到，每添加一个服务，node 节点上面的 &lt;code&gt;kube-dummy-if&lt;/code&gt; 网卡就会增加一个虚IP。&lt;/p&gt;

&lt;h3 id=&#34;session-affinity&#34;&gt;session affinity&lt;/h3&gt;

&lt;p&gt;Service 默认的策略是，通过 round-robin 算法来选择 backend Pod。 要实现基于客户端 IP 的会话亲和性，可以通过设置 &lt;code&gt;service.spec.sessionAffinity&lt;/code&gt; 的值为 &lt;code&gt;ClientIP&lt;/code&gt; （默认值为 &amp;ldquo;None&amp;rdquo;）。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl delete svc whats-my-ip
$ kubectl expose deploy whats-my-ip --target-port=8080 --port=8080 --session-affinity=ClientIP

$ ipvsadm -Ln

IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&amp;gt; 192.168.123.250:6443         Masq    1      0          0
TCP  10.254.226.105:8080 rr persistent 10800
  -&amp;gt; 172.20.135.8:8080            Masq    1      0          0
  -&amp;gt; 172.20.166.130:8080          Masq    1      0          0
  -&amp;gt; 172.20.104.9:8080            Masq    1      0          0
  
$ ipvsadm -S -n

-A -t 10.254.0.1:443 -s rr -p 10800
-a -t 10.254.0.1:443 -r 192.168.123.250:6443 -m -w 1
-A -t 10.254.226.105:8080 -s rr -p 10800
-a -t 10.254.226.105:8080 -r 172.20.135.8:8080 -m -w 1
-a -t 10.254.226.105:8080 -r 172.20.166.130:8080 -m -w 1
-a -t 10.254.226.105:8080 -r 172.20.104.9:8080 -m -w 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到 lvs 的规则条目里多了个 &lt;code&gt;persistent&lt;/code&gt;，即 lvs 的持久连接，关于 lvs 持久连接的具体内容可以参考我的另一篇博文 &lt;a href=&#34;https://www.yangcs.net/posts/lvs-persistent-connection/&#34; target=&#34;_blank&#34;&gt;LVS负载均衡之持久性连接介绍&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;可以通过设置 &lt;code&gt;service.spec.sessionAffinityConfig.clientIP.timeoutSeconds&lt;/code&gt; 的值来修改 lvs 的 &lt;code&gt;persistence_timeout&lt;/code&gt; 超时时间。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;$ kubectl get svc whats-my-ip -o yaml

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2018-04-20T08:16:38Z
  labels:
    run: whats-my-ip
  name: whats-my-ip
  namespace: default
  resourceVersion: &amp;quot;6323769&amp;quot;
  selfLink: /api/v1/namespaces/default/services/whats-my-ip
  uid: 26315fdf-4473-11e8-8388-005056a1bc83
spec:
  clusterIP: 10.254.226.105
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    run: whats-my-ip
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
  type: ClusterIP
status:
  loadBalancer: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;nodeport&#34;&gt;NodePort&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl delete svc whats-my-ip
$ kubectl expose deploy whats-my-ip --target-port=8080 --port=8080 --type=NodePort

$ ipvsadm -Ln

IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.123.249:34507 rr
  -&amp;gt; 172.20.135.8:8080            Masq    1      0          0
  -&amp;gt; 172.20.166.130:8080          Masq    1      0          0
  -&amp;gt; 172.20.104.9:8080            Masq    1      0          0
TCP  10.254.0.1:443 rr persistent 10800
  -&amp;gt; 192.168.123.250:6443         Masq    1      0          0
TCP  10.254.175.147:8080 rr
  -&amp;gt; 172.20.135.8:8080            Masq    1      0          0
  -&amp;gt; 172.20.166.130:8080          Masq    1      0          0
  -&amp;gt; 172.20.104.9:8080            Masq    1      0          0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到不仅有虚拟IP条目，还多了对应主机的 lvs 条目。&lt;/p&gt;

&lt;h3 id=&#34;更改算法&#34;&gt;更改算法&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;最少连接数&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl annotate service my-service &amp;quot;kube-router.io/service.scheduler=lc&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;轮询&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl annotate service my-service &amp;quot;kube-router.io/service.scheduler=rr&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;源地址哈希&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl annotate service my-service &amp;quot;kube-router.io/service.scheduler=sh&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;目的地址哈希&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl annotate service my-service &amp;quot;kube-router.io/service.scheduler=dh&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-id-h2-4-问题解决-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. 问题解决&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;接下来需要面对一些非常棘手的问题，我尽可能将问题描述清楚。&lt;/p&gt;

&lt;p id=&#34;div-border-top-red&#34;&gt;
&lt;strong&gt;问题1：&lt;/strong&gt;在集群内某个节点主机上通过 &lt;code&gt;SVC IP+Port&lt;/code&gt; 访问某个应用时，如果 lvs 转到后端的 pod 在本主机上，那么可以访问，如果该 pod 不在本主机上，那么无法访问。
&lt;/p&gt;

&lt;p&gt;可以通过抓包来看一下，现在 &lt;code&gt;service whats-my-ip&lt;/code&gt; 后端有三个 pod，分别运行在 &lt;code&gt;node1&lt;/code&gt;、&lt;code&gt;node2&lt;/code&gt; 和 &lt;code&gt;node3&lt;/code&gt; 上。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods -owide

NAME                           READY     STATUS    RESTARTS   AGE       IP               NODE
whats-my-ip-845d4ff4f6-d2ptz   1/1       Running   0          23h       172.20.135.8     192.168.123.249
whats-my-ip-845d4ff4f6-jxzzn   1/1       Running   0          23h       172.20.166.130   192.168.123.250
whats-my-ip-845d4ff4f6-szhhd   1/1       Running   0          34s       172.20.104.9     192.168.123.248
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;node3&lt;/code&gt; 上访问 &lt;code&gt;whats-my-ip&lt;/code&gt; 服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ip a show|grep 10.254.175.147

    inet 10.254.175.147/32 brd 10.254.175.147 scope link kube-dummy-if

$ ipvsadm -Ln -t 10.254.175.147:8080

Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.175.147:8080 rr
  -&amp;gt; 172.20.104.9:8080            Masq    1      0          0
  -&amp;gt; 172.20.135.8:8080            Masq    1      0          0
  -&amp;gt; 172.20.166.130:8080          Masq    1      0          0

# 第一次访问，不通
$ curl 10.254.175.147:8080

# 第二次访问
$ curl 10.254.175.147:8080

HOSTNAME:whats-my-ip-845d4ff4f6-d2ptz IP:172.20.135.8

# 第三次访问，不通
$ curl 10.254.175.147:8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时在 &lt;code&gt;node1&lt;/code&gt; 上抓包：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ tcpdump -i ens160 host 172.20.166.130 -nn

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on ens160, link-type EN10MB (Ethernet), capture size 262144 bytes
03:27:26.337553 IP 10.254.175.147.42036 &amp;gt; 172.20.166.130.8080: Flags [S], seq 405854371, win 43690, options [mss 65495,sackOK,TS val 359417229 ecr 0,nop,wscale 7], length 0
03:27:27.340131 IP 10.254.175.147.42036 &amp;gt; 172.20.166.130.8080: Flags [S], seq 405854371, win 43690, options [mss 65495,sackOK,TS val 359418232 ecr 0,nop,wscale 7], length 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到 &lt;code&gt;node1&lt;/code&gt; 将数据包丢弃了，因为源IP是 &lt;code&gt;10.254.175.147&lt;/code&gt;，系统认为这是 node1 自己本身。&lt;/p&gt;

&lt;p&gt;根本原因可以查看 &lt;code&gt;node3&lt;/code&gt; 的路由表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ip route show table local|grep 10.254.175.147

local 10.254.175.147 dev kube-dummy-if proto kernel scope host src 10.254.175.147
broadcast 10.254.175.147 dev kube-dummy-if proto kernel scope link src 10.254.175.147
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;src&lt;/code&gt; 的值用来告诉该 host 使用 &lt;code&gt;10.254.175.147&lt;/code&gt; 作为 &lt;code&gt;source address&lt;/code&gt;，可以通过修改路由表来解决这个问题：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ip route replace local 10.254.175.147 dev kube-dummy-if proto kernel scope host src 192.168.123.249 table local
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再次在 &lt;code&gt;node1&lt;/code&gt; 上抓包可以发现源IP已经变成了 &lt;code&gt;192.168.123.249&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ tcpdump -i ens160 host 172.20.166.130 -nn

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on ens160, link-type EN10MB (Ethernet), capture size 262144 bytes
03:39:42.824412 IP 192.168.123.249.52684 &amp;gt; 172.20.166.130.8080: Flags [S], seq 3520353543, win 43690, options [mss 65495,sackOK,TS val 360153716 ecr 0,nop,wscale 7], length 0
03:39:42.824542 IP 172.20.166.130.8080 &amp;gt; 192.168.123.249.52684: Flags [S.], seq 4057001749, ack 3520353544, win 28960, options [mss 1460,sackOK,TS val 360143668 ecr 360153716,nop,wscale 7], length 0
03:39:42.824706 IP 192.168.123.249.52684 &amp;gt; 172.20.166.130.8080: Flags [.], ack 1, win 342, options [nop,nop,TS val 360153716 ecr 360143668], length 0
03:39:42.825066 IP 192.168.123.249.52684 &amp;gt; 172.20.166.130.8080: Flags [P.], seq 1:84, ack 1, win 342, options [nop,nop,TS val 360153716 ecr 360143668], length 83: HTTP: GET / HTTP/1.1
03:39:42.825112 IP 172.20.166.130.8080 &amp;gt; 192.168.123.249.52684: Flags [.], ack 84, win 227, options [nop,nop,TS val 360143669 ecr 360153716], length 0
03:39:42.825589 IP 172.20.166.130.8080 &amp;gt; 192.168.123.249.52684: Flags [P.], seq 1:174, ack 84, win 227, options [nop,nop,TS val 360143669 ecr 360153716], length 173: HTTP: HTTP/1.1 200 OK
03:39:42.825735 IP 192.168.123.249.52684 &amp;gt; 172.20.166.130.8080: Flags [.], ack 174, win 350, options [nop,nop,TS val 360153717 ecr 360143669], length 0
03:39:42.825787 IP 192.168.123.249.52684 &amp;gt; 172.20.166.130.8080: Flags [F.], seq 84, ack 174, win 350, options [nop,nop,TS val 360153717 ecr 360143669], length 0
03:39:42.825882 IP 172.20.166.130.8080 &amp;gt; 192.168.123.249.52684: Flags [F.], seq 174, ack 85, win 227, options [nop,nop,TS val 360143669 ecr 360153717], length 0
03:39:42.826002 IP 192.168.123.249.52684 &amp;gt; 172.20.166.130.8080: Flags [.], ack 175, win 350, options [nop,nop,TS val 360153718 ecr 360143669], length 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;
&lt;p id=&#34;div-border-top-red&#34;&gt;
&lt;strong&gt;问题2：&lt;/strong&gt;在集群内某个节点主机上通过 &lt;code&gt;SVC IP+Port&lt;/code&gt; 访问 &lt;code&gt;service kubernetes&lt;/code&gt; 时，如果该节点是 master 节点（即 kube-apiserver 运行在该节点上），那么可以访问，如果该节点不是 master 节点，那么无法访问。
&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;原因和问题1类似，可以通过修改路由表解决：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 例如在 node3 节点上
$ ip route replace local 10.254.0.1 dev kube-dummy-if proto kernel scope host src 192.168.123.249 table local
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;
&lt;p id=&#34;div-border-top-red&#34;&gt;
&lt;strong&gt;问题3：&lt;/strong&gt;在某个 pod 内访问该 pod 本身的 &lt;code&gt;ClusterIP:Port&lt;/code&gt;，如果 lvs 转到后端的 IP 是该 pod 的 IP，那么无法访问，如果不是则可以访问。
&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kube-proxy&lt;/code&gt; 的 iptables 模式也有同样的问题，这个问题可以忽略。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;问题1和问题2修改路由表可以通过批量 shell 脚本来解决：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/bin/sh

default_if=$(ip route|grep default|awk &#39;{print $5}&#39;)
localip=$(ip a show ${default_if}|egrep -v inet6|grep inet|awk &#39;{print $2}&#39;|awk -F&amp;quot;/&amp;quot; &#39;{print $1}&#39;)
svc_ip=$(ip route show table local|egrep -v broadcast|grep kube-dummy-if|awk &#39;{print $2}&#39;)

for ip in $svc_ip; do
ip route replace local $ip dev kube-dummy-if proto kernel scope host src $localip table local;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想要在创建 &lt;code&gt;service&lt;/code&gt; 时自动修改路由表，最好还是将该 fix 整合进 kube-router 的源码中。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-5-参考-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;5. 参考&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudnativelabs/kube-router/blob/master/docs/README.md&#34; target=&#34;_blank&#34;&gt;Kube-router Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jianshu.com/p/d69b40580c87&#34; target=&#34;_blank&#34;&gt;kube-router之负载均衡器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudnativelabs/kube-router/issues/376&#34; target=&#34;_blank&#34;&gt;bad routing from host to service IP on same host&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
  </channel>
</rss>
