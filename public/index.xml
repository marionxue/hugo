<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ryan Yang</title>
    <link>https://www.yangcs.net/</link>
    <description>Recent content on Ryan Yang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Tue, 23 Jan 2018 08:26:58 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Linux全局智能分流方案</title>
      <link>https://www.yangcs.net/posts/linux-circumvent/</link>
      <pubDate>Tue, 23 Jan 2018 08:26:58 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/linux-circumvent/</guid>
      <description>&lt;p&gt;
&lt;strong&gt;本来我是决定不再写这样的文章了的。但是呢，最近连续配置了两次 &lt;code&gt;ArchLinux&lt;/code&gt;，在配置这种东西的时候连续撞到了同样的坑，加上这段时间经常有人问我关于 &lt;code&gt;Linux&lt;/code&gt; 下的 &lt;code&gt;shadowsocks&lt;/code&gt; 的问题，所以我想了想还是写一篇记录一下吧，也免得自己以后再忘记了。&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;这里有两种方案，都可以实现全局智能分流。第一种方案的思路是使用 &lt;code&gt;ipset&lt;/code&gt; 载入 &lt;code&gt;chnroute&lt;/code&gt; 的 &lt;code&gt;IP&lt;/code&gt; 列表并使用 &lt;code&gt;iptables&lt;/code&gt; 实现带自动分流国内外流量的全局代理。为什么不用 &lt;code&gt;PAC&lt;/code&gt; 呢？因为 &lt;code&gt;PAC&lt;/code&gt; 这种东西只对浏览器有用。难道你在浏览器之外就不需要科学上网了吗？反正我是不信的……&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;font color=Blue&gt;本教程所用系统为 &lt;code&gt;Archlinux&lt;/code&gt;，其他发型版类似，请自行参考相关资料。&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-markdown-1-style-margin-bottom-2em-margin-right-5px-padding-8px-15px-letter-spacing-2px-background-image-linear-gradient-to-right-bottom-rgb-0-188-212-rgb-63-81-181-background-color-rgb-63-81-181-color-rgb-255-255-255-border-left-10px-solid-rgb-51-51-51-border-radius-5px-text-shadow-rgb-102-102-102-1px-1px-1px-box-shadow-rgb-102-102-102-1px-1px-2px-1-通过-iptables-实现智能分流-p&#34;&gt;&lt;p markdown=&#34;1&#34; style=&#34;margin-bottom:2em; margin-right: 5px; padding: 8px 15px; letter-spacing: 2px; background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181)); background-color: rgb(63, 81, 181); color: rgb(255, 255, 255); border-left: 10px solid rgb(51, 51, 51); border-radius:5px; text-shadow: rgb(102, 102, 102) 1px 1px 1px; box-shadow: rgb(102, 102, 102) 1px 1px 2px;&#34;&gt;1. 通过 iptables 实现智能分流&lt;/p&gt;&lt;/h2&gt;

&lt;h3 id=&#34;1-1-安装相关软件&#34;&gt;1.1 安装相关软件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;shadowsocks-libev&lt;/li&gt;
&lt;li&gt;ipset&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pacman -S shadowsocks-libev ipset
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-2-配置shadowsocks-libev-略过&#34;&gt;1.2 配置shadowsocks-libev（略过）&lt;/h3&gt;

&lt;p&gt;假设shadowsocks配置文件为/etc/shadowsocks.json&lt;/p&gt;

&lt;h3 id=&#34;1-3-获取中国ip段&#34;&gt;1.3 获取中国IP段&lt;/h3&gt;

&lt;p&gt;将以下命令写入脚本保存执行（假设保存在/home/yang/bin/路由表/目录下）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/sh
wget -c http://ftp.apnic.net/stats/apnic/delegated-apnic-latest
cat delegated-apnic-latest | awk -F &#39;|&#39; &#39;/CN/&amp;amp;&amp;amp;/ipv4/ {print $4 &amp;quot;/&amp;quot; 32-log($5)/log(2)}&#39; | cat &amp;gt; /home/yang/bin/路由表/cn_rules.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-4-创建启动和关闭脚本&#34;&gt;1.4 创建启动和关闭脚本&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vim /home/yang/bin/shadowsocks/ss-up.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash

SOCKS_SERVER=$SERVER_IP # SOCKS 服务器的 IP 地址
# Setup the ipset
ipset -N chnroute hash:net maxelem 65536

for ip in $(cat &#39;/home/yang/bin/路由表/cn_rules.conf&#39;); do
  ipset add chnroute $ip
done

# 在nat表中新增一个链，名叫：SHADOWSOCKS
iptables -t nat -N SHADOWSOCKS

# Allow connection to the server
iptables -t nat -A SHADOWSOCKS -d $SOCKS_SERVER -j RETURN

# Allow connection to reserved networks
iptables -t nat -A SHADOWSOCKS -d 0.0.0.0/8 -j RETURN
iptables -t nat -A SHADOWSOCKS -d 10.0.0.0/8 -j RETURN
iptables -t nat -A SHADOWSOCKS -d 127.0.0.0/8 -j RETURN
iptables -t nat -A SHADOWSOCKS -d 169.254.0.0/16 -j RETURN
iptables -t nat -A SHADOWSOCKS -d 172.16.0.0/12 -j RETURN
iptables -t nat -A SHADOWSOCKS -d 192.168.0.0/16 -j RETURN
iptables -t nat -A SHADOWSOCKS -d 224.0.0.0/4 -j RETURN
iptables -t nat -A SHADOWSOCKS -d 240.0.0.0/4 -j RETURN

# Allow connection to chinese IPs
iptables -t nat -A SHADOWSOCKS -p tcp -m set --match-set chnroute dst -j RETURN
# 如果你想对 icmp 协议也实现智能分流，可以加上下面这一条
# iptables -t nat -A SHADOWSOCKS -p icmp -m set --match-set chnroute dst -j RETURN

# Redirect to Shadowsocks
# 把1081改成你的shadowsocks本地端口
iptables -t nat -A SHADOWSOCKS -p tcp -j REDIRECT --to-port 1081
# 如果你想对 icmp 协议也实现智能分流，可以加上下面这一条
# iptables -t nat -A SHADOWSOCKS -p icmp -j REDIRECT --to-port 1081

# 将SHADOWSOCKS链中所有的规则追加到OUTPUT链中
iptables -t nat -A OUTPUT -p tcp -j SHADOWSOCKS
# 如果你想对 icmp 协议也实现智能分流，可以加上下面这一条
# iptables -t nat -A OUTPUT -p icmp -j SHADOWSOCKS

# 内网流量流经 shadowsocks 规则链
iptables -t nat -A PREROUTING -s 192.168/16 -j SHADOWSOCKS
# 内网流量源NAT
iptables -t nat -A POSTROUTING -s 192.168/16 -j MASQUERADE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;emsp;&amp;emsp;这是在启动 &lt;code&gt;shadowsocks&lt;/code&gt; 之前执行的脚本，用来设置 &lt;code&gt;iptables&lt;/code&gt; 规则，对全局应用代理并将 &lt;code&gt;chnroute&lt;/code&gt; 导入 &lt;code&gt;ipset&lt;/code&gt; 来实现自动分流。注意要把服务器 &lt;code&gt;IP&lt;/code&gt; 和本地端口相关的代码全部替换成你自己的。
&amp;emsp;&amp;emsp;这里就有一个坑了，就是在把 &lt;code&gt;chnroute.txt&lt;/code&gt; 加入 &lt;code&gt;ipset&lt;/code&gt; 的时候。因为 &lt;code&gt;chnroute.txt&lt;/code&gt; 是一个 &lt;code&gt;IP&lt;/code&gt; 段列表，而中国持有的 &lt;code&gt;IP&lt;/code&gt; 数量上还是比较大的，所以如果使用 &lt;code&gt;hash:ip&lt;/code&gt; 来导入的话会使内存溢出。我在第二次重新配置的时候就撞进了这个大坑……
&amp;emsp;&amp;emsp;但是你也不能尝试把整个列表导入 &lt;code&gt;iptables&lt;/code&gt;。虽然导入 &lt;code&gt;iptables&lt;/code&gt; 不会导致内存溢出，但是 &lt;code&gt;iptables&lt;/code&gt; 是线性查表，即使你全部导入进去，也会因为低下的性能而抓狂。
&lt;br \&gt;
然后再创建 &lt;code&gt;/home/yang/bin/shadowsocks/ss-down.sh&lt;/code&gt;, 这是用来清除上述规则的脚本，比较简单&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash

# iptables -t nat -D OUTPUT -p icmp -j SHADOWSOCKS
iptables -t nat -D OUTPUT -p tcp -j SHADOWSOCKS
iptables -t nat -F SHADOWSOCKS
iptables -t nat -X SHADOWSOCKS
ipset destroy chnroute
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接着执行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ chmod +x ss-up.sh
$ chmod +x ss-down.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-5-配置ss-redir服务&#34;&gt;1.5 配置ss-redir服务&lt;/h3&gt;

&lt;p&gt;首先，默认的 &lt;code&gt;ss-local&lt;/code&gt; 并不能用来作为 &lt;code&gt;iptables&lt;/code&gt; 流量转发的目标，因为它是 &lt;code&gt;socks5&lt;/code&gt; 代理而非透明代理。我们至少要把 &lt;code&gt;systemd&lt;/code&gt; 执行的程序改成 &lt;code&gt;ss-redir&lt;/code&gt;。其次，上述两个脚本还不能自动执行，必须让 &lt;code&gt;systemd&lt;/code&gt; 分别在启动 &lt;code&gt;shadowsocks&lt;/code&gt; 之前和关闭之后将脚本执行，这样才能自动配置好 &lt;code&gt;iptables&lt;/code&gt; 规则。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vim /usr/lib/systemd/system/shadowsocks-libev@.service
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[Unit]
Description=Shadowsocks-Libev Client Service
After=network.target

[Service]
User=root
CapabilityBoundingSet=~CAP_SYS_ADMIN
ExecStart=
ExecStartPre=/home/yang/bin/shadowsocks/ss-up.sh
ExecStart=/usr/bin/ss-redir -u -c /etc/%i.json
ExecStopPost=/home/yang/bin/shadowsocks/ss-down.sh

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后启动服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl start shadowsocks-libev@shadowsocks
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;开机自启&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl enable shadowsocks-libev@shadowsocks
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-6-配置智能-dns-服务&#34;&gt;1.6 配置智能 DNS 服务&lt;/h3&gt;

&lt;p&gt;完成了以上工作之后是不是就可以实现全局科学上网了呢？答案是否定的，我们还有最后一项工作需要完成，那就是解决 &lt;code&gt;DNS&lt;/code&gt; 污染问题。如果你不知道什么是 &lt;code&gt;DNS&lt;/code&gt; 污染，我可以简单地给你普及一下：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;DNS&lt;/code&gt; 污染是一种让一般用户由于得到虚假目标主机 &lt;code&gt;IP&lt;/code&gt; 而不能与其通信的方法，是一种 &lt;code&gt;DNS&lt;/code&gt; 缓存投毒攻击（DNS cache poisoning）。其工作方式是：由于通常的 &lt;code&gt;DNS&lt;/code&gt; 查询没有任何认证机制，而且 &lt;code&gt;DNS&lt;/code&gt; 查询通常基于的 &lt;code&gt;UDP&lt;/code&gt; 是无连接不可靠的协议，因此 &lt;code&gt;DNS&lt;/code&gt; 的查询非常容易被篡改，通过对 &lt;code&gt;UDP&lt;/code&gt; 端口 53 上的 &lt;code&gt;DNS&lt;/code&gt; 查询进行入侵检测，一经发现与关键词相匹配的请求则立即伪装成目标域名的解析服务器（NS，Name Server）给查询者返回虚假结果。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;DNS&lt;/code&gt; 污染症状：目前一些被禁止访问的网站很多就是通过 &lt;code&gt;DNS&lt;/code&gt; 污染来实现的，例如 &lt;code&gt;YouTube&lt;/code&gt;、&lt;code&gt;Facebook&lt;/code&gt; 等网站。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;应对dns污染的方法&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对于 &lt;code&gt;DNS&lt;/code&gt; 污染，可以说，个人用户很难单单靠设置解决，通常可以使用 &lt;code&gt;VPN&lt;/code&gt; 或者域名远程解析的方法解决，但这大多需要购买付费的 &lt;code&gt;VPN&lt;/code&gt; 或 &lt;code&gt;SSH&lt;/code&gt; 等&lt;/li&gt;
&lt;li&gt;修改 &lt;code&gt;Hosts&lt;/code&gt; 的方法，手动设置域名正确的 &lt;code&gt;IP&lt;/code&gt; 地址&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dns&lt;/code&gt; 加密解析：&lt;a href=&#34;https://dnscrypt.org/&#34; target=&#34;_blank&#34;&gt;DNSCrypt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;忽略 &lt;code&gt;DNS&lt;/code&gt; 投毒污染小工具：&lt;a href=&#34;https://github.com/chengr28/Pcap_DNSProxy&#34; target=&#34;_blank&#34;&gt;Pcap_DNSProxy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们选择用 &lt;code&gt;Pcap_DNSProxy&lt;/code&gt; 来解决这个问题，以前用的是 &lt;code&gt;Pdnsd + Dnsmasq&lt;/code&gt; 组合， 后来发现 &lt;code&gt;TCP&lt;/code&gt; 请求效率太低加上家里网络与那些国外的 &lt;code&gt;DNS&lt;/code&gt; 丢包实在是严重， 所以打算用 &lt;code&gt;Pcap_DNSProxy&lt;/code&gt; 代替 &lt;code&gt;Pdnsd&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;关于 &lt;code&gt;Pcap_DNSProxy&lt;/code&gt; 的详细介绍，可以参考:
&lt;a href=&#34;https://github.com/chengr28/Pcap_DNSProxy&#34; target=&#34;_blank&#34;&gt;https://github.com/chengr28/Pcap_DNSProxy&lt;/a&gt;
安装过程可以参考：
&lt;a href=&#34;https://github.com/chengr28/Pcap_DNSProxy/blob/master/Documents/ReadMe_Linux.zh-Hans.txt&#34; target=&#34;_blank&#34;&gt;https://github.com/chengr28/Pcap_DNSProxy/blob/master/Documents/ReadMe_Linux.zh-Hans.txt&lt;/a&gt;
更详细的使用说明可以参考：
&lt;a href=&#34;https://github.com/chengr28/Pcap_DNSProxy/blob/master/Documents/ReadMe.zh-Hans.txt&#34; target=&#34;_blank&#34;&gt;https://github.com/chengr28/Pcap_DNSProxy/blob/master/Documents/ReadMe.zh-Hans.txt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这里主要重点强调一些需要注意的配置项：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DNS&lt;/code&gt; - 境外域名解析参数区域（这是最关键的一项配置）&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[DNS]
# 这里一定要填 IPv4 + TCP！！！表示只使用 TCP 协议向境外远程 DNS 服务器发出请求
Outgoing Protocol = IPv4 + TCP
# 建议当系统使用全局代理功能时启用，程序将除境内服务器外的所有请求直接交给系统而不作任何过滤等处理，系统会将请求自动发往远程服务器进行解析
Direct Request = IPv4
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Local DNS&lt;/code&gt; - 境内域名解析参数区域&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[Local DNS]
# 发送请求到境内 DNS 服务器时所使用的协议
Local Protocol = IPv4 + UDP
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Addresses&lt;/code&gt; - 普通模式地址区域&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[Addresses]
...
...
# IPv4 主要境外 DNS 服务器地址
IPv4 Main DNS Address = 8.8.4.4:53
# IPv4 备用境外 DNS 服务器地址
IPv4 Alternate DNS Address = 8.8.8.8:53|208.67.220.220:443|208.67.222.222:5353
# IPv4 主要境内 DNS 服务器地址，用于境内域名解析，推荐使用 onedns
IPv4 Local Main DNS Address = 112.124.47.27:53
# IPv4 备用境内 DNS 服务器地址，用于境内域名解析
IPv4 Local Alternate DNS Address = 114.215.126.16:53
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;1-7-配置系统-dns-服务器设置&#34;&gt;1.7 配置系统 DNS 服务器设置&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;可参见 &lt;a href=&#34;https://developers.google.com/speed/public-dns/docs/using&#34; target=&#34;_blank&#34;&gt;https://developers.google.com/speed/public-dns/docs/using&lt;/a&gt; 中 &lt;code&gt;Changing your DNS servers settings&lt;/code&gt; 中 &lt;code&gt;Linux&lt;/code&gt; 一节&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;图形界面以 &lt;code&gt;GNOME 3&lt;/code&gt; 为例：&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;打开所有程序列表，并 -&amp;gt; 设置 – 硬件分类 – 网络&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果要对当前的网络配置进行编辑 -&amp;gt; 单击齿轮按钮&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;选中 &lt;code&gt;IPv4&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;DNS&lt;/code&gt; 栏目中，将自动拨向关闭&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在服务器中填入 &lt;code&gt;127.0.0.1&lt;/code&gt; （或103.214.195.99:7300）并应用&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;选中 &lt;code&gt;IPv6&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;DNS&lt;/code&gt; 栏目中，将自动拨向关闭&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在服务器中填入 ::1 并应用&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;请务必确保只填入这两个地址，填入其它地址可能会导致系统选择其它 DNS 服务器绕过程序的代理&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;重启网络连接&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;直接修改系统文件修改 DNS 服务器设置：&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;自动获取地址(DHCP)时：&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;以 &lt;code&gt;root&lt;/code&gt; 权限进入 &lt;code&gt;/etc/dhcp&lt;/code&gt; 或 &lt;code&gt;/etc/dhcp3&lt;/code&gt; 目录（视乎 dhclient.conf 文件位置）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;直接修改 &lt;code&gt;dhclient.conf&lt;/code&gt; 文件，修改或添加 &lt;code&gt;prepend domain-name-servers&lt;/code&gt; 一项即可&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果 &lt;code&gt;prepend domain-name-servers&lt;/code&gt; 一项被 # 注释则需要把注释去掉以使配置生效，不需要添加新的条目&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;dhclient.conf&lt;/code&gt; 文件可能存在多个 &lt;code&gt;prepend domain-name-servers&lt;/code&gt; 项，是各个网络接口的配置项目，直接修改总的配置项目即可&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用 &lt;code&gt;service network(/networking) restart&lt;/code&gt; 或 &lt;code&gt;ifdown/ifup&lt;/code&gt; 或 &lt;code&gt;ifconfig stop/start&lt;/code&gt; 重启网络服务/网络端口&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;非自动获取地址(DHCP)时：&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;以 &lt;code&gt;root&lt;/code&gt; 权限进入 &lt;code&gt;/etc&lt;/code&gt; 目录&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;直接修改 &lt;code&gt;resolv.conf&lt;/code&gt; 文件里的 &lt;code&gt;nameserver&lt;/code&gt; 即可&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果重启后配置被覆盖，则需要修改或新建 &lt;code&gt;/etc/resolvconf/resolv.conf.d&lt;/code&gt; 文件，内容和 &lt;code&gt;resolv.conf&lt;/code&gt; 一样&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用 &lt;code&gt;service network(/networking) restart&lt;/code&gt; 或 &lt;code&gt;ifdown/ifup&lt;/code&gt; 或 &lt;code&gt;ifconfig stop/start&lt;/code&gt; 重启网络服务/网络端口&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;1-8-打开流量转发&#34;&gt;1.8 打开流量转发&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat /etc/sysctl.d/30-ipforward.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;net.ipv4.ip_forward=1

net.ipv6.conf.all.forwarding = 1

net.ipv4.tcp_congestion_control=westwood

net.ipv4.tcp_syn_retries = 5

net.ipv4.tcp_synack_retries = 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑完成后，执行以下命令使变动立即生效&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sysctl -p
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-markdown-1-style-margin-bottom-2em-margin-right-5px-padding-8px-15px-letter-spacing-2px-background-image-linear-gradient-to-right-bottom-rgb-0-188-212-rgb-63-81-181-background-color-rgb-63-81-181-color-rgb-255-255-255-border-left-10px-solid-rgb-51-51-51-border-radius-5px-text-shadow-rgb-102-102-102-1px-1px-1px-box-shadow-rgb-102-102-102-1px-1px-2px-2-通过-nftables-实现智能分流-p&#34;&gt;&lt;p markdown=&#34;1&#34; style=&#34;margin-bottom:2em; margin-right: 5px; padding: 8px 15px; letter-spacing: 2px; background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181)); background-color: rgb(63, 81, 181); color: rgb(255, 255, 255); border-left: 10px solid rgb(51, 51, 51); border-radius:5px; text-shadow: rgb(102, 102, 102) 1px 1px 1px; box-shadow: rgb(102, 102, 102) 1px 1px 2px;&#34;&gt;2. 通过 nftables 实现智能分流&lt;/p&gt;&lt;/h2&gt;

&lt;h3 id=&#34;2-1-安装相关软件&#34;&gt;2.1 安装相关软件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;shadowsocks-libev&lt;/li&gt;
&lt;li&gt;nftables&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pacman -S shadowsocks-libev nftables
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-2-配置shadowsocks-libev-略过&#34;&gt;2.2 配置shadowsocks-libev（略过）&lt;/h3&gt;

&lt;p&gt;假设shadowsocks配置文件为/etc/shadowsocks.json&lt;/p&gt;

&lt;h3 id=&#34;2-3-获取中国ip段&#34;&gt;2.3 获取中国IP段&lt;/h3&gt;

&lt;p&gt;将以下命令写入脚本保存执行（假设保存在/home/yang/bin/路由表/目录下）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/sh
wget -c http://ftp.apnic.net/stats/apnic/delegated-apnic-latest
cat delegated-apnic-latest | awk -F &#39;|&#39; &#39;/CN/&amp;amp;&amp;amp;/ipv4/ {print $4 &amp;quot;/&amp;quot; 32-log($5)/log(2)}&#39; | cat &amp;gt; /home/yang/bin/路由表/cn_rules.conf
cat cn_rules.conf|sed &#39;:label;N;s/\n/, /;b label&#39;|sed &#39;s/$/&amp;amp; }/g&#39;|sed &#39;s/^/{ &amp;amp;/g&#39; &amp;gt; /home/yang/bin/路由表/cn_rules1.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-4-创建启动和关闭脚本&#34;&gt;2.4 创建启动和关闭脚本&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vim /home/yang/bin/shadowsocks/nftables-up.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#! /bin/bash

nft_pre=&amp;quot;/usr/sbin/nft add rule nat prerouting&amp;quot;
nft_out=&amp;quot;/usr/sbin/nft add rule nat output&amp;quot;
chnroute=$(cat &#39;/home/yang/bin/路由表/cn_rules1.conf&#39;)

/usr/bin/nft -f /etc/nftables.conf

${nft_pre} tcp dport 8385 return
${nft_pre} ip daddr 139.162.87.98 return
${nft_pre} ip daddr { 0.0.0.0/8, 10.0.0.0/8, 127.0.0.0/8, 169.254.0.0/16, 172.16.0.0/12, 192.168.0.0/16, 224.0.0.0/4, 240.0.0.0/4, 172.16.39.0/24} return
${nft_pre} ip daddr $chnroute return
${nft_pre} tcp sport { 32768-61000} redirect to 1081
#${nft_pre} ip protocol icmp redirect to 1081
# 内网流量源NAT
nft add rule nat postrouting ip saddr 192.168.0.0/12 masquerade

${nft_out} tcp dport 8385 return
${nft_out} ip daddr 139.162.87.98 return
${nft_out} ip daddr { 0.0.0.0/8, 10.0.0.0/8, 127.0.0.0/8, 169.254.0.0/16, 172.16.0.0/12, 192.168.0.0/16, 224.0.0.0/4, 240.0.0.0/4, 172.16.39.0/24} return
${nft_out} ip daddr $chnroute return
# /proc/sys/net/ipv4/ip_local_port_range，本地发起的连接的端口范围
${nft_out} tcp sport { 32768-61000} redirect to 1081
${nft_out} ip protocol icmp redirect to 1081
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;emsp;&amp;emsp;这是在启动 &lt;code&gt;shadowsocks&lt;/code&gt; 之前执行的脚本，用来设置 &lt;code&gt;nftables&lt;/code&gt; 规则。
然后再创建 &lt;code&gt;/home/yang/bin/shadowsocks/nftables-down.sh&lt;/code&gt;, 这是用来清除上述规则的脚本，比较简单&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash

sudo nft flush table nat
#sudo nft flush table filter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接着执行&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ chmod +x nftables-up.sh
$ chmod +x nftables-down.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-5-配置ss-redir服务&#34;&gt;2.5 配置ss-redir服务&lt;/h3&gt;

&lt;p&gt;首先，默认的 &lt;code&gt;ss-local&lt;/code&gt; 并不能用来作为 &lt;code&gt;nftables&lt;/code&gt; 流量转发的目标，因为它是 &lt;code&gt;socks5&lt;/code&gt; 代理而非透明代理。我们至少要把 &lt;code&gt;systemd&lt;/code&gt; 执行的程序改成 &lt;code&gt;ss-redir&lt;/code&gt;。其次，上述两个脚本还不能自动执行，必须让 &lt;code&gt;systemd&lt;/code&gt; 分别在启动 &lt;code&gt;shadowsocks&lt;/code&gt; 之前和关闭之后将脚本执行，这样才能自动配置好 &lt;code&gt;nftables&lt;/code&gt; 规则。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vim /usr/lib/systemd/system/shadowsocks-libev@.service
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[Unit]
Description=Shadowsocks-Libev Client Service
After=network.target

[Service]
User=root
CapabilityBoundingSet=~CAP_SYS_ADMIN
ExecStart=
ExecStartPre=/home/yang/bin/shadowsocks/nftables-up.sh
ExecStart=/usr/bin/ss-redir -u -c /etc/%i.json
ExecStopPost=/home/yang/bin/shadowsocks/nftables-down.sh

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后启动服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl start nftables
$ systemctl start shadowsocks-libev@shadowsocks
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;开机自启&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl enable nftables
$ systemctl enable shadowsocks-libev@shadowsocks
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-6-配置智能-dns-服务&#34;&gt;2.6 配置智能 DNS 服务&lt;/h3&gt;

&lt;p&gt;同上&lt;/p&gt;

&lt;h3 id=&#34;2-7-配置系统-dns-服务器设置&#34;&gt;2.7 配置系统 DNS 服务器设置&lt;/h3&gt;

&lt;p&gt;同上&lt;/p&gt;

&lt;h3 id=&#34;2-8-打开流量转发&#34;&gt;2.8 打开流量转发&lt;/h3&gt;

&lt;p&gt;同上&lt;/p&gt;

&lt;h2 id=&#34;p-markdown-1-style-margin-bottom-2em-margin-right-5px-padding-8px-15px-letter-spacing-2px-background-image-linear-gradient-to-right-bottom-rgb-0-188-212-rgb-63-81-181-background-color-rgb-63-81-181-color-rgb-255-255-255-border-left-10px-solid-rgb-51-51-51-border-radius-5px-text-shadow-rgb-102-102-102-1px-1px-1px-box-shadow-rgb-102-102-102-1px-1px-2px-3-通过策略路由实现智能分流-p&#34;&gt;&lt;p markdown=&#34;1&#34; style=&#34;margin-bottom:2em; margin-right: 5px; padding: 8px 15px; letter-spacing: 2px; background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181)); background-color: rgb(63, 81, 181); color: rgb(255, 255, 255); border-left: 10px solid rgb(51, 51, 51); border-radius:5px; text-shadow: rgb(102, 102, 102) 1px 1px 1px; box-shadow: rgb(102, 102, 102) 1px 1px 2px;&#34;&gt;3. 通过策略路由实现智能分流&lt;/p&gt;&lt;/h2&gt;

&lt;h3 id=&#34;3-1-安装相关软件&#34;&gt;3.1 安装相关软件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;badvpn&lt;/li&gt;
&lt;li&gt;shadowsocks&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pacman -S badvpn shadowsocks
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-2-配置shadowsocks-略过&#34;&gt;3.2 配置shadowsocks（略过）&lt;/h3&gt;

&lt;p&gt;假设shadowsocks配置文件为/etc/shadowsocks.json&lt;/p&gt;

&lt;h3 id=&#34;3-3-获取中国ip段&#34;&gt;3.3 获取中国IP段&lt;/h3&gt;

&lt;p&gt;将以下命令写入脚本保存执行（假设保存在/home/yang/bin/路由表/目录下）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/sh
wget -c http://ftp.apnic.net/stats/apnic/delegated-apnic-latest
cat delegated-apnic-latest | awk -F &#39;|&#39; &#39;/CN/&amp;amp;&amp;amp;/ipv4/ {print $4 &amp;quot;/&amp;quot; 32-log($5)/log(2)}&#39; | cat &amp;gt; /home/yang/bin/路由表/cn_rules.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-4-配置智能-dns-服务&#34;&gt;3.4 配置智能 DNS 服务&lt;/h3&gt;

&lt;p&gt;同上&lt;/p&gt;

&lt;h3 id=&#34;3-5-配置系统-dns-服务器设置&#34;&gt;3.5 配置系统 DNS 服务器设置&lt;/h3&gt;

&lt;p&gt;同上&lt;/p&gt;

&lt;h3 id=&#34;3-6-写路由表启动和终止脚本&#34;&gt;3.6 写路由表启动和终止脚本&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vim /usr/local/bin/socksfwd
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
SOCKS_SERVER=$SERVER_IP # SOCKS 服务器的 IP 地址
SOCKS_PORT=1081 # 本地SOCKS 服务器的端口
GATEWAY_IP=$(ip route|grep &amp;quot;default&amp;quot;|awk &#39;{print $3}&#39;) # 家用网关（路由器）的 IP 地址，你也可以手动指定
TUN_NETWORK_DEV=tun0 # 选一个不冲突的 tun 设备号
TUN_NETWORK_PREFIX=10.0.0 # 选一个不冲突的内网 IP 段的前缀


start_fwd() {
ip tuntap del dev &amp;quot;$TUN_NETWORK_DEV&amp;quot; mode tun
# 添加虚拟网卡
ip tuntap add dev &amp;quot;$TUN_NETWORK_DEV&amp;quot; mode tun
# 给虚拟网卡绑定IP地址
ip addr add &amp;quot;$TUN_NETWORK_PREFIX.1/24&amp;quot; dev &amp;quot;$TUN_NETWORK_DEV&amp;quot;
# 启动虚拟网卡
ip link set &amp;quot;$TUN_NETWORK_DEV&amp;quot; up
ip route del default via &amp;quot;$GATEWAY_IP&amp;quot;
ip route add &amp;quot;$SOCKS_SERVER&amp;quot; via &amp;quot;$GATEWAY_IP&amp;quot;
# 特殊ip段走家用网关（路由器）的 IP 地址（如局域网联机）
# ip route add &amp;quot;172.16.39.0/24&amp;quot; via &amp;quot;$GATEWAY_IP&amp;quot;
# 国内网段走家用网关（路由器）的 IP 地址
for i in $(cat /home/yang/bin/路由表/cn_rules.conf)
do
ip route add &amp;quot;$i&amp;quot; via &amp;quot;$GATEWAY_IP&amp;quot;
done
# 将默认网关设为虚拟网卡的IP地址
ip route add 0.0.0.0/1 via &amp;quot;$TUN_NETWORK_PREFIX.1&amp;quot;
ip route add 128.0.0.0/1 via &amp;quot;$TUN_NETWORK_PREFIX.1&amp;quot;
# 将socks5转为vpn
badvpn-tun2socks --tundev &amp;quot;$TUN_NETWORK_DEV&amp;quot; --netif-ipaddr &amp;quot;$TUN_NETWORK_PREFIX.2&amp;quot; --netif-netmask 255.255.255.0 --socks-server-addr &amp;quot;127.0.0.1:$SOCKS_PORT&amp;quot;
TUN2SOCKS_PID=&amp;quot;$!&amp;quot;
}


stop_fwd() {
ip route del 128.0.0.0/1 via &amp;quot;$TUN_NETWORK_PREFIX.1&amp;quot;
ip route del 0.0.0.0/1 via &amp;quot;$TUN_NETWORK_PREFIX.1&amp;quot;
for i in $(cat /home/yang/bin/路由表/cn_rules.conf)
do
ip route del &amp;quot;$i&amp;quot; via &amp;quot;$GATEWAY_IP&amp;quot;
done
ip route del &amp;quot;172.16.39.0/24&amp;quot; via &amp;quot;$GATEWAY_IP&amp;quot;
ip route del &amp;quot;$SOCKS_SERVER&amp;quot; via &amp;quot;$GATEWAY_IP&amp;quot;
ip route add default via &amp;quot;$GATEWAY_IP&amp;quot;
ip link set &amp;quot;$TUN_NETWORK_DEV&amp;quot; down
ip addr del &amp;quot;$TUN_NETWORK_PREFIX.1/24&amp;quot; dev &amp;quot;$TUN_NETWORK_DEV&amp;quot;
ip tuntap del dev &amp;quot;$TUN_NETWORK_DEV&amp;quot; mode tun
}



start_fwd
trap stop_fwd INT TERM
wait &amp;quot;$TUN2SOCKS_PID&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vim /etc/systemd/system/socksfwd.service
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[Unit]

Description=Transparent SOCKS5 forwarding

After=network-online.target

[Service]

Type=simple

ExecStart=/usr/local/bin/socksfwd

LimitNOFILE=1048576


[Install]

WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl start socksfwd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;开机自启&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl enable socksfwd
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-7-打开流量转发&#34;&gt;3.7 打开流量转发&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat /etc/sysctl.d/30-ipforward.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;net.ipv4.ip_forward=1

net.ipv6.conf.all.forwarding = 1

net.ipv4.tcp_congestion_control=westwood

net.ipv4.tcp_syn_retries = 5

net.ipv4.tcp_synack_retries = 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑完成后，执行以下命令使变动立即生效&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sysctl -p
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title> 修复 Service Endpoint 更新的延迟</title>
      <link>https://www.yangcs.net/posts/kubernetes-fixing-delayed-service-endpoint-updates/</link>
      <pubDate>Fri, 15 Jun 2018 14:02:11 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/kubernetes-fixing-delayed-service-endpoint-updates/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;几个月前，我在更新 Kubernetes 集群中的 &lt;code&gt;Deployment&lt;/code&gt; 时发现了一个很奇怪的连接超时现象，在更新 Deployment 之后的 30 秒到两分钟左右，所有与以该 Deployment 作为服务后端的 &lt;code&gt;Service&lt;/code&gt; 的连接都会超时或失败。同时我还注意到其他应用在这段时间内也会出现莫名其妙的延迟现象。&lt;/p&gt;

&lt;p&gt;一开始我怀疑是&lt;a href=&#34;https://hackernoon.com/graceful-shutdown-in-kubernetes-435b98794461&#34; target=&#34;_blank&#34;&gt;应用没有优雅删除&lt;/a&gt;导致的，但当我在更新 Deployment 的过程中（删除旧的 Pod，启动新的 Pod）通过 &lt;code&gt;curl&lt;/code&gt; 来测试该应用的健康检查（liveness）和就绪检查（readiness）&lt;code&gt;Endpoints&lt;/code&gt; 时，很快就排除了这个可能性。&lt;/p&gt;

&lt;p&gt;我开始怀疑人生，开始怀疑我的职业选择，几个小时之后我忽然想起来 &lt;code&gt;Service&lt;/code&gt; 并不是直接与 Deployment 关联的，而是按照标签对一组提供相同功能的 Pods 的抽象，并为它们提供一个统一的入口。更重要的是，Service 是由一组 &lt;code&gt;Endpoint&lt;/code&gt; 组成的，只要 Service 中的一组 Pod 发生变更，Endpoint 就会被更新。&lt;/p&gt;

&lt;p&gt;想到这里，就可以继续排查问题了。下面在更新 Deployment 的过程中通过 &lt;code&gt;watch&lt;/code&gt; 命令来观察有问题的 Service 的 Endpoint。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ watch kubectl describe endpoints [endpoint name]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后我就发现了罪魁祸首，在旧 Pod 被移除的 30 秒到几分钟左右的时间段内，这些被删除的 Pod 的 &lt;code&gt;IP:Port&lt;/code&gt; 仍然出现在 Endpoint 的就绪列表中，同时新启动的 Pod 的 &lt;code&gt;IP:Port&lt;/code&gt; 也没有被添加到 Endpoint 中。终于发现了连接失败的根源，但是为什么会出现这种状况呢？仍然无解。&lt;/p&gt;

&lt;p&gt;又经历了几天折腾之后，我又有了新点子，那就是调试负责更新 Endpoint 的组件：&lt;code&gt;kube-controller-manager&lt;/code&gt;，最后终于在 kube-controller-manager 的日志输出中发现了如下可疑的信息：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;I0412 22:59:59.914517       1 request.go:638] Throttling request took 2.489742918s, request: GET:https://10.3.0.1:443/api/v1/namespaces/[some namespace]/endpoints/[some endpoints]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但还是感觉哪里不对劲，明明延迟了几分钟，为什么这里显示的只有两秒？&lt;/p&gt;

&lt;p&gt;在阅读了 kube-controller-manager 的源码后，我发现了问题所在。Kube-controller-manager 的主要职责是通过内部的众多 &lt;code&gt;Controller&lt;/code&gt; 将集群的当前状态调整到期望状态，其中 &lt;code&gt;Endpoint Controller&lt;/code&gt; 用于监控 Pod 的生命周期事件并根据这些事件更新 Endpoint。&lt;/p&gt;

&lt;p&gt;Endpoint Controller 内部运行了一组 &lt;code&gt;workers&lt;/code&gt; 来处理这些事件并更新 Endpoint，如果有足够多的对 Endpoint 发起的请求被阻塞，那么所有的 workers 都会忙于等待被阻塞的请求，这时候新事件只能被添加到队列中排队等待，如果该队列很长，就会花很长时间来更新 Endpoint。&lt;/p&gt;

&lt;p&gt;为了解决这个问题，首先我通过调整 kube-controller-manager 的 参数 &lt;code&gt;--concurrent-endpoints-syncs&lt;/code&gt; 来增加 Endpoint Controller 的 workers，但收效甚微。&lt;/p&gt;

&lt;p&gt;再次仔细阅读源码后，我找到了两个可以可以扭转战局的参数：&lt;code&gt;--kube-api-qps&lt;/code&gt; 和 &lt;code&gt;--kube-api-burst&lt;/code&gt;。kube-controller-manager 可以通过这两个参数来限制任何 Controller（包括 Endpoint Controller）对 kube-apiserver 发起的请求的速率。&lt;/p&gt;

&lt;p&gt;这两个参数的默认值是 20，但当集群中的主机数量非常多时，默认值显然不满足集群运行的工作负载。经过不断调试之后，我将参数 &lt;code&gt;--kube-api-qps&lt;/code&gt; 的值设置为 300，将 &lt;code&gt;--kube-api-burst&lt;/code&gt; 的值设置为 325，上面的日志信息便消失了，同时添加或移除 Pod 时 Endpoint 也能够立即更新。&lt;/p&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;&lt;code&gt;--kube-api-qps&lt;/code&gt; 和 &lt;code&gt;--kube-api-burst&lt;/code&gt; 参数的值越大，kube-apiserver 和 etcd 的负载就越高。在我的集群中，通过适当地增加一些负载来解决这个问题是很值得的。&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&#34;p-id-h2-原文链接-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;原文链接&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://engineering.dollarshaveclub.com/kubernetes-fixing-delayed-service-endpoint-updates-fd4d0a31852c&#34; target=&#34;_blank&#34;&gt;Kubernetes: Fixing Delayed Service Endpoint Updates&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes 的奇技淫巧</title>
      <link>https://www.yangcs.net/posts/kubernetes-fucking-trick/</link>
      <pubDate>Mon, 11 Jun 2018 04:35:48 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/kubernetes-fucking-trick/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes 作为云原生时代的“操作系统”，熟悉和使用它是每名用户（User）的必备技能。如果你正在 Kubernetes 上工作，你需要正确的工具和技巧来确保 Kubernetes 集群的高可用以及工作负载的稳定运行。&lt;/p&gt;

&lt;p&gt;随着 Kubernetes 的发展和演变，人们可以从内部来驯服它的无节制行为。但有些人并不情愿干等 Kubernetes 变得易于使用，并且为已投入生产的 Kubernetes 中遇到的很多常见问题制定了自己的解决方案。&lt;/p&gt;

&lt;p&gt;这里我们将介绍一些提高操作效率的技巧，同时列举几个比较有用的开源 Kubernetes 工具，这些工具以各种方式简化 Kubernetes，包括简化命令行交互，简化应用程序部署语法等。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-kubectl-自动补全-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;kubectl 自动补全&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;kubectl&lt;/code&gt; 这个命令行工具非常重要，与之相关的命令也很多，我们也记不住那么多的命令，而且也会经常写错，所以命令自动补全是很有必要的，kubectl 工具本身就支持自动补全，只需简单设置一下即可。&lt;/p&gt;

&lt;h3 id=&#34;bash-用户&#34;&gt;bash 用户&lt;/h3&gt;

&lt;p&gt;大多数用户的 shell 使用的是 &lt;code&gt;bash&lt;/code&gt;，Linux 系统可以通过下面的命令来设置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;$ echo &amp;quot;source &amp;lt;(kubectl completion bash)&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
$ source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果发现不能自动补全，可以尝试安装 &lt;code&gt;bash-completion&lt;/code&gt; 然后刷新即可！&lt;/p&gt;

&lt;h3 id=&#34;zsh-用户&#34;&gt;zsh 用户&lt;/h3&gt;

&lt;p&gt;如果你使用的 shell 是 &lt;code&gt;zsh&lt;/code&gt;，可以通过下面的命令来设置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ echo &amp;quot;source &amp;lt;(kubectl completion zsh)&amp;quot; &amp;gt;&amp;gt; ~/.zshrc
$ source ~/.zshrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-自定义-kubectl-get-输出-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;自定义 kubectl get 输出&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;kubectl get&lt;/code&gt; 相关资源，默认输出为 kubectl 内置，一般我们也可以使用 &lt;code&gt;-o json&lt;/code&gt; 或者 &lt;code&gt;-o yaml&lt;/code&gt; 查看其完整的资源信息。但是很多时候，我们需要关心的信息并不全面，因此我们需要自定义输出的列，那么可以使用 &lt;code&gt;go-template&lt;/code&gt; 来进行实现。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;go-template&lt;/code&gt; 是 golang 的一种模板，可以参考 &lt;a href=&#34;https://golang.org/pkg/text/template/&#34; target=&#34;_blank&#34;&gt;template的相关说明&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;比如仅仅想要查看获取的 pods 中的各个 pod 的 &lt;code&gt;uid&lt;/code&gt;，则可以使用以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ini&#34;&gt;$ kubectl get pods --all-namespaces -o go-template=&#39;{{range .items}}{{.metadata.uid}}
{{end}}&#39;

2ea418d4-533e-11e8-b722-005056a1bc83
7178b8bf-4e93-11e8-8175-005056a1bc83
a0341475-5338-11e8-b722-005056a1bc83
...
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;$ kubectl get pods -o yaml

apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    name: nginx-deployment-1751389443-26gbm
    namespace: default
    uid: a911e34b-f445-11e7-9cda-40f2e9b98448
  ...
- apiVersion: v1
  kind: Pod
  metadata:
    name: nginx-deployment-1751389443-rsbkc
    namespace: default
    uid: a911d2d2-f445-11e7-9cda-40f2e9b98448
  ...
- apiVersion: v1
  kind: Pod
  metadata:
    name: nginx-deployment-1751389443-sdbkx
    namespace: default
    uid: a911da1a-f445-11e7-9cda-40f2e9b98448
    ...
kind: List
metadata: {}
resourceVersion: &amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为 get pods 的返回结果是 &lt;code&gt;List&lt;/code&gt; 类型，获取的 pods 都在 &lt;code&gt;items&lt;/code&gt; 这个的 value 中，因此需要遍历 items，也就有了 &lt;code&gt;{{range .items}}&lt;/code&gt;。而后通过模板选定需要展示的内容，就是 items 中的每个 &lt;code&gt;{{.metadata.uid}}&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;这里特别注意，要做一个特别的处理，就是要把 &lt;code&gt;{{end}}&lt;/code&gt; 前进行换行，以便在模板中插入换行符。&lt;/p&gt;

&lt;p&gt;当然，如果觉得这样处理不优雅的话，也可以使用 &lt;code&gt;printf&lt;/code&gt; 函数，在其中使用 &lt;code&gt;\n&lt;/code&gt; 即可实现换行符的插入。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get pods --all-namespaces -o go-template --template=&#39;{{range .items}}{{printf &amp;quot;%s\n&amp;quot; .metadata.uid}}{{end}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者可以这样：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get pods --all-namespaces -o go-template --template=&#39;{{range .items}}{{.metadata.uid}}{{&amp;quot;\n&amp;quot;}}{{end}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实有了 &lt;code&gt;printf&lt;/code&gt;，就可以很容易的实现对应字段的输出，且样式可以进行自己控制。比如可以这样&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get pods --all-namespaces -o go-template --template=&#39;{{range .items}}{{printf &amp;quot;|%-20s|%-50s|%-30s|\n&amp;quot; .metadata.namespace .metadata.name .metadata.uid}}{{end}}&#39;

|default             |details-v1-64b86cd49-85vks                        |2e7a2a66-533e-11e8-b722-005056a1bc83|
|default             |productpage-v1-84f77f8747-7tkwb                   |2eb4e840-533e-11e8-b722-005056a1bc83|
|default             |ratings-v1-5f46655b57-qlrxp                       |2e89f981-533e-11e8-b722-005056a1bc83|
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面举两个 go-template 高级用法的例子：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;range 嵌套&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 列出所有容器使用的镜像名
$ kubectl get pods --all-namespaces -o go-template --template=&#39;{{range .items}}{{range .spec.containers}}{{printf &amp;quot;%s\n&amp;quot; .image}}{{end}}{{end}}&#39;

istio/examples-bookinfo-details-v1:1.5.0
istio/examples-bookinfo-productpage-v1:1.5.0
istio/examples-bookinfo-ratings-v1:1.5.0
...
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;条件判断&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 列出所有不可调度节点的节点名与 IP
$ kubectl get no -o go-template=&#39;{{range .items}}{{if .spec.unschedulable}}{{.metadata.name}} {{.spec.externalID}}{{&amp;quot;\n&amp;quot;}}{{end}}{{end}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除了使用 &lt;code&gt;go-template&lt;/code&gt; 之外，还可以使用逗号分隔的自定义列列表打印表格：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n kube-system get pods coredns-64b597b598-7547d -o custom-columns=NAME:.metadata.name,hostip:.status.hostIP

NAME                       hostip
coredns-64b597b598-7547d   192.168.123.250
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以使用 &lt;code&gt;go-template-file&lt;/code&gt; 自定义模板列表，模板不用通过参数传进去，而是写成一个文件，然后需要指定 &lt;code&gt;template&lt;/code&gt; 指向该文件即可。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat &amp;gt; test.tmpl &amp;lt;&amp;lt; EOF 
NAME                      HOSTIP
metadata.name       status.hostIP
EOF

$ kubectl -n kube-system get pods coredns-64b597b598-7547d -o custom-columns-file=test.tmpl

NAME                       HOSTIP
coredns-64b597b598-7547d   192.168.123.250
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-kube-prompt-https-github-com-c-bata-kube-prompt-交互式-kubernetes-客户端-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;&lt;a href=&#34;https://github.com/c-bata/kube-prompt&#34; target=&#34;_blank&#34;&gt;Kube-prompt&lt;/a&gt;：交互式 Kubernetes 客户端&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;Kube-prompt&lt;/code&gt; 可以让你在 Kubernetes 客户端输入相当于交互式命令会话的东西，并为每个命令提供自动填充的背景信息，你不必键入 kubectl 来为每个命令添加前缀。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/kube-prompt.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-kubectl-aliases-https-github-com-ahmetb-kubectl-aliases-生成-kubectl-别名-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;&lt;a href=&#34;https://github.com/ahmetb/kubectl-aliases&#34; target=&#34;_blank&#34;&gt;Kubectl Aliases&lt;/a&gt;：生成 kubectl 别名&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;如果你需要频繁地使用 kubectl 和 kubernetes api 进行交互，使用别名将会为你节省大量的时间，开源项目 &lt;a href=&#34;https://github.com/ahmetb/kubectl-aliases&#34; target=&#34;_blank&#34;&gt;kubectl-aliases&lt;/a&gt; 可以通过编程的方式生成 kubectl 别名，别名生成规则如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/kubectl-alias.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;简单别名示例&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;font color=red&gt;kd&lt;/font&gt; → &lt;font color=red&gt;k&lt;/font&gt;ubectl &lt;font color=red&gt;d&lt;/font&gt;escribe&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;高级别名示例&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;font color=red&gt;kgdepallw&lt;/font&gt; → &lt;font color=red&gt;k&lt;/font&gt;ubectl &lt;font color=red&gt;g&lt;/font&gt;et &lt;font color=red&gt;dep&lt;/font&gt;loyment &amp;ndash;&lt;font color=red&gt;all&lt;/font&gt;-namespaces &amp;ndash;&lt;font color=red&gt;w&lt;/font&gt;atch&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-kubeval-https-github-com-garethr-kubeval-校验配置文件-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;&lt;a href=&#34;https://github.com/garethr/kubeval&#34; target=&#34;_blank&#34;&gt;Kubeval&lt;/a&gt;：校验配置文件&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;如果你手动写 Kubernetes manifest 文件，检查 manifest 文件的语法是很困难的，特别是当你有多个不同版本的 Kubernetes 集群时，确认配置文件语法是否正确更是难上加难。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/garethr/kubeval&#34; target=&#34;_blank&#34;&gt;Kubeval&lt;/a&gt; 是一个用于校验Kubernetes YAML或JSON配置文件的工具，支持多个Kubernetes版本，可以帮助我们解决不少的麻烦。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用示例&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubeval nginx.yaml

The document nginx.yaml contains an invalid Deployment
---&amp;gt; spec.replicas: Invalid type. Expected: integer, given: string
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-kedge-http-kedgeproject-org-简化-kubernetes-部署定义-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;&lt;a href=&#34;http://kedgeproject.org/&#34; target=&#34;_blank&#34;&gt;Kedge&lt;/a&gt;：简化 Kubernetes 部署定义&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;很多人都抱怨 Kubernetes manifest 文件的定义太复杂和冗长。它们很难写，而且很难维护，如果能够简化部署定义就会极大地降低维护难度。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://kedgeproject.org/&#34; target=&#34;_blank&#34;&gt;Kedge&lt;/a&gt; 提供更简单、更简洁的语法，然后 kedge 将其转换为 Kubernetes manifest 文件。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用示例&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# Web server Kedge example
name: httpd
deployments:
- containers:
  - image: centos/httpd
services:
- name: httpd
  type: LoadBalancer
  portMappings: 
    - 8080:80
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# Converted Kubernetes artifact file(s)
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: httpd
  name: httpd
spec:
  ports:
  - name: httpd-8080
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: httpd
  type: LoadBalancer
status:
  loadBalancer: {}
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: httpd
  name: httpd
spec:
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: httpd
      name: httpd
    spec:
      containers:
      - image: centos/httpd
        name: httpd
        resources: {}
status: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-参考-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;参考&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://segmentfault.com/a/1190000014526263?utm_source=tag-newest&#34; target=&#34;_blank&#34;&gt;为高效 Ops 和 SRE 团队准备的 10 个开源 k8s 工具&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/posts/configuring-efficient-kubernetes-cli-terminal/&#34; target=&#34;_blank&#34;&gt;打造高效的Kubernetes命令行终端&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>kubectl run 背后到底发生了什么？</title>
      <link>https://www.yangcs.net/posts/what-happens-when-k8s/</link>
      <pubDate>Fri, 01 Jun 2018 11:36:45 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/what-happens-when-k8s/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;想象一下，如果我想将 nginx 部署到 Kubernetes 集群，我可能会在终端中输入类似这样的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl run --image=nginx --replicas=3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后回车。几秒钟后，你就会看到三个 nginx pod 分布在所有的工作节点上。这一切就像变魔术一样，但你并不知道这一切的背后究竟发生了什么事情。&lt;/p&gt;

&lt;p&gt;Kubernetes 的神奇之处在于：它可以通过用户友好的 API 来处理跨基础架构的 &lt;code&gt;deployments&lt;/code&gt;，而背后的复杂性被隐藏在简单的抽象中。但为了充分理解它为我们提供的价值，我们需要理解它的内部原理。&lt;/p&gt;

&lt;p&gt;本指南将引导您理解从 client 到 &lt;code&gt;Kubelet&lt;/code&gt; 的请求的完整生命周期，必要时会通过源代码来说明背后发生了什么。&lt;/p&gt;

&lt;p&gt;这是一份可以在线修改的文档，如果你发现有什么可以改进或重写的，欢迎提供帮助！&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-kubectl-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. kubectl&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;验证和生成器&#34;&gt;验证和生成器&lt;/h3&gt;

&lt;p&gt;当敲下回车键以后，&lt;code&gt;kubectl&lt;/code&gt; 首先会执行一些客户端验证操作，以确保不合法的请求（例如，创建不支持的资源或使用格式错误的镜像名称）将会快速失败，也不会发送给 &lt;code&gt;kube-apiserver&lt;/code&gt;。通过减少不必要的负载来提高系统性能。&lt;/p&gt;

&lt;p&gt;验证通过之后， kubectl 开始将发送给 kube-apiserver 的 HTTP 请求进行封装。&lt;code&gt;kube-apiserver&lt;/code&gt; 与 etcd 进行通信，所有尝试访问或更改 Kubernetes 系统状态的请求都会通过 kube-apiserver 进行，kubectl 也不例外。kubectl 使用生成器（&lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl-conventions/#generators&#34; target=&#34;_blank&#34;&gt;generators&lt;/a&gt;）来构造 HTTP 请求。生成器是一个用来处理序列化的抽象概念。&lt;/p&gt;

&lt;p&gt;通过 &lt;code&gt;kubectl run&lt;/code&gt; 不仅可以运行 &lt;code&gt;deployment&lt;/code&gt;，还可以通过指定参数 &lt;code&gt;--generator&lt;/code&gt; 来部署其他多种资源类型。如果没有指定 &lt;code&gt;--generator&lt;/code&gt; 参数的值，kubectl 将会自动判断资源的类型。&lt;/p&gt;

&lt;p&gt;例如，带有参数 &lt;code&gt;--restart-policy=Always&lt;/code&gt; 的资源将被部署为 Deployment，而带有参数 &lt;code&gt;--restart-policy=Never&lt;/code&gt; 的资源将被部署为 Pod。同时 kubectl 也会检查是否需要触发其他操作，例如记录命令（用来进行回滚或审计）。&lt;/p&gt;

&lt;p&gt;在 kubectl 判断出要创建一个 Deployment 后，它将使用 &lt;code&gt;DeploymentV1Beta1&lt;/code&gt; 生成器从我们提供的参数中生成一个&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/7650665059e65b4b22375d1e28da5306536a12fb/pkg/kubectl/run.go#L59&#34; target=&#34;_blank&#34;&gt;运行时对象&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;api-版本协商与-api-组&#34;&gt;API 版本协商与 API 组&lt;/h3&gt;

&lt;p&gt;为了更容易地消除字段或者重新组织资源结构，Kubernetes 支持多个 API 版本，每个版本都在不同的 API 路径下，例如 &lt;code&gt;/api/v1&lt;/code&gt; 或者 &lt;code&gt;/apis/extensions/v1beta1&lt;/code&gt;。不同的 API 版本表明不同的稳定性和支持级别，更详细的描述可以参考 &lt;a href=&#34;https://k8smeetup.github.io/docs/reference/api-overview/&#34; target=&#34;_blank&#34;&gt;Kubernetes API 概述&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;API 组旨在对类似资源进行分类，以便使得 Kubernetes API 更容易扩展。API 的组别在 REST 路径或者序列化对象的 &lt;code&gt;apiVersion&lt;/code&gt; 字段中指定。例如，Deployment 的 API 组名是 &lt;code&gt;apps&lt;/code&gt;，最新的 API 版本是 &lt;code&gt;v1beta2&lt;/code&gt;，这就是为什么你要在 Deployment manifests 顶部输入 &lt;code&gt;apiVersion: apps/v1beta2&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;kubectl 在生成运行时对象后，开始为它&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/cmd/run.go#L580-L597&#34; target=&#34;_blank&#34;&gt;找到适当的 API 组和 API 版本&lt;/a&gt;，然后&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/cmd/run.go#L598&#34; target=&#34;_blank&#34;&gt;组装成一个版本化客户端&lt;/a&gt;，该客户端知道资源的各种 REST 语义。该阶段被称为版本协商，kubectl 会扫描 &lt;code&gt;remote API&lt;/code&gt; 上的 &lt;code&gt;/apis&lt;/code&gt; 路径来检索所有可能的 API 组。由于 kube-apiserver 在 &lt;code&gt;/apis&lt;/code&gt; 路径上公开了 OpenAPI 格式的模式文档， 因此客户端很容易找到合适的 API。&lt;/p&gt;

&lt;p&gt;为了提高性能，kubectl &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/7650665059e65b4b22375d1e28da5306536a12fb/pkg/kubectl/cmd/util/factory_client_access.go#L117&#34; target=&#34;_blank&#34;&gt;将 OpenAPI 模式缓存&lt;/a&gt;到了 &lt;code&gt;~/.kube/cache&lt;/code&gt; 目录。如果你想了解 API 发现的过程，请尝试删除该目录并在运行 kubectl 命令时将 &lt;code&gt;-v&lt;/code&gt; 参数的值设为最大值，然后你将会看到所有试图找到这些 API 版本的HTTP 请求。参考 &lt;a href=&#34;https://k8smeetup.github.io/docs/reference/kubectl/cheatsheet/&#34; target=&#34;_blank&#34;&gt;kubectl 备忘单&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;最后一步才是真正地发送 HTTP 请求。一旦请求发送之后获得成功的响应，kubectl 将会根据所需的输出格式打印 success message。&lt;/p&gt;

&lt;h3 id=&#34;客户端身份认证&#34;&gt;客户端身份认证&lt;/h3&gt;

&lt;p&gt;在发送 HTTP 请求之前还要进行客户端认证，这是之前没有提到的，现在可以来看一下。&lt;/p&gt;

&lt;p&gt;为了能够成功发送请求，kubectl 需要先进行身份认证。用户凭证保存在 &lt;code&gt;kubeconfig&lt;/code&gt; 文件中，kubectl 通过以下顺序来找到 kubeconfig 文件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果提供了 &lt;code&gt;--kubeconfig&lt;/code&gt; 参数， kubectl 就使用 &amp;ndash;kubeconfig 参数提供的 kubeconfig 文件。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;如果没有提供 &amp;ndash;kubeconfig 参数，但设置了环境变量 &lt;code&gt;$KUBECONFIG&lt;/code&gt;，则使用该环境变量提供的 kubeconfig 文件。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;如果 &amp;ndash;kubeconfig 参数和环境变量 &lt;code&gt;$KUBECONFIG&lt;/code&gt; 都没有提供，kubectl 就使用默认的 kubeconfig 文件 &lt;code&gt;$HOME/.kube/config&lt;/code&gt;。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;解析完 kubeconfig 文件后，kubectl 会确定当前要使用的上下文、当前指向的群集以及与当前用户关联的任何认证信息。如果用户提供了额外的参数（例如 &amp;ndash;username），则优先使用这些参数覆盖 kubeconfig 中指定的值。一旦拿到这些信息之后， kubectl 就会把这些信息填充到将要发送的 HTTP 请求头中：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;x509 证书使用 &lt;a href=&#34;https://github.com/kubernetes/client-go/blob/82aa063804cf055e16e8911250f888bc216e8b61/rest/transport.go#L80-L89&#34; target=&#34;_blank&#34;&gt;tls.TLSConfig&lt;/a&gt; 发送（包括 CA 证书）。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bearer tokens&lt;/code&gt; 在 HTTP 请求头 &lt;code&gt;Authorization&lt;/code&gt; 中&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/c6f8cf2c47d21d55fa0df928291b2580544886c8/transport/round_trippers.go#L314&#34; target=&#34;_blank&#34;&gt;发送&lt;/a&gt;。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;用户名和密码通过 HTTP 基本认证&lt;a href=&#34;https://github.com/kubernetes/client-go/blob/c6f8cf2c47d21d55fa0df928291b2580544886c8/transport/round_trippers.go#L223&#34; target=&#34;_blank&#34;&gt;发送&lt;/a&gt;。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;OpenID&lt;/code&gt; 认证过程是由用户事先手动处理的，产生一个像 bearer token 一样被发送的 token。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;p-id-h2-2-kube-apiserver-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. kube-apiserver&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;认证-https-k8smeetup-github-io-docs-admin-authentication&#34;&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/admin/authentication/&#34; target=&#34;_blank&#34;&gt;认证&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;现在我们的请求已经成功发送了，接下来将会发生什么？这时候就该 &lt;code&gt;kube-apiserver&lt;/code&gt; 闪亮登场了！kube-apiserver 是客户端和系统组件用来保存和检索集群状态的主要接口。为了执行相应的功能，kube-apiserver 需要能够验证请求者是合法的，这个过程被称为认证。&lt;/p&gt;

&lt;p&gt;那么 apiserver 如何对请求进行认证呢？当 kube-apiserver 第一次启动时，它会查看用户提供的所有 &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/&#34; target=&#34;_blank&#34;&gt;CLI 参数&lt;/a&gt;，并组合成一个合适的令牌列表。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;举个例子：&lt;/strong&gt;如果提供了 &lt;code&gt;--client-ca-file&lt;/code&gt; 参数，则会将 x509 客户端证书认证添加到令牌列表中；如果提供了 &lt;code&gt;--token-auth-file&lt;/code&gt; 参数，则会将 breaer token 添加到令牌列表中。&lt;/p&gt;

&lt;p&gt;每次收到请求时，apiserver 都会&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/51bebaffa01be9dc28195140da276c2f39a10cd4/pkg/authentication/request/union/union.go#L54&#34; target=&#34;_blank&#34;&gt;通过令牌链进行认证，直到某一个认证成功为止&lt;/a&gt;：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/51bebaffa01be9dc28195140da276c2f39a10cd4/pkg/authentication/request/x509/x509.go#L60&#34; target=&#34;_blank&#34;&gt;x509 处理程序&lt;/a&gt;将验证 HTTP 请求是否是由 CA 根证书签名的 TLS 密钥进行编码的。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/51bebaffa01be9dc28195140da276c2f39a10cd4/pkg/authentication/request/bearertoken/bearertoken.go#L38&#34; target=&#34;_blank&#34;&gt;bearer token 处理程序&lt;/a&gt;将验证 &lt;code&gt;--token-auth-file&lt;/code&gt; 参数提供的 token 文件是否存在。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/51bebaffa01be9dc28195140da276c2f39a10cd4/plugin/pkg/authenticator/request/basicauth/basicauth.go#L37&#34; target=&#34;_blank&#34;&gt;基本认证处理程序&lt;/a&gt;确保 HTTP 请求的基本认证凭证与本地的状态匹配。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/20bfbdf738a0643fe77ffd527b88034dcde1b8e3/pkg/authentication/request/union/union.go#L71&#34; target=&#34;_blank&#34;&gt;认证失败&lt;/a&gt;，则请求失败并返回相应的错误信息；如果验证成功，则将请求中的 &lt;code&gt;Authorization&lt;/code&gt; 请求头删除，并&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/e30df5e70ef9127ea69d607207c894251025e55b/pkg/endpoints/filters/authentication.go#L71-L75&#34; target=&#34;_blank&#34;&gt;将用户信息添加到&lt;/a&gt;其上下文中。这给后续的授权和准入控制器提供了访问之前建立的用户身份的能力。&lt;/p&gt;

&lt;h3 id=&#34;授权-https-k8smeetup-github-io-docs-admin-authorization&#34;&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/admin/authorization/&#34; target=&#34;_blank&#34;&gt;授权&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;OK，现在请求已经发送，并且 kube-apiserver 已经成功验证我们是谁，终于解脱了！&lt;/p&gt;

&lt;p&gt;然而事情并没有结束，虽然我们已经证明了&lt;strong&gt;我们是合法的&lt;/strong&gt;，但我们有权执行此操作吗？毕竟身份和权限不是一回事。为了进行后续的操作，kube-apiserver 还要对用户进行授权。&lt;/p&gt;

&lt;p&gt;kube-apiserver 处理授权的方式与处理身份验证的方式相似：通过 kube-apiserver 的启动参数 &lt;code&gt;--authorization_mode&lt;/code&gt; 参数设置。它将组合一系列授权者，这些授权者将针对每个传入的请求进行授权。如果所有授权者都拒绝该请求，则该请求会被禁止响应并且&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/e30df5e70ef9127ea69d607207c894251025e55b/pkg/endpoints/filters/authorization.go#L60&#34; target=&#34;_blank&#34;&gt;不会再继续响应&lt;/a&gt;。如果某个授权者批准了该请求，则请求继续。&lt;/p&gt;

&lt;p&gt;kube-apiserver 目前支持以下几种授权方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/admin/authorization/webhook/&#34; target=&#34;_blank&#34;&gt;webhook&lt;/a&gt;: 它与集群外的 HTTP(S) 服务交互。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/admin/authorization/abac/&#34; target=&#34;_blank&#34;&gt;ABAC&lt;/a&gt;: 它执行静态文件中定义的策略。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/admin/authorization/rbac/&#34; target=&#34;_blank&#34;&gt;RBAC&lt;/a&gt;: 它使用 &lt;code&gt;rbac.authorization.k8s.io&lt;/code&gt;  API Group实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/admin/authorization/node/&#34; target=&#34;_blank&#34;&gt;Node&lt;/a&gt;: 它确保 kubelet 只能访问自己节点上的资源。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;准入控制-https-k8smeetup-github-io-docs-admin-admission-controllers&#34;&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/admin/admission-controllers/&#34; target=&#34;_blank&#34;&gt;准入控制&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;突破了之前所说的认证和授权两道关口之后，客户端的调用请求就能够得到 API Server 的真正响应了吗？答案是：不能！&lt;/p&gt;

&lt;p&gt;从 kube-apiserver 的角度来看，它已经验证了我们的身份并且赋予了相应的权限允许我们继续，但对于 Kubernetes 而言，其他组件对于应不应该允许发生的事情还是很有意见的。所以这个请求还需要通过 &lt;code&gt;Admission Controller&lt;/code&gt; 所控制的一个 &lt;code&gt;准入控制链&lt;/code&gt; 的层层考验，官方标准的 “关卡” 有近十个之多，而且还能自定义扩展！&lt;/p&gt;

&lt;p&gt;虽然授权的重点是回答用户是否有权限，但准入控制器会拦截请求以确保它符合集群的更广泛的期望和规则。它们是资源对象保存到 &lt;code&gt;etcd&lt;/code&gt; 之前的最后一个堡垒，封装了一系列额外的检查以确保操作不会产生意外或负面结果。不同于授权和认证只关心请求的用户和操作，准入控制还处理请求的内容，并且仅对创建、更新、删除或连接（如代理）等有效，而对读操作无效。&lt;/p&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;准入控制器的工作方式与授权者和验证者的工作方式类似，但有一点区别：与验证链和授权链不同，如果某个准入控制器检查不通过，则整个链会中断，整个请求将立即被拒绝并且返回一个错误给终端用户。&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;准入控制器设计的重点在于提高可扩展性，某个控制器都作为一个插件存储在 &lt;code&gt;plugin/pkg/admission&lt;/code&gt; 目录中，并且与某一个接口相匹配，最后被编译到 kube-apiserver 二进制文件中。&lt;/p&gt;

&lt;p&gt;大部分准入控制器都比较容易理解，接下来着重介绍 &lt;code&gt;SecurityContextDeny&lt;/code&gt;、&lt;code&gt;ResourceQuota&lt;/code&gt; 及 &lt;code&gt;LimitRanger&lt;/code&gt; 这三个准入控制器。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;SecurityContextDeny&lt;/span&gt; 该插件将禁止创建设置了 Security Context 的 Pod。&lt;br /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;ResourceQuota&lt;/span&gt; 不仅能限制某个 Namespace 中创建资源的数量，而且能限制某个 Namespace 中被 Pod 所请求的资源总量。该准入控制器和资源对象 &lt;code&gt;ResourceQuota&lt;/code&gt; 一起实现了资源配额管理。&lt;br /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;LimitRanger&lt;/span&gt; 作用类似于上面的 ResourceQuota 控制器，针对 Namespace 资源的每个个体（Pod 与 Container 等）的资源配额。该插件和资源对象 &lt;code&gt;LimitRange&lt;/code&gt; 一起实现资源配额管理。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;p-id-h2-3-etcd-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. etcd&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;到现在为止，Kubernetes 已经对该客户端的调用请求进行了全面彻底地审查，并且已经验证通过，运行它进入下一个环节。下一步 kube-apiserver 将对 HTTP 请求进行反序列化，然后利用得到的结果构建运行时对象（有点像 kubectl 生成器的逆过程），并保存到 &lt;code&gt;etcd&lt;/code&gt; 中。下面我们将这个过程分解一下。&lt;/p&gt;

&lt;p&gt;当收到请求时，kube-apiserver 是如何知道它该怎么做的呢？事实上，在客户端发送调用请求之前就已经产生了一系列非常复杂的流程。我们就从 kube-apiserver 二进制文件首次运行开始分析吧：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;当运行 kube-apiserver 二进制文件时，它会&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/cmd/kube-apiserver/app/server.go#L119&#34; target=&#34;_blank&#34;&gt;创建一个允许 apiserver 聚合的服务链&lt;/a&gt;。这是一种对 &lt;code&gt;Kubernetes API&lt;/code&gt; 进行扩展的方式。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;同时会创建一个 &lt;code&gt;generic apiserver&lt;/code&gt; 作为默认的 apiserver。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;然后&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/server/config.go#L149&#34; target=&#34;_blank&#34;&gt;生成 OpenAPI 规范的配置&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;然后 kube-apiserver 遍历数据结构中指定的所有 API 组，并将每一个 API 组作为通用的存储抽象保存到 etcd 中。当你访问或变更资源状态时，kube-apiserver 就会调用这些 API 组。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;每个 API 组都会遍历它的所有组版本，并且将每个 HTTP 路由&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/endpoints/groupversion.go#L92&#34; target=&#34;_blank&#34;&gt;映射到 REST 路径中&lt;/a&gt;。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;当请求的 METHOD 是 &lt;code&gt;POST&lt;/code&gt; 时，kube-apiserver 就会将请求转交给 &lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/endpoints/handlers/create.go#L37&#34; target=&#34;_blank&#34;&gt;资源创建处理器&lt;/a&gt;。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;现在 kube-apiserver 已经知道了所有的路由及其对应的 REST 路径，以便在请求匹配时知道调用哪些处理器和键值存储。多么机智的设计！现在假设客户端的 HTTP 请求已经被 kube-apiserver 收到了：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;如果处理链可以将请求与已经注册的路由进行匹配，就会将该请求交给注册到该路由的&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/server/handler.go#L143&#34; target=&#34;_blank&#34;&gt;专用处理器&lt;/a&gt;来处理；如果没有任何一个路由可以匹配该请求，就会将请求转交给&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/server/mux/pathrecorder.go#L248&#34; target=&#34;_blank&#34;&gt;基于路径的处理器&lt;/a&gt;（比如当调用 &lt;code&gt;/apis&lt;/code&gt; 时）；如果没有任何一个基于路径的处理器注册到该路径，请求就会被转交给 &lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/server/mux/pathrecorder.go#L254&#34; target=&#34;_blank&#34;&gt;not found 处理器&lt;/a&gt;，最后返回 &lt;code&gt;404&lt;/code&gt;。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;幸运的是，我们有一个名为 &lt;code&gt;createHandler&lt;/code&gt; 的注册路由！它有什么作用呢？首先它会解码 HTTP 请求并进行基本的验证，例如确保请求提供的 json 与 API 资源的版本相匹配。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;接下来进入&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/endpoints/handlers/create.go#L93-L104&#34; target=&#34;_blank&#34;&gt;审计和准入控制&lt;/a&gt;阶段。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;然后资源将会通过 &lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/19667a1afc13cc13930c40a20f2c12bbdcaaa246/pkg/registry/generic/registry/store.go#L327&#34; target=&#34;_blank&#34;&gt;storage provider&lt;/a&gt; 保存&lt;a href=&#34;https://github.com/kubernetes/apiserver/blob/7001bc4df8883d4a0ec84cd4b2117655a0009b6c/pkg/endpoints/handlers/create.go#L111&#34; target=&#34;_blank&#34;&gt;到 etcd&lt;/a&gt; 中。默认情况下保存到 etcd 中的键的格式为 &lt;code&gt;&amp;lt;namespace&amp;gt;/&amp;lt;name&amp;gt;&lt;/code&gt;，你也可以自定义。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;资源创建过程中出现的任何错误都会被捕获，最后 &lt;code&gt;storage provider&lt;/code&gt; 会执行 &lt;code&gt;get&lt;/code&gt; 调用来确认该资源是否被成功创建。如果需要额外的清理工作，就会调用后期创建的处理器和装饰器。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;最后构造 HTTP 响应并返回给客户端。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;原来 apiserver 做了这么多的工作，以前竟然没有发现呢！到目前为止，我们创建的 &lt;code&gt;Deployment&lt;/code&gt; 资源已经保存到了 etcd 中，但 apiserver 仍然看不到它。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-4-初始化-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. 初始化&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;在一个资源对象被持久化到数据存储之后，apiserver 还无法完全看到或调度它，在此之前还要执行一系列&lt;a href=&#34;https://kubernetes.io/docs/admin/extensible-admission-controllers/#initializers&#34; target=&#34;_blank&#34;&gt;初始化器&lt;/a&gt;。初始化器是一种与资源类型相关联的控制器，它会在资源对外可用之前执行某些逻辑。如果某个资源类型没有初始化器，就会跳过此初始化步骤立即使资源对外可见。&lt;/p&gt;

&lt;p&gt;正如&lt;a href=&#34;https://ahmet.im/blog/initializers/&#34; target=&#34;_blank&#34;&gt;大佬的博客&lt;/a&gt;指出的那样，初始化器是一个强大的功能，因为它允许我们执行通用引导操作。例如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;将代理边车容器注入到暴露 80 端口的 Pod 中，或者加上特定的 &lt;code&gt;annotation&lt;/code&gt;。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;将保存着测试证书的 &lt;code&gt;volume&lt;/code&gt; 注入到特定命名空间的所有 Pod 中。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;如果 &lt;code&gt;Secret&lt;/code&gt; 中的密码小于 20 个字符，就组织其创建。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;initializerConfiguration&lt;/code&gt; 资源对象允许你声明某些资源类型应该运行哪些初始化器。如果你想每创建一个 Pod 时就运行一个自定义初始化器，你可以这样做：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: admissionregistration.k8s.io/v1alpha1
kind: InitializerConfiguration
metadata:
  name: custom-pod-initializer
initializers:
  - name: podimage.example.com
    rules:
      - apiGroups:
          - &amp;quot;&amp;quot;
        apiVersions:
          - v1
        resources:
          - pods
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过该配置创建资源对象 &lt;code&gt;InitializerConfiguration&lt;/code&gt; 之后，就会在每个 Pod 的 &lt;code&gt;metadata.initializers.pending&lt;/code&gt; 字段中添加 &lt;code&gt;custom-pod-initializer&lt;/code&gt; 字段。该初始化控制器会定期扫描新的 Pod，一旦在 Pod 的 &lt;code&gt;pending&lt;/code&gt; 字段中检测到自己的名称，就会执行其逻辑，执行完逻辑之后就会将 &lt;code&gt;pending&lt;/code&gt; 字段下的自己的名称删除。&lt;/p&gt;

&lt;p&gt;只有在 &lt;code&gt;pending&lt;/code&gt; 字段下的列表中的第一个初始化器可以对资源进行操作，当所有的初始化器执行完成，并且 &lt;code&gt;pending&lt;/code&gt; 字段为空时，该对象就会被认为初始化成功。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;你可能会注意到一个问题：如果 kube-apiserver 不能显示这些资源，那么用户级控制器是如何处理资源的呢？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;为了解决这个问题，kube-apiserver 暴露了一个 &lt;code&gt;?includeUninitialized&lt;/code&gt; 查询参数，它会返回所有的资源对象（包括未初始化的）。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-5-控制循环-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;5. 控制循环&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;deployments-controller&#34;&gt;Deployments controller&lt;/h3&gt;

&lt;p&gt;到了这个阶段，我们的 Deployment 记录已经保存在 etcd 中，并且所有的初始化逻辑都执行完成，接下来的阶段将会涉及到该资源所依赖的拓扑结构。在 Kubernetes 中，Deployment 实际上只是一系列 &lt;code&gt;Replicaset&lt;/code&gt; 的集合，而 Replicaset 是一系列 &lt;code&gt;Pod&lt;/code&gt; 的集合。那么 Kubernetes 是如何从一个 HTTP 请求按照层级结构依次创建这些资源的呢？其实这些工作都是由 Kubernetes 内置的 &lt;code&gt;Controller&lt;/code&gt;(控制器) 来完成的。&lt;/p&gt;

&lt;p&gt;Kubernetes 在整个系统中使用了大量的 Controller，Controller 是一个用于将系统状态从“当前状态”修正到“期望状态”的异步脚本。所有 Controller 都通过 &lt;code&gt;kube-controller-manager&lt;/code&gt; 组件并行运行，每种 Controller 都负责一种具体的控制流程。首先介绍一下 &lt;code&gt;Deployment Controller&lt;/code&gt;：&lt;/p&gt;

&lt;p&gt;将 Deployment 记录存储到 etcd 并初始化后，就可以通过 kube-apiserver 使其可见，然后 &lt;code&gt;Deployment Controller&lt;/code&gt; 就会检测到它（它的工作就是负责监听 Deployment 记录的更改）。在我们的例子中，控制器通过一个 &lt;code&gt;Informer&lt;/code&gt; &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/deployment/deployment_controller.go#L122&#34; target=&#34;_blank&#34;&gt;注册一个创建事件的特定回调函数&lt;/a&gt;（更多信息参加下文）。&lt;/p&gt;

&lt;p&gt;当 Deployment 第一次对外可见时，该 Controller 就会&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/deployment/deployment_controller.go#L170&#34; target=&#34;_blank&#34;&gt;将该资源对象添加到内部工作队列&lt;/a&gt;，然后开始处理这个资源对象：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;通过使用标签选择器查询 kube-apiserver 来&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/deployment/deployment_controller.go#L633&#34; target=&#34;_blank&#34;&gt;检查&lt;/a&gt;该 Deployment 是否有与其关联的 &lt;code&gt;ReplicaSet&lt;/code&gt; 或 &lt;code&gt;Pod&lt;/code&gt; 记录。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;有趣的是，这个同步过程是状态不可知的，它核对新记录与核对已经存在的记录采用的是相同的方式。&lt;/p&gt;

&lt;p&gt;在意识到没有与其关联的 &lt;code&gt;ReplicaSet&lt;/code&gt; 或 &lt;code&gt;Pod&lt;/code&gt; 记录后，Deployment Controller 就会开始执行&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/deployment/sync.go#L385&#34; target=&#34;_blank&#34;&gt;弹性伸缩流程&lt;/a&gt;：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;创建 ReplicaSet 资源，为其分配一个标签选择器并将其版本号设置为 1。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ReplicaSet 的 &lt;code&gt;PodSpec&lt;/code&gt; 字段从 Deployment 的 manifest 以及其他相关元数据中复制而来。有时 Deployment 记录在此之后也需要更新（例如，如果设置了 &lt;code&gt;process deadline&lt;/code&gt;）。&lt;/p&gt;

&lt;p&gt;当完成以上步骤之后，该 Deployment 的 &lt;code&gt;status&lt;/code&gt; 就会被更新，然后重新进入与之前相同的循环，等待 Deployment 与期望的状态相匹配。由于 Deployment Controller 只关心 ReplicaSet，因此需要通过 &lt;code&gt;ReplicaSet Controller&lt;/code&gt; 来继续协调。&lt;/p&gt;

&lt;h3 id=&#34;replicasets-controller&#34;&gt;ReplicaSets controller&lt;/h3&gt;

&lt;p&gt;在前面的步骤中，Deployment Controller 创建了第一个 ReplicaSet，但仍然还是没有 Pod，这时候就该 &lt;code&gt;ReplicaSet Controller&lt;/code&gt; 登场了！ReplicaSet Controller 的工作是监视 ReplicaSets 及其相关资源（Pod）的生命周期。和大多数其他 Controller 一样，它通过触发某些事件的处理器来实现此目的。&lt;/p&gt;

&lt;p&gt;当创建 ReplicaSet 时（由 Deployment Controller 创建），RS Controller &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/replicaset/replica_set.go#L583&#34; target=&#34;_blank&#34;&gt;检查新 ReplicaSet 的状态&lt;/a&gt;，并检查当前状态与期望状态之间存在的偏差，然后通过&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/replicaset/replica_set.go#L460&#34; target=&#34;_blank&#34;&gt;调整 Pod 的副本数&lt;/a&gt;来达到期望的状态。&lt;/p&gt;

&lt;p&gt;Pod 的创建也是批量进行的，从 &lt;code&gt;SlowStartInitialBatchSize&lt;/code&gt; 开始，然后在每次成功的迭代中以一种 &lt;code&gt;slow start&lt;/code&gt; 操作加倍。这样做的目的是在大量 Pod 启动失败时（例如，由于资源配额），可以减轻 kube-apiserver 被大量不必要的 HTTP 请求吞没的风险。如果创建失败，最好能够优雅地失败，并且对其他的系统组件造成的影响最小！&lt;/p&gt;

&lt;p&gt;Kubernetes 通过 &lt;code&gt;Owner References&lt;/code&gt;（在子级资源的某个字段中引用其父级资源的 ID） 来构造严格的资源对象层级结构。这确保了一旦 Controller 管理的资源被删除（级联删除），子资源就会被垃圾收集器删除，同时还为父级资源提供了一种有效的方式来避免他们竞争同一个子级资源（想象两对父母都认为他们拥有同一个孩子的场景）。&lt;/p&gt;

&lt;p&gt;Owner References 的另一个好处是：它是有状态的。如果有任何 Controller 重启了，那么由于资源对象的拓扑关系与 Controller 无关，该操作不会影响到系统的稳定运行。这种对资源隔离的重视也体现在 Controller 本身的设计中：Controller 不能对自己没有明确拥有的资源进行操作，它们应该选择对资源的所有权，互不干涉，互不共享。&lt;/p&gt;

&lt;p&gt;有时系统中也会出现孤儿（orphaned）资源，通常由以下两种途径产生：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;父级资源被删除，但子级资源没有被删除&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;垃圾收集策略禁止删除子级资源&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当发生这种情况时，Controller 将会确保孤儿资源拥有新的 &lt;code&gt;Owner&lt;/code&gt;。多个父级资源可以相互竞争同一个孤儿资源，但只有一个会成功（其他父级资源会收到验证错误）。&lt;/p&gt;

&lt;h3 id=&#34;informers&#34;&gt;Informers&lt;/h3&gt;

&lt;p&gt;你可能已经注意到，某些 Controller（例如 RBAC 授权器或 Deployment Controller）需要先检索集群状态然后才能正常运行。拿 RBAC 授权器举例，当请求进入时，授权器会将用户的初始状态缓存下来，然后用它来检索与 etcd 中的用户关联的所有  角色（&lt;code&gt;Role&lt;/code&gt;）和 角色绑定（&lt;code&gt;RoleBinding&lt;/code&gt;）。那么问题来了，Controller 是如何访问和修改这些资源对象的呢？事实上 Kubernetes 是通过 &lt;code&gt;Informer&lt;/code&gt; 机制来解决这个问题的。&lt;/p&gt;

&lt;p&gt;Infomer 是一种模式，它允许 Controller 查找缓存在本地内存中的数据(这份数据由 Informer 自己维护)并列出它们感兴趣的资源。&lt;/p&gt;

&lt;p&gt;虽然 Informer 的设计很抽象，但它在内部实现了大量的对细节的处理逻辑（例如缓存），缓存很重要，因为它不但可以减少对 Kubenetes API 的直接调用，同时也能减少 Server 和 Controller 的大量重复性工作。通过使用 Informer，不同的 Controller 之间以线程安全（Thread safety）的方式进行交互，而不必担心多个线程访问相同的资源时会产生冲突。&lt;/p&gt;

&lt;p&gt;有关 Informer 的更多详细解析，请参考这篇文章：&lt;a href=&#34;https://borismattijssen.github.io/articles/kubernetes-informers-controllers-reflectors-stores&#34; target=&#34;_blank&#34;&gt;Kubernetes: Controllers, Informers, Reflectors and Stores&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;scheduler&#34;&gt;Scheduler&lt;/h3&gt;

&lt;p&gt;当所有的 Controller 正常运行后，etcd 中就会保存一个 Deployment、一个 ReplicaSet 和 三个 Pod 资源记录，并且可以通过 kube-apiserver 查看。然而，这些 Pod 资源现在还处于 &lt;code&gt;Pending&lt;/code&gt; 状态，因为它们还没有被调度到集群中合适的 Node 上运行。这个问题最终要靠调度器（Scheduler）来解决。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Scheduler&lt;/code&gt; 作为一个独立的组件运行在集群控制平面上，工作方式与其他 Controller 相同：监听实际并将系统状态调整到期望的状态。具体来说，Scheduler 的作用是将待调度的 Pod 按照特定的算法和调度策略绑定（Binding）到集群中某个合适的 Node 上，并将绑定信息写入 etcd 中（它会过滤其 PodSpec 中 &lt;code&gt;NodeName&lt;/code&gt; 字段为空的 Pod），默认的调度算法的工作方式如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;当 Scheduler 启动时，会&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/2d64ce5e8e45e26b02492d2b6c85e5ebfb1e4761/plugin/pkg/scheduler/algorithmprovider/defaults/defaults.go#L65-L81&#34; target=&#34;_blank&#34;&gt;注册一个默认的预选策略链&lt;/a&gt;，这些&lt;code&gt;预选策略&lt;/code&gt;会对备选节点进行评估，判断备选节点是否&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/2d64ce5e8e45e26b02492d2b6c85e5ebfb1e4761/plugin/pkg/scheduler/core/generic_scheduler.go#L117&#34; target=&#34;_blank&#34;&gt;满足备选 Pod 的需求&lt;/a&gt;。例如，如果 PodSpec 字段限制了 CPU 和内存资源，那么当备选节点的资源容量不满足备选 Pod 的需求时，备选 Pod 就不会被调度到该节点上（&lt;strong&gt;资源容量=备选节点资源总量-节点中已存在 Pod 的所有容器的需求资源（CPU 和内存）的总和&lt;/strong&gt;）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;一旦筛选出符合要求的候选节点，就会采用&lt;code&gt;优选策略&lt;/code&gt;计算出每个候选节点的积分，然后对这些候选节点进行排序，积分最高者胜出。例如，为了在整个系统中分摊工作负载，这些优选策略会从备选节点列表中选出资源消耗最小的节点。每个节点通过优选策略时都会算出一个得分，计算各项得分，最终选出分值大的节点作为优选的结果。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;一旦找到了合适的节点，Scheduler 就会创建一个 &lt;code&gt;Binding&lt;/code&gt; 对象，该对象的 &lt;code&gt;Name&lt;/code&gt; 和 &lt;code&gt;Uid&lt;/code&gt; 与 Pod 相匹配，并且其 &lt;code&gt;ObjectReference&lt;/code&gt; 字段包含所选节点的名称，然后通过 &lt;code&gt;POST&lt;/code&gt; 请求&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/2d64ce5e8e45e26b02492d2b6c85e5ebfb1e4761/plugin/pkg/scheduler/factory/factory.go#L1095&#34; target=&#34;_blank&#34;&gt;发送给 apiserver&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;当 kube-apiserver 接收到此 Binding 对象时，注册吧会将该对象&lt;strong&gt;反序列化&lt;/strong&gt;并更新 Pod 资源中的以下字段：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;将 &lt;code&gt;NodeName&lt;/code&gt; 的值设置为 ObjectReference 中的 NodeName。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;添加相关的注释。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;将 &lt;code&gt;PodScheduled&lt;/code&gt; 的 &lt;code&gt;status&lt;/code&gt; 值设置为 True。可以通过 kubectl 来查看：&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get &amp;lt;PODNAME&amp;gt; -o go-template=&#39;{{range .status.conditions}}{{if eq .type &amp;quot;PodScheduled&amp;quot;}}{{.status}}{{end}}{{end}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一旦 Scheduler 将 Pod 调度到某个节点上，该节点的 &lt;code&gt;Kubelet&lt;/code&gt; 就会接管该 Pod 并开始部署。&lt;/p&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;预选策略和优选策略都可以通过 &lt;code&gt;--policy-config-file&lt;/code&gt; 参数来扩展，如果默认的调度器不满足要求，还可以部署自定义的调度器。如果  &lt;code&gt;podSpec.schedulerName&lt;/code&gt; 的值设置为其他的调度器，则 Kubernetes 会将该 Pod 的调度转交给那个调度器。&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&#34;p-id-h2-6-kubelet-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;6. Kubelet&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;pod-同步&#34;&gt;Pod 同步&lt;/h3&gt;

&lt;p&gt;现在，所有的 Controller 都完成了工作，我们来总结一下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;HTTP 请求通过了认证、授权和准入控制阶段。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;一个 Deployment、ReplicaSet 和三个 Pod 资源被持久化到 etcd 存储中。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;然后运行了一系列初始化器。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;最后每个 Pod 都被调度到合适的节点。&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然而到目前为止，所有的状态变化仅仅只是针对保存在 etcd 中的资源记录，接下来的步骤涉及到运行在工作节点之间的 Pod 的分布状况，这是分布式系统（比如 Kubernetes）的关键因素。这些任务都是由 &lt;code&gt;Kubelet&lt;/code&gt; 组件完成的，让我们开始吧！&lt;/p&gt;

&lt;p&gt;在 Kubernetes 集群中，每个 Node 节点上都会启动一个 Kubelet 服务进程，该进程用于处理 Scheduler 下发到本节点的任务，管理 Pod 的生命周期，包括挂载卷、容器日志记录、垃圾回收以及其他与 Pod 相关的事件。&lt;/p&gt;

&lt;p&gt;如果换一种思维模式，你可以把 Kubelet 当成一种特殊的 Controller，它每隔 20 秒（可以自定义）向 kube-apiserver 通过 &lt;code&gt;NodeName&lt;/code&gt; 获取自身 Node 上所要运行的 Pod 清单。一旦获取到了这个清单，它就会通过与自己的内部缓存进行比较来检测新增加的 Pod，如果有差异，就开始同步 Pod 列表。我们来详细分析一下同步过程：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;如果 Pod 正在创建， Kubelet 就会&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/fc8bfe2d8929e11a898c4557f9323c482b5e8842/pkg/kubelet/kubelet.go#L1519&#34; target=&#34;_blank&#34;&gt;记录一些在 &lt;code&gt;Prometheus&lt;/code&gt; 中用于追踪 Pod 启动延时的指标&lt;/a&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;然后生成一个 &lt;code&gt;PodStatus&lt;/code&gt; 对象，它表示 Pod 当前阶段的状态。Pod 的状态(&lt;code&gt;Phase&lt;/code&gt;) 是 Pod 在其生命周期中的最精简的概要，包括 &lt;code&gt;Pending&lt;/code&gt;，&lt;code&gt;Running&lt;/code&gt;，&lt;code&gt;Succeeded&lt;/code&gt;，&lt;code&gt;Failed&lt;/code&gt; 和 &lt;code&gt;Unkown&lt;/code&gt; 这几个值。状态的产生过程非常过程，所以很有必要深入了解一下背后的原理：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;首先串行执行一系列 Pod 同步处理器（&lt;code&gt;PodSyncHandlers&lt;/code&gt;），每个处理器检查检查 Pod 是否应该运行在该节点上。当所有的处理器都认为该 Pod 不应该运行在该节点上，则 Pod 的 &lt;code&gt;Phase&lt;/code&gt; 值就会变成 &lt;code&gt;PodFailed&lt;/code&gt;，并且将该 Pod 从该节点上驱逐出去。例如当你创建一个 &lt;code&gt;Job&lt;/code&gt; 时，如果 Pod 失败重试的时间超过了 &lt;code&gt;spec.activeDeadlineSeconds&lt;/code&gt; 设置的值，就会将 Pod 从该节点驱逐出去。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;接下来，Pod 的 Phase 值由 &lt;code&gt;init 容器&lt;/code&gt; 和应用容器的状态共同来决定。因为目前容器还没有启动，容器被视为&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/fc8bfe2d8929e11a898c4557f9323c482b5e8842/pkg/kubelet/kubelet_pods.go#L1244&#34; target=&#34;_blank&#34;&gt;处于等待阶段&lt;/a&gt;，如果 Pod 中至少有一个容器处于等待阶段，则其 &lt;code&gt;Phase&lt;/code&gt; 值为 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/fc8bfe2d8929e11a898c4557f9323c482b5e8842/pkg/kubelet/kubelet_pods.go#L1258-L1261&#34; target=&#34;_blank&#34;&gt;Pending&lt;/a&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;最后，Pod 的 &lt;code&gt;Condition&lt;/code&gt; 字段由 Pod 内所有容器的状态决定。现在我们的容器还没有被容器运行时创建，所以 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/fc8bfe2d8929e11a898c4557f9323c482b5e8842/pkg/kubelet/status/generate.go#L70-L81&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;PodReady&lt;/code&gt; 的状态被设置为 &lt;code&gt;False&lt;/code&gt;&lt;/a&gt;。可以通过 kubectl 查看：&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get &amp;lt;PODNAME&amp;gt; -o go-template=&#39;{{range .status.conditions}}{{if eq .type &amp;quot;Ready&amp;quot;}}{{.status}}{{end}}{{end}}&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;生成 PodStatus 之后（Pod 中的 &lt;code&gt;status&lt;/code&gt; 字段），Kubelet 就会将它发送到 Pod 的状态管理器，该管理器的任务是通过 apiserver 异步更新 etcd 中的记录。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;接下来运行一系列&lt;strong&gt;准入处理器&lt;/strong&gt;来确保该 Pod 是否具有相应的权限（包括强制执行 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/fc8bfe2d8929e11a898c4557f9323c482b5e8842/pkg/kubelet/kubelet.go#L883-L884&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;AppArmor&lt;/code&gt; 配置文件和 &lt;code&gt;NO_NEW_PRIVS&lt;/code&gt;&lt;/a&gt;），被准入控制器拒绝的 Pod 将一直保持 &lt;code&gt;Pending&lt;/code&gt; 状态。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果 Kubelet 启动时指定了 &lt;code&gt;cgroups-per-qos&lt;/code&gt; 参数，Kubelet 就会为该 Pod 创建 &lt;code&gt;cgroup&lt;/code&gt; 并进行相应的资源限制。这是为了更方便地对 Pod 进行服务质量（QoS）管理。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;然后为 Pod 创建相应的目录，包括 Pod 的目录（&lt;code&gt;/var/run/kubelet/pods/&amp;lt;podID&amp;gt;&lt;/code&gt;），该 Pod 的卷目录（&lt;code&gt;&amp;lt;podDir&amp;gt;/volumes&lt;/code&gt;）和该 Pod 的插件目录（&lt;code&gt;&amp;lt;podDir&amp;gt;/plugins&lt;/code&gt;）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;卷管理器&lt;/strong&gt;会&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/2723e06a251a4ec3ef241397217e73fa782b0b98/pkg/kubelet/volumemanager/volume_manager.go#L330&#34; target=&#34;_blank&#34;&gt;挂载 &lt;code&gt;Spec.Volumes&lt;/code&gt; 中定义的相关数据卷，然后等待是否挂载成功&lt;/a&gt;。根据挂载卷类型的不同，某些 Pod 可能需要等待更长的时间（比如 NFS 卷）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/dd9981d038012c120525c9e6df98b3beb3ef19e1/pkg/kubelet/kubelet_pods.go#L788&#34; target=&#34;_blank&#34;&gt;从 apiserver 中检索&lt;/a&gt; &lt;code&gt;Spec.ImagePullSecrets&lt;/code&gt; 中定义的所有 &lt;code&gt;Secret&lt;/code&gt;，然后将其注入到容器中。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;最后通过容器运行时接口（&lt;code&gt;Container Runtime Interface（CRI）&lt;/code&gt;）开始启动容器（下面会详细描述）。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;cri-与-pause-容器&#34;&gt;CRI 与 pause 容器&lt;/h3&gt;

&lt;p&gt;到了这个阶段，大量的初始化工作都已经完成，容器已经准备好开始启动了，而容器是由&lt;strong&gt;容器运行时&lt;/strong&gt;（例如 &lt;code&gt;Docker&lt;/code&gt; 和 &lt;code&gt;Rkt&lt;/code&gt;）启动的。&lt;/p&gt;

&lt;p&gt;为了更容易扩展，Kubelet 从 1.5.0 开始通过&lt;strong&gt;容器运行时接口&lt;/strong&gt;与容器运行时（Container Runtime）交互。简而言之，CRI 提供了 Kubelet 和特定的运行时之间的抽象接口，它们之间通过&lt;a href=&#34;https://github.com/google/protobuf&#34; target=&#34;_blank&#34;&gt;协议缓冲区&lt;/a&gt;（它像一个更快的 JSON）和 &lt;a href=&#34;https://grpc.io/&#34; target=&#34;_blank&#34;&gt;gRPC API&lt;/a&gt;（一种非常适合执行 Kubernetes 操作的 API）。这是一个非常酷的想法，通过使用 Kubelet 和运行时之间定义的契约关系，容器如何编排的具体实现细节已经变得无关紧要。由于不需要修改 Kubernetes 的核心代码，开发者可以以最小的开销添加新的运行时。&lt;/p&gt;

&lt;p&gt;不好意思有点跑题了，让我们继续回到容器启动的阶段。第一次启动 Pod 时，Kubelet 会通过 &lt;code&gt;Remote Procedure Command&lt;/code&gt;(RPC) 协议调用 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/2d64ce5e8e45e26b02492d2b6c85e5ebfb1e4761/pkg/kubelet/kuberuntime/kuberuntime_sandbox.go#L51&#34; target=&#34;_blank&#34;&gt;RunPodSandbox&lt;/a&gt;。&lt;code&gt;sandbox&lt;/code&gt; 用于描述一组容器，例如在 Kubernetes 中它表示的是 Pod。&lt;code&gt;sandbox&lt;/code&gt; 是一个很宽泛的概念，所以对于其他没有使用容器的运行时仍然是有意义的（比如在一个基于 &lt;code&gt;hypervisor&lt;/code&gt; 的运行时中，sandbox 可能指的就是虚拟机）。&lt;/p&gt;

&lt;p&gt;我们的例子中使用的容器运行时是 Docker，创建 sandbox 时首先创建的是 &lt;code&gt;pause&lt;/code&gt; 容器。pause 容器作为同一个 Pod 中所有其他容器的基础容器，它为 Pod 中的每个业务容器提供了大量的 Pod 级别资源，这些资源都是 Linux 命名空间（包括网络命名空间，IPC 命名空间和 PID 命名空间）。&lt;/p&gt;

&lt;p&gt;pause 容器提供了一种方法来管理所有这些命名空间并允许业务容器共享它们，在同一个网络命名空间中的好处是：同一个 Pod 中的容器可以使用 &lt;code&gt;localhost&lt;/code&gt; 来相互通信。pause 容器的第二个功能与 PID 命名空间的工作方式相关，在 PID 命名空间中，进程之间形成一个树状结构，一旦某个子进程由于父进程的错误而变成了“孤儿进程”，其便会被 &lt;code&gt;init&lt;/code&gt; 进程进行收养并最终回收资源。关于 pause 工作方式的详细信息可以参考：&lt;a href=&#34;https://www.ianlewis.org/en/almighty-pause-container&#34; target=&#34;_blank&#34;&gt;The Almighty Pause Container&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;一旦创建好了 pause 容器，下面就会开始检查磁盘状态然后开始启动业务容器。&lt;/p&gt;

&lt;h3 id=&#34;cni-和-pod-网络&#34;&gt;CNI 和 Pod 网络&lt;/h3&gt;

&lt;p&gt;现在我们的 Pod 已经有了基本的骨架：一个共享所有命名空间以允许业务容器在同一个 Pod 里进行通信的 pause 容器。但现在还有一个问题，那就是容器的网络是如何建立的？&lt;/p&gt;

&lt;p&gt;当 Kubelet 为 Pod 创建网络时，它会将创建网络的任务交给 &lt;code&gt;CNI&lt;/code&gt; 插件。CNI 表示容器网络接口（Container Network Interface），和容器运行时的运行方式类似，它也是一种抽象，允许不同的网络提供商为容器提供不同的网络实现。通过将 json 配置文件（默认在 &lt;code&gt;/etc/cni/net.d&lt;/code&gt; 路径下）中的数据传送到相关的 CNI 二进制文件（默认在 &lt;code&gt;/opt/cni/bin&lt;/code&gt; 路径下）中，cni 插件可以给 pause 容器配置相关的网络，然后 Pod 中其他的容器都使用 pause 容器的网络。下面是一个简单的示例配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;cniVersion&amp;quot;: &amp;quot;0.3.1&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;bridge&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;bridge&amp;quot;,
    &amp;quot;bridge&amp;quot;: &amp;quot;cnio0&amp;quot;,
    &amp;quot;isGateway&amp;quot;: true,
    &amp;quot;ipMasq&amp;quot;: true,
    &amp;quot;ipam&amp;quot;: {
        &amp;quot;type&amp;quot;: &amp;quot;host-local&amp;quot;,
        &amp;quot;ranges&amp;quot;: [
          [{&amp;quot;subnet&amp;quot;: &amp;quot;${POD_CIDR}&amp;quot;}]
        ],
        &amp;quot;routes&amp;quot;: [{&amp;quot;dst&amp;quot;: &amp;quot;0.0.0.0/0&amp;quot;}]
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CNI 插件还会通过 &lt;code&gt;CNI_ARGS&lt;/code&gt; 环境变量为 Pod 指定其他的元数据，包括 Pod 名称和命名空间。&lt;/p&gt;

&lt;p&gt;下面的步骤因 CNI 插件而异，我们以 &lt;code&gt;bridge&lt;/code&gt; 插件举例：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;该插件首先会在根网络命名空间（也就是宿主机的网络命名空间）中设置本地 Linux 网桥，以便为该主机上的所有容器提供网络服务。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;然后它会将一个网络接口（&lt;code&gt;veth&lt;/code&gt; 设备对的一端）插入到 pause 容器的网络命名空间中，并将另一端连接到网桥上。你可以这样来理解 veth 设备对：它就像一根很长的管道，一端连接到容器，一端连接到根网络命名空间中，数据包就在管道中进行传播。&lt;br /&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;接下来 json 文件中指定的 &lt;code&gt;IPAM&lt;/code&gt; Plugin 会为 pause 容器的网络接口分配一个 IP 并设置相应的路由，现在 Pod 就有了自己的 IP。&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;IPAM Plugin 的工作方式和 CNI Plugin 类似：通过二进制文件调用并具有标准化的接口，每一个 IPAM Plugin 都必须要确定容器网络接口的 IP、子网以及网关和路由，并将信息返回给 CNI 插件。最常见的 IPAM Plugin 是 &lt;code&gt;host-local&lt;/code&gt;，它从预定义的一组地址池中为容器分配 IP 地址。它将地址池的信息以及分配信息保存在主机的文件系统中，从而确保了同一主机上每个容器的 IP 地址的唯一性。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;最后 Kubelet 会将集群内部的 &lt;code&gt;DNS&lt;/code&gt; 服务器的 &lt;code&gt;Cluster IP&lt;/code&gt; 地址传给 CNI 插件，然后 CNI 插件将它们写到容器的 &lt;code&gt;/etc/resolv.conf&lt;/code&gt; 文件中。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一旦完成了上面的步骤，CNI 插件就会将操作的结果以 json 的格式返回给 Kubelet。&lt;/p&gt;

&lt;h3 id=&#34;跨主机容器网络&#34;&gt;跨主机容器网络&lt;/h3&gt;

&lt;p&gt;到目前为止，我们已经描述了容器如何与宿主机进行通信，但跨主机之间的容器如何通信呢？&lt;/p&gt;

&lt;p&gt;通常情况下使用 &lt;code&gt;overlay&lt;/code&gt; 网络来进行跨主机容器通信，这是一种动态同步多个主机间路由的方法。 其中最常用的 overlay 网络插件是 &lt;code&gt;flannel&lt;/code&gt;，flannel 具体的工作方式可以参考 &lt;a href=&#34;https://github.com/coreos/flannel&#34; target=&#34;_blank&#34;&gt;CoreOS 的文档&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;容器启动&#34;&gt;容器启动&lt;/h3&gt;

&lt;p&gt;所有网络都配置完成后，接下来就开始真正启动业务容器了！&lt;/p&gt;

&lt;p&gt;一旦 sanbox 完成初始化并处于 &lt;code&gt;active&lt;/code&gt; 状态，Kubelet 就可以开始为其创建容器了。首先&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/5adfb24f8f25a0d57eb9a7b158db46f9f46f0d80/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L690&#34; target=&#34;_blank&#34;&gt;启动 PodSpec 中定义的 init 容器&lt;/a&gt;，然后再启动业务容器。具体过程如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;首先拉取容器的镜像。如果是私有仓库的镜像，就会利用 PodSpec 中指定的 Secret 来拉取该镜像。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;然后通过 CRI 接口创建容器。Kubelet 向 PodSpec 中填充了一个 &lt;code&gt;ContainerConfig&lt;/code&gt; 数据结构（在其中定义了命令，镜像，标签，挂载卷，设备，环境变量等待），然后通过 &lt;code&gt;protobufs&lt;/code&gt; 发送给 CRI 接口。对于 Docker 来说，它会将这些信息反序列化并填充到自己的配置信息中，然后再发送给 &lt;code&gt;Dockerd&lt;/code&gt; 守护进程。在这个过程中，它会将一些元数据标签（例如容器类型，日志路径，dandbox ID 等待）添加到容器中。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;接下来会使用 CPU 管理器来约束容器，这是 Kubelet 1.8 中新添加的 alpha 特性，它使用 &lt;code&gt;UpdateContainerResources&lt;/code&gt; CRI 方法将容器分配给本节点上的 CPU 资源池。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;最后容器开始真正&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/5f9f4a1c5939436fa320e9bc5973a55d6446e59f/pkg/kubelet/kuberuntime/kuberuntime_container.go#L135&#34; target=&#34;_blank&#34;&gt;启动&lt;/a&gt;。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;如果 Pod 中配置了容器生命周期钩子（Hook），容器启动之后就会运行这些 &lt;code&gt;Hook&lt;/code&gt;。Hook 的类型包括两种：&lt;code&gt;Exec&lt;/code&gt;（执行一段命令） 和 &lt;code&gt;HTTP&lt;/code&gt;（发送HTTP请求）。如果 PostStart Hook 启动的时间过长、挂起或者失败，容器将永远不会变成 &lt;code&gt;running&lt;/code&gt; 状态。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;p-id-h2-7-总结-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;7. 总结&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;如果上面一切顺利，现在你的集群上应该会运行三个容器，所有的网络，数据卷和秘钥都被通过 CRI 接口添加到容器中并配置成功。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-8-原文链接-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;8. 原文链接&lt;/p&gt;&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jamiehannaford/what-happens-when-k8s&#34; target=&#34;_blank&#34;&gt;What happens when &amp;hellip; Kubernetes edition!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>英语学习终极秘诀</title>
      <link>https://www.yangcs.net/learn-english/</link>
      <pubDate>Fri, 25 May 2018 06:46:19 +0000</pubDate>
      
      <guid>https://www.yangcs.net/learn-english/</guid>
      <description>

&lt;p&gt;很久很久以前，当网易还在做梦幻西游和大话西游的时候，网易有道教研部诞生了一位大牛，他是&lt;strong&gt;国内最知名，最有影响力的英语学习研究者，新东方口语大赛评委、网易有道教研总监&lt;span id=&#34;inline-purple&#34;&gt;@恶魔奶爸Sam&lt;/span&gt;&lt;/strong&gt;。不过他现在已经不在网易了，可能是不想陪丁磊一起养猪 😄&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/naiba.png?imageView/2/w/300/q/100&#34; alt=&#34;&#34; /&gt;&lt;/center&gt;
&lt;center&gt;- 恶魔奶爸Sam -&lt;/center&gt;
&lt;center&gt;前网易有道教研总监&lt;/center&gt;
&lt;center&gt;新东方全国口语大赛评委&lt;/center&gt;
&lt;center&gt;知乎英语学习领域百万赞答主&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;辜鸿铭曾说过，“今人学英文十年，张目仅能读报，伸手仅能修函。”，那是一百多年前的事情，如今呢？“今人学英文十年，张目不能读报，伸手不能修函。”&lt;/p&gt;

&lt;p&gt;实用能力，可谓一塌糊涂。&lt;/p&gt;

&lt;p&gt;在此之前，我还在英语学习的艰苦道路上徘徊不前，我自己个人在大学时期苦修英文，同样不得其法，非常苦闷，不论怎么学习，英语水平就是局限在某个瓶颈，就是突破不了。&lt;/p&gt;

&lt;p&gt;因为现在的英语材料和英语方法是在太多了，美剧，新闻，新概念，听写，跟读，模仿，仿写，复述，这个名师讲课，那个名师课堂，等等等等，让人眼花缭乱，目不衔接，根本不知道如何选择，所以更加感觉无从下手。&lt;/p&gt;

&lt;p&gt;我以为我的英语学习生涯到此应该就止步了，可就在我将要丧失斗志之际，我读到了奶爸的文章，他那高屋建瓴，纵览全局的英语学习方法论让我感到醍醐灌顶，跳出之前二十年英语学习的误区，重新燃起了斗志！&lt;/p&gt;

&lt;p&gt;至于具体的方法论，由于篇幅有限，再此不多作介绍，本文末尾会放出详细的链接。&lt;/p&gt;

&lt;p&gt;大家可以通过以下几个途径关注他：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://site.douban.com/195274/&#34; target=&#34;_blank&#34;&gt;豆瓣学习小组：奶爸的英语教室&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;微信公众号：恶魔奶爸Sam&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;奶爸的方法论大多借鉴了 &lt;a href=&#34;http://blog.sina.com.cn/u/1264366955&#34; target=&#34;_blank&#34;&gt;伍君仪&lt;/a&gt; 的方法论，并加以改进。而伍君仪的主要方法论都集中在他和奶爸合作写的一本教材《把你的英语用起来》中。本文节选出该教材的前言部分，大家感受一下：&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-奶爸-这是一本游戏指南-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;奶爸：这是一本游戏指南&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;这是一本游戏指南。没错，你没有看错，这就是一本游戏指南。当然，这本指南针对的只是名为“英文”的游戏。&lt;/p&gt;

&lt;p&gt;把英文和电子游戏比较一下，我们会发现，这两者有惊人的相似之处。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第一&lt;/strong&gt;，它们都需要你耗费大量的时间、投入很多精力去练级，磨炼你的技能，唯一不同的是电脑游戏要你练的是魔法和剑术技能，而英文要你练的是听、说、读、写的技能。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第二&lt;/strong&gt;，它们都要求你积累大量的经验值，游戏要求你打怪和做各种任务，而英语要求你完成大量的听力、跟读、阅读、写作等练习。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第三&lt;/strong&gt;，这两者都有严格的等级分别，不同的等级从某种程度上说，代表着你可以在这个世界上“为非作歹”、“为所欲为”的程度。游戏中的等级高，你可以在 PK 时处处取胜，带着美眉做任务，博得她们的芳心。英文等级高了，你可以出国，找份不错的工作，去英语角博得美眉的芳心或者干脆直接去泡外国妞，这两者在这方面真是有异曲同工之妙。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第四&lt;/strong&gt;，不管是打游戏还是学英语，你都必须闯过多重关卡，而每个关卡都有你不得不面对的 boss。不管这个 boss 是名为暗黑破坏神巫妖王，还是四六级托福雅思 GRE，每个关卡你是非过不可，要么你打死 boss，要么被 boss 打死。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第五&lt;/strong&gt;，在游戏级数升到一定级别之后，你都会遇到很严重的瓶颈，级数再往上升会很困难，因为它需要更多的经验值、更多的时间投入。英文同样如此，很多人学到一定层次之后都很容易陷入平台期，不知道如何进一步提高，耗费很多时间却没多大进展，甚至干脆放弃。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第六&lt;/strong&gt;，在打游戏和学英语的过程中，你都有可能发生意外。游戏中，你可能会膝盖中了一箭，而如果英语学得不好，你可能学位证都拿不到，升职机会就抓不住。为了避免这些意外发生，我们都应该好好想一想，用什么样的攻略和方法去达到自己的目标。&lt;/p&gt;

&lt;p&gt;这两者的相似程度如此之高，以至于我们有理由相信：学英语，也可以像玩游戏一样充满乐趣，越玩越想玩！&lt;/p&gt;

&lt;p&gt;因此，本书的目的有四：&lt;/p&gt;

&lt;p&gt;&lt;font color=#0099ff&gt;第一，给出一个完整的英文学习攻略，涵盖了从ABC初级入门，到轻松看懂无字幕影视、原版专业教材的各个等级和阶段。&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;不管你是一个新人还是一个小有名气的高手，我们都会提供循序渐进的、详细且完整的步骤供你参考，而不像任何一本其他攻略那样只告诉你某一个任务关的攻略（比如只偏重听说而没有涵盖读写之类的学习方法指南）。我们涵盖的是全部流程，每一步都稳扎稳打，让你从一名 newbie（菜鸟）变成一个真正的骨灰级高手。&lt;/p&gt;

&lt;p&gt;&lt;font color=#0099ff&gt;第二，教你如何迅速突破英文学习的平台期和瓶颈——这也是本书的核心内容。&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;如果你英文水平还算不错的话，肯定会明白平台期的痛苦——苦学三个月毫无进步，那这本攻略就正是你所需要的。它可以帮你扫清一切学习上的迷思和痛苦，迅速把你带到真正高水平的世界。事实上，这本攻略是如此有效，以至于你在用它来指导自己学习的时候，进步的速度会快得让你自己都感到吃惊。&lt;/p&gt;

&lt;p&gt;&lt;font color=#0099ff&gt;第三，让大家能够坚持下去学好英文——这也是本书最大的特色。&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;讲英文学习的方法有很多，我们在这里不仅仅是告诉大家如何学习，更重要的是，我们还会告诉大家如何去“坚持学习”。换言之，我们会手把手地教会大家正确的陶冶心智的方法，杜绝不当或者错误的自我认知，一步步地通过自我管理和心智培养，来实现人生满足。&lt;/p&gt;

&lt;p&gt;&lt;font color=#0099ff&gt;第四，让大家学英文像打电子游戏一样，充满乐趣，不再为英文头痛，快速通关！&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;我们在书中设置了中级、中高级、听说进阶级、高级四大关卡，让大家好像打游戏一样来闯关得分，提高自己的英文水准！这四大关卡分别涵盖了英文的听力、口语交际、原版书阅读、语法、词汇、写作等各个部分，大家在学习自己喜欢和感兴趣的材料时，不知不觉中这些能力都能够得到高效的提高&lt;/p&gt;

&lt;p&gt;正是抱着这样的目的，伍君仪老师和不才二人汇总了各自的一些粗浅经验，写出了这一本英文游戏攻略。我们虽不算是功力震古烁今的绝顶高手，但作为浸淫英文游戏多年的过来人，也算是略有心得。在这本书中，我们竭尽所能，将这个名为“英文”的游戏中的各种法门、陷阱、弯路一一道来，让诸君的英文能力升级像开外挂那么迅速，一路闯过各种关卡，灭掉名为“英文听、说、读、写”的四大魔王也好，打败各类名为“四六级”的boss也好，击退各种名为“英文会议”或是“英文面试”的小卒也好，请拿出你的汗水、勇气、智慧去努力奋斗，直至你有能力去挑战名为“流利英语，母语水平”的超级大魔王，让“美女剑豪带着酒来了”的热血故事广为流传！&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-福利-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;福利&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;本书完整版可以到当当或亚马逊购买，我这里也提供了免费的电子版：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://default-1252251317.cossh.myqcloud.com/%E6%8A%8A%E4%BD%A0%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%94%A8%E8%B5%B7%E6%9D%A5.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://default-1252251317.cossh.myqcloud.com/%E6%8A%8A%E4%BD%A0%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%94%A8%E8%B5%B7%E6%9D%A5.epub&#34; target=&#34;_blank&#34;&gt;epub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;关于本书中推荐的 Podcast: 《A Day in the life of Jeff》，我已经将其录音材料免费分享到互联网上，同时也将录音内容整理成电子书，大家可以免费下载：&lt;/p&gt;

&lt;h4 id=&#34;录音材料&#34;&gt;录音材料&lt;/h4&gt;

&lt;p&gt;&lt;a id=&#34;download&#34; href=&#34;https://pan.baidu.com/s/1K0v274ULsghYmjjyWe93Bw&#34;&gt;&lt;i class=&#34;fa fa-download&#34;&gt;&lt;/i&gt;&lt;span&gt; Download Now&lt;/span&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;电子书&#34;&gt;电子书&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.yangcs.net/a-day-in-the-life-of-jeff/&#34; target=&#34;_blank&#34;&gt;在线阅读&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://default-1252251317.cossh.myqcloud.com/a-day-in-the-life-of-jeff.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://default-1252251317.cossh.myqcloud.com/a-day-in-the-life-of-jeff.epub&#34; target=&#34;_blank&#34;&gt;epub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://default-1252251317.cossh.myqcloud.com/a-day-in-the-life-of-jeff.mobi&#34; target=&#34;_blank&#34;&gt;mobi&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果后续还有整理好的福利，将会在这里补充 😊&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;
</description>
    </item>
    
    <item>
      <title>走进 Descheduler</title>
      <link>https://www.yangcs.net/posts/introduce-kubernetes-descheduler/</link>
      <pubDate>Wed, 23 May 2018 10:23:29 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/introduce-kubernetes-descheduler/</guid>
      <description>&lt;p&gt;
&lt;code&gt;kube-scheduler&lt;/code&gt; 是 Kubernetes 中负责调度的组件，它本身的调度功能已经很强大了。但由于 Kubernetes 集群非常活跃，它的状态会随时间而改变，由于各种原因，你可能需要将已经运行的 Pod 移动到其他节点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;某些节点负载过高&lt;/li&gt;
&lt;li&gt;某些资源对象被添加了 &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature&#34; target=&#34;_blank&#34;&gt;node 亲和性&lt;/a&gt; 或 &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature&#34; target=&#34;_blank&#34;&gt;pod （反）亲和性&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;集群中加入了新节点&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一旦 Pod 启动之后 &lt;code&gt;kube-scheduler&lt;/code&gt; 便不会再尝试重新调度它。根据环境的不同，你可能会有很多需要手动调整 Pod 的分布，例如：如果集群中新加入了一个节点，那么已经运行的 Pod 并不会被分摊到这台节点上，这台节点可能只运行了少量的几个 Pod，这并不理想，对吧？&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-descheduler-如何工作-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. Descheduler 如何工作？&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/descheduler&#34; target=&#34;_blank&#34;&gt;Descheduler&lt;/a&gt; 会检查 Pod 的状态，并根据自定义的策略将不满足要求的 Pod 从该节点上驱逐出去。Descheduler 并不是 &lt;code&gt;kube-scheduler&lt;/code&gt; 的替代品，而是要依赖于它。该项目目前放在 Kubernetes 的孵化项目中，还没准备投入生产，但经过我实验发现它的运行效果很好，而且非常稳定。那么该如何安装呢？&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-2-部署方法-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. 部署方法&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;你可以通过 &lt;code&gt;Job&lt;/code&gt; 或 &lt;code&gt;CronJob&lt;/code&gt; 来运行 descheduler。我已经创建了一个镜像 &lt;code&gt;komljen/descheduler:v0.5.0-4-ga7ceb671&lt;/code&gt;（包含在下面的 yaml 文件中），但由于这个项目的更新速度很快，你可以通过以下的命令创建你自己的镜像：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/kubernetes-incubator/descheduler
$ cd descheduler &amp;amp;&amp;amp; make image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后打好标签 push 到自己的镜像仓库中。&lt;/p&gt;

&lt;p&gt;通过我创建的 chart 模板，你可以用 &lt;code&gt;Helm&lt;/code&gt; 来部署 descheduler，该模板支持 RBAC 并且已经在 Kubernetes v1.9 上测试通过。&lt;/p&gt;

&lt;p&gt;添加我的 helm 私有仓库，然后部署 descheduler：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm repo add akomljen-charts \
  https://raw.githubusercontent.com/komljen/helm-charts/master/charts/
  
$ helm install --name ds \
  --namespace kube-system \
  akomljen-charts/descheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;你也可以不使用 helm，通过手动部署。首先创建 serviceaccount 和 clusterrolebinding：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create a cluster role
$ cat &amp;lt;&amp;lt; EOF| kubectl create -n kube-system -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: descheduler
rules:
- apiGroups: [&amp;quot;&amp;quot;]
  resources: [&amp;quot;nodes&amp;quot;]
  verbs: [&amp;quot;get&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;list&amp;quot;]
- apiGroups: [&amp;quot;&amp;quot;]
  resources: [&amp;quot;pods&amp;quot;]
  verbs: [&amp;quot;get&amp;quot;, &amp;quot;watch&amp;quot;, &amp;quot;list&amp;quot;, &amp;quot;delete&amp;quot;]
- apiGroups: [&amp;quot;&amp;quot;]
  resources: [&amp;quot;pods/eviction&amp;quot;]
  verbs: [&amp;quot;create&amp;quot;]
EOF

# Create a service account
$ kubectl create sa descheduler -n kube-system

# Bind the cluster role to the service account
$ kubectl create clusterrolebinding descheduler \
    -n kube-system \
    --clusterrole=descheduler \
    --serviceaccount=kube-system:descheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后通过 &lt;code&gt;configmap&lt;/code&gt; 创建 descheduler 策略。目前只支持四种策略：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/descheduler#removeduplicates&#34; target=&#34;_blank&#34;&gt;RemoveDuplicates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/descheduler#lownodeutilization&#34; target=&#34;_blank&#34;&gt;LowNodeUtilization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/descheduler#removepodsviolatinginterpodantiaffinity&#34; target=&#34;_blank&#34;&gt;RemovePodsViolatingInterPodAntiAffinity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/descheduler#removepodsviolatingnodeaffinity&#34; target=&#34;_blank&#34;&gt;RemovePodsViolatingNodeAffinity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;默认这四种策略全部开启，你可以根据需要关闭它们。下面在 &lt;code&gt;kube-suystem&lt;/code&gt; 命名空间中创建一个 configmap：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;$ cat &amp;lt;&amp;lt; EOF| kubectl create -n kube-system -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: descheduler
data:
  policy.yaml: |-  
    apiVersion: descheduler/v1alpha1
    kind: DeschedulerPolicy
    strategies:
      RemoveDuplicates:
         enabled: false
      LowNodeUtilization:
         enabled: true
         params:
           nodeResourceUtilizationThresholds:
             thresholds:
               cpu: 20
               memory: 20
               pods: 20
             targetThresholds:
               cpu: 50
               memory: 50
               pods: 50
      RemovePodsViolatingInterPodAntiAffinity:
        enabled: true
      RemovePodsViolatingNodeAffinity:
        enabled: true
        params:
          nodeAffinityType:
          - requiredDuringSchedulingIgnoredDuringExecution
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;kube-system&lt;/code&gt; 命名空间中创建一个 CronJob：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;$ cat &amp;lt;&amp;lt; EOF| kubectl create -n kube-system -f -
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: descheduler
spec:
  schedule: &amp;quot;*/30 * * * *&amp;quot;
  jobTemplate:
    metadata:
      name: descheduler
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: &amp;quot;true&amp;quot;
    spec:
      template:
        spec:
          serviceAccountName: descheduler
          containers:
          - name: descheduler
            image: komljen/descheduler:v0.5.0-4-ga7ceb671
            volumeMounts:
            - mountPath: /policy-dir
              name: policy-volume
            command:
            - /bin/descheduler
            - --v=4
            - --max-pods-to-evict-per-node=10
            - --policy-config-file=/policy-dir/policy.yaml
          restartPolicy: &amp;quot;OnFailure&amp;quot;
          volumes:
          - name: policy-volume
            configMap:
              name: descheduler
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get cronjobs -n kube-system

NAME             SCHEDULE       SUSPEND   ACTIVE    LAST SCHEDULE   AGE
descheduler      */30 * * * *   False     0         2m              32m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该 CroJob 每 30 分钟运行一次，当 CronJob 开始工作后，可以通过以下命令查看已经成功结束的 Pod：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get pods -n kube-system -a | grep Completed

descheduler-1525520700-297pq          0/1       Completed   0          1h
descheduler-1525521000-tz2ch          0/1       Completed   0          32m
descheduler-1525521300-mrw4t          0/1       Completed   0          2m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以查看这些 Pod 的日志，然后根据需要调整 descheduler 策略：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl logs descheduler-1525521300-mrw4t -n kube-system

I0505 11:55:07.554195       1 reflector.go:202] Starting reflector *v1.Node (1h0m0s) from github.com/kubernetes-incubator/descheduler/pkg/descheduler/node/node.go:84
I0505 11:55:07.554255       1 reflector.go:240] Listing and watching *v1.Node from github.com/kubernetes-incubator/descheduler/pkg/descheduler/node/node.go:84
I0505 11:55:07.767903       1 lownodeutilization.go:147] Node &amp;quot;ip-10-4-63-172.eu-west-1.compute.internal&amp;quot; is appropriately utilized with usage: api.ResourceThresholds{&amp;quot;cpu&amp;quot;:41.5, &amp;quot;memory&amp;quot;:1.3635487207675927, &amp;quot;pods&amp;quot;:8.181818181818182}
I0505 11:55:07.767942       1 lownodeutilization.go:149] allPods:9, nonRemovablePods:9, bePods:0, bPods:0, gPods:0
I0505 11:55:07.768141       1 lownodeutilization.go:144] Node &amp;quot;ip-10-4-36-223.eu-west-1.compute.internal&amp;quot; is over utilized with usage: api.ResourceThresholds{&amp;quot;cpu&amp;quot;:48.75, &amp;quot;memory&amp;quot;:61.05259502942694, &amp;quot;pods&amp;quot;:30}
I0505 11:55:07.768156       1 lownodeutilization.go:149] allPods:33, nonRemovablePods:12, bePods:1, bPods:19, gPods:1
I0505 11:55:07.768376       1 lownodeutilization.go:144] Node &amp;quot;ip-10-4-41-14.eu-west-1.compute.internal&amp;quot; is over utilized with usage: api.ResourceThresholds{&amp;quot;cpu&amp;quot;:39.125, &amp;quot;memory&amp;quot;:98.19259268881142, &amp;quot;pods&amp;quot;:33.63636363636363}
I0505 11:55:07.768390       1 lownodeutilization.go:149] allPods:37, nonRemovablePods:8, bePods:0, bPods:29, gPods:0
I0505 11:55:07.768538       1 lownodeutilization.go:147] Node &amp;quot;ip-10-4-34-29.eu-west-1.compute.internal&amp;quot; is appropriately utilized with usage: api.ResourceThresholds{&amp;quot;memory&amp;quot;:43.19826999287199, &amp;quot;pods&amp;quot;:30.90909090909091, &amp;quot;cpu&amp;quot;:35.25}
I0505 11:55:07.768552       1 lownodeutilization.go:149] allPods:34, nonRemovablePods:11, bePods:8, bPods:15, gPods:0
I0505 11:55:07.768556       1 lownodeutilization.go:65] Criteria for a node under utilization: CPU: 20, Mem: 20, Pods: 20
I0505 11:55:07.768571       1 lownodeutilization.go:69] No node is underutilized, nothing to do here, you might tune your thersholds further
I0505 11:55:07.768576       1 pod_antiaffinity.go:45] Processing node: &amp;quot;ip-10-4-63-172.eu-west-1.compute.internal&amp;quot;
I0505 11:55:07.779313       1 pod_antiaffinity.go:45] Processing node: &amp;quot;ip-10-4-36-223.eu-west-1.compute.internal&amp;quot;
I0505 11:55:07.796766       1 pod_antiaffinity.go:45] Processing node: &amp;quot;ip-10-4-41-14.eu-west-1.compute.internal&amp;quot;
I0505 11:55:07.813303       1 pod_antiaffinity.go:45] Processing node: &amp;quot;ip-10-4-34-29.eu-west-1.compute.internal&amp;quot;
I0505 11:55:07.829109       1 node_affinity.go:40] Executing for nodeAffinityType: requiredDuringSchedulingIgnoredDuringExecution
I0505 11:55:07.829133       1 node_affinity.go:45] Processing node: &amp;quot;ip-10-4-63-172.eu-west-1.compute.internal&amp;quot;
I0505 11:55:07.840416       1 node_affinity.go:45] Processing node: &amp;quot;ip-10-4-36-223.eu-west-1.compute.internal&amp;quot;
I0505 11:55:07.856735       1 node_affinity.go:45] Processing node: &amp;quot;ip-10-4-41-14.eu-west-1.compute.internal&amp;quot;
I0505 11:55:07.945566       1 request.go:480] Throttling request took 88.738917ms, request: GET:https://100.64.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-4-41-14.eu-west-1.compute.internal%2Cstatus.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded
I0505 11:55:07.972702       1 node_affinity.go:45] Processing node: &amp;quot;ip-10-4-34-29.eu-west-1.compute.internal&amp;quot;
I0505 11:55:08.145559       1 request.go:480] Throttling request took 172.751657ms, request: GET:https://100.64.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dip-10-4-34-29.eu-west-1.compute.internal%2Cstatus.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded
I0505 11:55:08.160964       1 node_affinity.go:72] Evicted 0 pods
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;哇哦，现在你的集群中已经运行了一个 descheduler！&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-总结-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 总结&lt;/p&gt;&lt;/h2&gt;

&lt;p&gt;Kubernetes 的默认调度器已经做的很好，但由于集群处于不断变化的状态中，某些 Pod 可能运行在错误的节点上，或者你想要均衡集群资源的分配，这时候就需要 descheduler 来帮助你将某些节点上的 Pod 驱逐到正确的节点上去。我很期待正式版的发布！&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-4-原文链接-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. 原文链接&lt;/p&gt;&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://akomljen.com/meet-a-kubernetes-descheduler/&#34; target=&#34;_blank&#34;&gt;Meet a Kubernetes Descheduler&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>Pod 的生命周期管理</title>
      <link>https://www.yangcs.net/posts/pods-life/</link>
      <pubDate>Thu, 03 May 2018 12:08:01 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/pods-life/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;本文我们将从实践者的角度仔细研究整个pod生命周期，包括如何影响启动和关闭行为，并通过实践来理解对应用程序健康状况的检查。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-pod-的生命周期-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. Pod 的生命周期&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;pod-phase&#34;&gt;Pod phase&lt;/h3&gt;

&lt;p&gt;Pod 的 status 在信息保存在 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L2471&#34; target=&#34;_blank&#34;&gt;PodStatus&lt;/a&gt; 中定义，其中有一个 phase 字段。&lt;/p&gt;

&lt;p&gt;Pod 的相位（phase）是 Pod 在其生命周期中的简单宏观概述。该阶段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机。&lt;/p&gt;

&lt;p&gt;Pod 相位的数量和含义是严格指定的。除了本文档中列举的状态外，不应该再假定 Pod 有其他的 phase 值。&lt;/p&gt;

&lt;p&gt;无论你是手动创建 Pod，还是通过 &lt;code&gt;deployment&lt;/code&gt;、&lt;code&gt;daemonset&lt;/code&gt; 或 &lt;code&gt;statefulset&lt;/code&gt;来创建，Pod 的 phase 都有以下几个可能的值：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;挂起（Pending）&lt;/strong&gt;：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;运行中（Running）&lt;/strong&gt;：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;成功（Successed）&lt;/strong&gt;：Pod 中的所有容器都被成功终止，并且不会再重启。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;失败（Failed）&lt;/strong&gt;：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;未知（Unkonwn）&lt;/strong&gt;：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下图是 Pod 的生命周期示意图，从图中可以看到 Pod 状态的变化。&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;https://jimmysong.io/kubernetes-handbook/images/kubernetes-pod-life-cycle.jpg&#34; alt=&#34;&#34; /&gt;&lt;/center&gt;
&lt;center&gt;图片 - Pod的生命周期示意图&lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&#34;pod-状态&#34;&gt;Pod 状态&lt;/h3&gt;

&lt;p&gt;Pod 有一个 PodStatus 对象，其中包含一个 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L1964&#34; target=&#34;_blank&#34;&gt;PodCondition&lt;/a&gt; 数组。 PodCondition 数组的每个元素都有一个 type 字段和一个 status 字段。type 字段是字符串，可能的值有 &lt;code&gt;PodScheduled&lt;/code&gt;、&lt;code&gt;Ready&lt;/code&gt;、&lt;code&gt;Initialized&lt;/code&gt; 和 &lt;code&gt;Unschedulable&lt;/code&gt;。status 字段是一个字符串，可能的值有 &lt;code&gt;True&lt;/code&gt;、&lt;code&gt;False&lt;/code&gt; 和 &lt;code&gt;Unknown&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;当你通过 &lt;code&gt;kubectl get pod&lt;/code&gt; 查看 Pod 时，&lt;code&gt;STATUS&lt;/code&gt; 这一列可能会显示与上述5个状态不同的值，例如 &lt;code&gt;Init:0/1&lt;/code&gt; 和 &lt;code&gt;CrashLoopBackOff&lt;/code&gt;。这是因为 Pod 状态的定义除了包含 phase 之外，还有 &lt;code&gt;InitContainerStatuses&lt;/code&gt; 和 &lt;code&gt;containerStatuses&lt;/code&gt; 等其他字段，具体代码参考 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/3ae0b84e0b114692dc666d9486fb032d8a33bb58/pkg/api/types.go#L2471&#34; target=&#34;_blank&#34;&gt;overall status of a pod&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;如果想知道究竟发生了什么，可以通过命令 &lt;code&gt;kubectl describe pod/$PODNAME&lt;/code&gt; 查看输出信息的 &lt;code&gt;Events&lt;/code&gt; 条目。通过 Events 条目可以看到一些具体的信息，比如正在拉取容器镜像，Pod 已经被调度，或者某个 container 处于 unhealthy 状态。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-2-pod-的启动关闭流程-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. Pod 的启动关闭流程&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;下面通过一个具体的示例来探究一下 Pod 的整个生命周期流程。为了确定事情发生的顺序，通过下面的 manifest 来部署一个 deployment。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kind:                   Deployment
apiVersion:             apps/v1beta1
metadata:
  name:                 loap
spec:
  replicas:             1
  template:
    metadata:
      labels:
        app:            loap
    spec:
      initContainers:
      - name:           init
        image:          busybox
        command:       [&#39;sh&#39;, &#39;-c&#39;, &#39;echo $(date +%s): INIT &amp;gt;&amp;gt; /loap/timing&#39;]
        volumeMounts:
        - mountPath:    /loap
          name:         timing
      containers:
      - name:           main
        image:          busybox
        command:       [&#39;sh&#39;, &#39;-c&#39;, &#39;echo $(date +%s): START &amp;gt;&amp;gt; /loap/timing;
sleep 10; echo $(date +%s): END &amp;gt;&amp;gt; /loap/timing;&#39;]
        volumeMounts:
        - mountPath:    /loap
          name:         timing
        livenessProbe:
          exec:
            command:   [&#39;sh&#39;, &#39;-c&#39;, &#39;echo $(date +%s): LIVENESS &amp;gt;&amp;gt; /loap/timing&#39;]
        readinessProbe:
          exec:
            command:   [&#39;sh&#39;, &#39;-c&#39;, &#39;echo $(date +%s): READINESS &amp;gt;&amp;gt; /loap/timing&#39;]
        lifecycle:
          postStart:
            exec:
              command:   [&#39;sh&#39;, &#39;-c&#39;, &#39;echo $(date +%s): POST-START &amp;gt;&amp;gt; /loap/timing&#39;]
          preStop:
            exec:
              command:  [&#39;sh&#39;, &#39;-c&#39;, &#39;echo $(date +%s): PRE-HOOK &amp;gt;&amp;gt; /loap/timing&#39;]
      volumes:
      - name:           timing
        hostPath:
          path:         /tmp/loap
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;等待 Pod 状态变为 &lt;code&gt;Running&lt;/code&gt; 之后，通过以下命令来强制停止 Pod：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl scale deployment loap --replicas=0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 &lt;code&gt;/tmp/loap/timing&lt;/code&gt; 文件的内容：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ cat /tmp/loap/timing

1525334577: INIT
1525334581: START
1525334581: POST-START
1525334584: READINESS
1525334584: LIVENESS
1525334588: PRE-HOOK
1525334589: END
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;/tmp/loap/timing&lt;/code&gt; 文件的内容很好地体现了 Pod 的启动和关闭流程，具体过程如下：&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/loap.png&#34; alt=&#34;&#34; /&gt;&lt;/center&gt;
&lt;center&gt;图片 - Pod 的启动和关闭流程&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;首先启动一个 Infra 容器（又叫 Pause 容器），用来和 Pod 中的其他容器共享 linux 命名空间，并开启 init 进程。（上图中忽略了这一步）&lt;/li&gt;
&lt;li&gt;然后启动 Init 容器，它是一种专用的容器，在应用程序容器启动之前运行，用来对 Pod 进行一些初始化操作，并包括一些应用镜像中不存在的实用工具和安装脚本。&lt;/li&gt;
&lt;li&gt;4 秒之后，应用程序容器和 &lt;code&gt;post-start hook&lt;/code&gt; 同时启动。&lt;/li&gt;
&lt;li&gt;7 秒之后开始启动 &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/&#34; target=&#34;_blank&#34;&gt;liveness 和 readiness 探针&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;11 秒之后，通过手动杀掉 Pod，&lt;code&gt;pre-stop hook&lt;/code&gt; 执行，优雅删除期限过期后（默认是 30 秒），应用程序容器停止。实际的 Pod 终止过程要更复杂，具体参考 &lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/concepts/pod.html&#34; target=&#34;_blank&#34;&gt;Pod 的终止&lt;/a&gt;。&lt;/li&gt;
&lt;/ol&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;必须主动杀掉 Pod 才会触发 &lt;code&gt;pre-stop hook&lt;/code&gt;，如果是 Pod 自己 Down 掉，则不会执行 &lt;code&gt;pre-stop hook&lt;/code&gt;。&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-如何快速-debug-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 如何快速 DEBUG&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;当 Pod 出现致命的错误时，如果能够快速 DEBUG，将会帮助我们快速定位问题。为了实现这个目的，可以把把致命事件的信息通过 &lt;code&gt;.spec.terminationMessagePath&lt;/code&gt; 配置写入指定位置的文件，就像打印错误、异常和堆栈信息一样。该位置的内容可以很方便的通过 dashboards、监控软件等工具检索和展示，默认路径为 &lt;code&gt;/dev/termination-log&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;以下是一个小例子：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# termination-demo.yaml
apiVersion: v1
kind: Pod
metadata:
  name: termination-demo
spec:
  containers:
  - name: termination-demo-container
    image: alpine
    command: [&amp;quot;/bin/sh&amp;quot;]
    args: [&amp;quot;-c&amp;quot;, &amp;quot;sleep 10 &amp;amp;&amp;amp; echo Sleep expired &amp;gt; /dev/termination-log&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些消息的最后部分会使用其他的规定来单独存储：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl create -f termination-demo.yaml

$ sleep 20

$ kubectl get pod termination-demo -o go-template=&#39;{{range .status.containerStatuses}}{{.lastState.terminated.message}}{{end}}&#39;

Sleep expired

$ kubectl get pod termination-demo -o go-template=&#39;{{range .status.containerStatuses}}{{.lastState.terminated.exitCode}}{{end}}&#39;

0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-4-参考-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. 参考&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/concepts/pod-hook.html&#34; target=&#34;_blank&#34;&gt;Pod hook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.openshift.com/kubernetes-pods-life/&#34; target=&#34;_blank&#34;&gt;Kubernetes: A Pod’s Life&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8smeetup.github.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/&#34; target=&#34;_blank&#34;&gt;确定 Pod 失败的原因&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>Kube-router 实战</title>
      <link>https://www.yangcs.net/posts/kube-router/</link>
      <pubDate>Fri, 20 Apr 2018 04:36:40 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/kube-router/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cloudnativelabs/kube-router&#34; target=&#34;_blank&#34;&gt;Kube-router&lt;/a&gt; 是一个挺有想法的项目，兼备了 &lt;code&gt;calico&lt;/code&gt; 和 &lt;code&gt;kube-proxy&lt;/code&gt; 的功能，是基于 Kubernetes 网络设计的一个集负载均衡器、防火墙和容器网络的综合方案。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-体系架构-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. 体系架构&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;Kube-router 是围绕 &lt;span id=&#34;inline-blue&#34;&gt;观察者&lt;/span&gt; 和 &lt;span id=&#34;inline-blue&#34;&gt;控制器&lt;/span&gt; 的概念而建立的。&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;观察者&lt;/span&gt; 使用 &lt;code&gt;Kubernetes watch API&lt;/code&gt; 来获取与创建，更新和删除 Kubernetes 对象有关的事件的通知。 每个观察者获取与特定 API 对象相关的通知。 在从 API 服务器接收事件时，观察者广播事件。&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;inline-blue&#34;&gt;控制器&lt;/span&gt; 注册以获取观察者的事件更新，并处理事件。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Kube-router&lt;/code&gt; 由3个核心控制器和多个观察者组成，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/kube-router-arch.png&#34; alt=&#34;&#34; /&gt;&lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&#34;流程分析&#34;&gt;流程分析&lt;/h3&gt;

&lt;p&gt;Kube-router 启动之后，首先创建 &lt;code&gt;wathcer&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (kr *KubeRouter) Run() error {
	...
	err = kr.startApiWatchers()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;startApiWatchers&lt;/code&gt; 中，会启动 endpoint、namespace、pod、node、networkpolicy、service 这六个 wather。&lt;/p&gt;

&lt;p&gt;这六个 wathcer 将监听的变化发送到 &lt;code&gt;Broadcaster&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func NewBroadcaster() *Broadcaster {
	return &amp;amp;Broadcaster{}
}

func (b *Broadcaster) Add(listener Listener) {
	b.listenerLock.Lock()
	defer b.listenerLock.Unlock()
	b.listeners = append(b.listeners, listener)
}

func (b *Broadcaster) Notify(instance interface{}) {
	b.listenerLock.RLock()
	listeners := b.listeners
	b.listenerLock.RUnlock()
	for _, listener := range listeners {
		go listener.OnUpdate(instance)
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后创建三个 controller：&lt;code&gt;NetworkPolicyController&lt;/code&gt;、&lt;code&gt;NetworkRoutingController&lt;/code&gt;、&lt;code&gt;NetworkServicesControllers&lt;/code&gt;。 每个 controller 会监听所关心的资源的变化。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func NewNetworkServicesController(clientset *kubernetes.Clientset,\
	config *options.KubeRouterConfig) (*NetworkServicesController, error) {
	...
	nsc := NetworkServicesController{}
	...
	watchers.EndpointsWatcher.RegisterHandler(&amp;amp;nsc)
	watchers.ServiceWatcher.RegisterHandler(&amp;amp;nsc)
	...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个 &lt;a href=&#34;https://github.com/cloudnativelabs/kube-router/tree/master/pkg/controllers&#34; target=&#34;_blank&#34;&gt;controller&lt;/a&gt; 遵循以下结构。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func Run() {
    for {
        Sync() // control loop that runs for ever and perfom sync at periodic interval
    }
}

func OnUpdate() {
    Sync() // on receiving update of a watched API object (namespace, node, pod, network policy etc)
}

Sync() {
    //re-concile any state changes
}

Cleanup() {
    // cleanup any changes (to iptables, ipvs, network etc) done to the system
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-id-h2-2-主要功能-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. 主要功能&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;基于-ipvs-lvs-的负载均衡器-run-service-proxy&#34;&gt;基于 IPVS/LVS 的负载均衡器 | &lt;code&gt;--run-service-proxy&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;Kube-router&lt;/code&gt; 采用 Linux 内核的 &lt;code&gt;IPVS&lt;/code&gt; 模块为 K8s 提供 &lt;code&gt;Service&lt;/code&gt; 的代理。&lt;/p&gt;

&lt;p&gt;Kube-router 的负载均衡器功能，会在物理机上创建一个虚拟的 &lt;code&gt;kube-dummy-if&lt;/code&gt; 网卡，然后利用 k8s 的 watch APi 实时更新 &lt;code&gt;svc&lt;/code&gt; 和 &lt;code&gt;ep&lt;/code&gt; 的信息。svc 的 &lt;code&gt;cluster_ip&lt;/code&gt; 会绑定在 kube-dummy-if 网卡上，作为 lvs 的 &lt;code&gt;virtual server&lt;/code&gt; 的地址。&lt;code&gt;realserver&lt;/code&gt; 的 ip 则通过 ep 获取到容器的IP地址。&lt;/p&gt;

&lt;p&gt;基于 Kubernetes 网络服务代理的 Kube-router IPVS 演示&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://asciinema.org/a/120312&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://asciinema.org/a/120312.png&#34; alt=&#34;asciicast&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;特征：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;轮询负载均衡&lt;/li&gt;
&lt;li&gt;基于客户端IP的会话保持&lt;/li&gt;
&lt;li&gt;如果服务控制器与网络路由控制器（带有 &lt;code&gt;–-run-router&lt;/code&gt; 标志的 kube-router）一起使用，源IP将被保留&lt;/li&gt;
&lt;li&gt;用 &lt;code&gt;–-masquerade-all&lt;/code&gt; 参数明确标记伪装(SNAT)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更多详情可以参考：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://link.jianshu.com/?t=https://cloudnativelabs.github.io/post/2017-05-10-kube-network-service-proxy/&#34; target=&#34;_blank&#34;&gt;Kubernetes network services prox with IPVS/LVS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://link.jianshu.com/?t=https://blog.codeship.com/kernel-load-balancing-for-docker-containers-using-ipvs/&#34; target=&#34;_blank&#34;&gt;Kernel Load-Balancing for Docker Containers Using IPVS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.yangcs.net/posts/lvs-persistent-connection/&#34; target=&#34;_blank&#34;&gt;LVS负载均衡之持久性连接介绍&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;容器网络-run-router&#34;&gt;容器网络 | &lt;code&gt;--run-router&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;Kube-router 利用 BGP 协议和 Go 的 &lt;code&gt;GoBGP&lt;/code&gt; 库和为容器网络提供直连的方案。因为用了原生的 Kubernetes API 去构建容器网络，意味着在使用 kube-router 时，不需要在你的集群里面引入其他依赖。&lt;/p&gt;

&lt;p&gt;同样的，kube-router 在引入容器 CNI 时也没有其它的依赖，官方的 &lt;code&gt;bridge&lt;/code&gt; 插件就能满足 kube-rouetr 的需求。&lt;/p&gt;

&lt;p&gt;更多关于 BGP 协议在 Kubernetes 中的使用可以参考：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://link.jianshu.com/?t=https://cloudnativelabs.github.io/post/2017-05-22-kube-pod-networking/&#34; target=&#34;_blank&#34;&gt;Kubernetes pod networking and beyond with BGP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;网络策略管理-run-firewall&#34;&gt;网络策略管理 | &lt;code&gt;--run-firewall&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;网络策略控制器负责从 Kubernetes API 服务器读取命名空间、网络策略和 pod 信息，并相应地使用 &lt;code&gt;ipset&lt;/code&gt; 配置 iptables 以向 pod 提供入口过滤，保证防火墙的规则对系统性能有较低的影响。&lt;/p&gt;

&lt;p&gt;Kube-router 支持 &lt;code&gt;networking.k8s.io/NetworkPolicy&lt;/code&gt; 接口或网络策略 V1/GA &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/39164#issue-197243974&#34; target=&#34;_blank&#34;&gt;semantics&lt;/a&gt; 以及网络策略的 beta 语义。&lt;/p&gt;

&lt;p&gt;更多关于 kube-router 防火墙的功能可以参考：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://link.jianshu.com/?t=https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/&#34; target=&#34;_blank&#34;&gt;Enforcing Kubernetes network policies with iptables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;p-id-h2-3-使用-kube-router-替代-kube-proxy-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 使用 kube-router 替代 kube-proxy&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;下面进入实战阶段，本方案只使用 kube-router 的 &lt;code&gt;service-proxy&lt;/code&gt; 功能，网络插件仍然使用 &lt;code&gt;calico&lt;/code&gt;（估计只有我能想到这么奇葩的组合了 ✌️）&lt;/p&gt;

&lt;h3 id=&#34;前提&#34;&gt;前提&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;已有一个 k8s 集群&lt;/li&gt;
&lt;li&gt;kube-router 能够连接 &lt;code&gt;apiserver&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;如果您选择以 &lt;code&gt;daemonset&lt;/code&gt; 运行 kube-router，那么 kube-apiserver 和 kubelet 必须以 &lt;code&gt;–allow-privileged=true&lt;/code&gt; 选项运行&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;集群环境&#34;&gt;集群环境&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;角色&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;IP 地址&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;主机名&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;k8s master&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;192.168.123.250&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;node1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;k8s node&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;192.168.123.248&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;node2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;k8s node&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;192.168.123.249&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;node3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;安装步骤&#34;&gt;安装步骤&lt;/h3&gt;

&lt;p&gt;如果你正在使用 &lt;code&gt;kube-proxy&lt;/code&gt;，需要先停止 kube-proxy 服务，并且删除相关 iptables 规则。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl stop kube-proxy
$ kube-proxy --cleanup-iptables
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来以 &lt;code&gt;daemonset&lt;/code&gt; 运行 kube-router，这里我们使用 DR 模式。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl --namespace=kube-system create configmap kube-proxy  --from-file=kubeconfig.conf=/root/.kube/config
$ wget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features-dsr.yaml

# 将 kubeadm-kuberouter-all-features-dsr.yaml 里的 --run-router 参数和 --run-firewall 参数的值改为 false
$ kubectl create -f kubeadm-kuberouter-all-features-dsr.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在每台机器上查看 lvs 条目&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ipvsadm -Ln

IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&amp;gt; 192.168.123.250:6443         Masq    1      0          0
  
$ ipvsadm -S -n

-A -t 10.254.0.1:443 -s rr -p 10800
-a -t 10.254.0.1:443 -r 192.168.123.250:6443 -m -w 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出，kube-router 使用的是 lvs 的 nat 模式。&lt;/p&gt;

&lt;h3 id=&#34;创建一个应用测试-kube-router&#34;&gt;创建一个应用测试 kube-router&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl run whats-my-ip --image=cloudnativelabs/whats-my-ip --replicas=3

# 暴露服务
$ kubectl expose deploy whats-my-ip --target-port=8080 --port=8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看创建好的服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get pods -owide

NAME                           READY     STATUS    RESTARTS   AGE       IP               NODE
whats-my-ip-845d4ff4f6-d2ptz   1/1       Running   0          23h       172.20.135.8     192.168.123.249
whats-my-ip-845d4ff4f6-jxzzn   1/1       Running   0          23h       172.20.166.130   192.168.123.250
whats-my-ip-845d4ff4f6-szhhd   1/1       Running   0          34s       172.20.104.9     192.168.123.248

$ kubectl get svc

NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
kubernetes    ClusterIP   10.254.0.1       &amp;lt;none&amp;gt;        443/TCP    45d
whats-my-ip   ClusterIP   10.254.108.117   &amp;lt;none&amp;gt;        8080/TCP   16s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 lvs 规则条目&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ipvsadm -Ln

IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&amp;gt; 192.168.123.250:6443         Masq    1      0          0
TCP  10.254.175.147:8080 rr
  -&amp;gt; 172.20.104.9:8080            Masq    1      0          0
  -&amp;gt; 172.20.135.8:8080            Masq    1      0          0
  -&amp;gt; 172.20.166.130:8080          Masq    1      0          0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以发现本机的 &lt;code&gt;Cluster IP&lt;/code&gt; 代理后端真实 &lt;code&gt;Pod IP&lt;/code&gt;，使用 rr 算法。&lt;/p&gt;

&lt;p&gt;通过 &lt;code&gt;ip a&lt;/code&gt; 可以看到，每添加一个服务，node 节点上面的 &lt;code&gt;kube-dummy-if&lt;/code&gt; 网卡就会增加一个虚IP。&lt;/p&gt;

&lt;h3 id=&#34;session-affinity&#34;&gt;session affinity&lt;/h3&gt;

&lt;p&gt;Service 默认的策略是，通过 round-robin 算法来选择 backend Pod。 要实现基于客户端 IP 的会话亲和性，可以通过设置 &lt;code&gt;service.spec.sessionAffinity&lt;/code&gt; 的值为 &lt;code&gt;ClientIP&lt;/code&gt; （默认值为 &amp;ldquo;None&amp;rdquo;）。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl delete svc whats-my-ip
$ kubectl expose deploy whats-my-ip --target-port=8080 --port=8080 --session-affinity=ClientIP

$ ipvsadm -Ln

IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&amp;gt; 192.168.123.250:6443         Masq    1      0          0
TCP  10.254.226.105:8080 rr persistent 10800
  -&amp;gt; 172.20.135.8:8080            Masq    1      0          0
  -&amp;gt; 172.20.166.130:8080          Masq    1      0          0
  -&amp;gt; 172.20.104.9:8080            Masq    1      0          0
  
$ ipvsadm -S -n

-A -t 10.254.0.1:443 -s rr -p 10800
-a -t 10.254.0.1:443 -r 192.168.123.250:6443 -m -w 1
-A -t 10.254.226.105:8080 -s rr -p 10800
-a -t 10.254.226.105:8080 -r 172.20.135.8:8080 -m -w 1
-a -t 10.254.226.105:8080 -r 172.20.166.130:8080 -m -w 1
-a -t 10.254.226.105:8080 -r 172.20.104.9:8080 -m -w 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到 lvs 的规则条目里多了个 &lt;code&gt;persistent&lt;/code&gt;，即 lvs 的持久连接，关于 lvs 持久连接的具体内容可以参考我的另一篇博文 &lt;a href=&#34;https://www.yangcs.net/posts/lvs-persistent-connection/&#34; target=&#34;_blank&#34;&gt;LVS负载均衡之持久性连接介绍&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;可以通过设置 &lt;code&gt;service.spec.sessionAffinityConfig.clientIP.timeoutSeconds&lt;/code&gt; 的值来修改 lvs 的 &lt;code&gt;persistence_timeout&lt;/code&gt; 超时时间。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;$ kubectl get svc whats-my-ip -o yaml

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2018-04-20T08:16:38Z
  labels:
    run: whats-my-ip
  name: whats-my-ip
  namespace: default
  resourceVersion: &amp;quot;6323769&amp;quot;
  selfLink: /api/v1/namespaces/default/services/whats-my-ip
  uid: 26315fdf-4473-11e8-8388-005056a1bc83
spec:
  clusterIP: 10.254.226.105
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    run: whats-my-ip
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
  type: ClusterIP
status:
  loadBalancer: {}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;nodeport&#34;&gt;NodePort&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl delete svc whats-my-ip
$ kubectl expose deploy whats-my-ip --target-port=8080 --port=8080 --type=NodePort

$ ipvsadm -Ln

IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.123.249:34507 rr
  -&amp;gt; 172.20.135.8:8080            Masq    1      0          0
  -&amp;gt; 172.20.166.130:8080          Masq    1      0          0
  -&amp;gt; 172.20.104.9:8080            Masq    1      0          0
TCP  10.254.0.1:443 rr persistent 10800
  -&amp;gt; 192.168.123.250:6443         Masq    1      0          0
TCP  10.254.175.147:8080 rr
  -&amp;gt; 172.20.135.8:8080            Masq    1      0          0
  -&amp;gt; 172.20.166.130:8080          Masq    1      0          0
  -&amp;gt; 172.20.104.9:8080            Masq    1      0          0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到不仅有虚拟IP条目，还多了对应主机的 lvs 条目。&lt;/p&gt;

&lt;h3 id=&#34;更改算法&#34;&gt;更改算法&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;最少连接数&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl annotate service my-service &amp;quot;kube-router.io/service.scheduler=lc&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;轮询&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl annotate service my-service &amp;quot;kube-router.io/service.scheduler=rr&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;源地址哈希&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl annotate service my-service &amp;quot;kube-router.io/service.scheduler=sh&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;目的地址哈希&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl annotate service my-service &amp;quot;kube-router.io/service.scheduler=dh&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-id-h2-4-问题解决-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. 问题解决&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;接下来需要面对一些非常棘手的问题，我尽可能将问题描述清楚。&lt;/p&gt;

&lt;p id=&#34;div-border-top-red&#34;&gt;
&lt;strong&gt;问题1：&lt;/strong&gt;在集群内某个节点主机上通过 &lt;code&gt;SVC IP+Port&lt;/code&gt; 访问某个应用时，如果 lvs 转到后端的 pod 在本主机上，那么可以访问，如果该 pod 不在本主机上，那么无法访问。
&lt;/p&gt;

&lt;p&gt;可以通过抓包来看一下，现在 &lt;code&gt;service whats-my-ip&lt;/code&gt; 后端有三个 pod，分别运行在 &lt;code&gt;node1&lt;/code&gt;、&lt;code&gt;node2&lt;/code&gt; 和 &lt;code&gt;node3&lt;/code&gt; 上。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods -owide

NAME                           READY     STATUS    RESTARTS   AGE       IP               NODE
whats-my-ip-845d4ff4f6-d2ptz   1/1       Running   0          23h       172.20.135.8     192.168.123.249
whats-my-ip-845d4ff4f6-jxzzn   1/1       Running   0          23h       172.20.166.130   192.168.123.250
whats-my-ip-845d4ff4f6-szhhd   1/1       Running   0          34s       172.20.104.9     192.168.123.248
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 &lt;code&gt;node3&lt;/code&gt; 上访问 &lt;code&gt;whats-my-ip&lt;/code&gt; 服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ip a show|grep 10.254.175.147

    inet 10.254.175.147/32 brd 10.254.175.147 scope link kube-dummy-if

$ ipvsadm -Ln -t 10.254.175.147:8080

Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.175.147:8080 rr
  -&amp;gt; 172.20.104.9:8080            Masq    1      0          0
  -&amp;gt; 172.20.135.8:8080            Masq    1      0          0
  -&amp;gt; 172.20.166.130:8080          Masq    1      0          0

# 第一次访问，不通
$ curl 10.254.175.147:8080

# 第二次访问
$ curl 10.254.175.147:8080

HOSTNAME:whats-my-ip-845d4ff4f6-d2ptz IP:172.20.135.8

# 第三次访问，不通
$ curl 10.254.175.147:8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时在 &lt;code&gt;node1&lt;/code&gt; 上抓包：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ tcpdump -i ens160 host 172.20.166.130 -nn

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on ens160, link-type EN10MB (Ethernet), capture size 262144 bytes
03:27:26.337553 IP 10.254.175.147.42036 &amp;gt; 172.20.166.130.8080: Flags [S], seq 405854371, win 43690, options [mss 65495,sackOK,TS val 359417229 ecr 0,nop,wscale 7], length 0
03:27:27.340131 IP 10.254.175.147.42036 &amp;gt; 172.20.166.130.8080: Flags [S], seq 405854371, win 43690, options [mss 65495,sackOK,TS val 359418232 ecr 0,nop,wscale 7], length 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看到 &lt;code&gt;node1&lt;/code&gt; 将数据包丢弃了，因为源IP是 &lt;code&gt;10.254.175.147&lt;/code&gt;，系统认为这是 node1 自己本身。&lt;/p&gt;

&lt;p&gt;根本原因可以查看 &lt;code&gt;node3&lt;/code&gt; 的路由表：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ip route show table local|grep 10.254.175.147

local 10.254.175.147 dev kube-dummy-if proto kernel scope host src 10.254.175.147
broadcast 10.254.175.147 dev kube-dummy-if proto kernel scope link src 10.254.175.147
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;src&lt;/code&gt; 的值用来告诉该 host 使用 &lt;code&gt;10.254.175.147&lt;/code&gt; 作为 &lt;code&gt;source address&lt;/code&gt;，可以通过修改路由表来解决这个问题：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ip route replace local 10.254.175.147 dev kube-dummy-if proto kernel scope host src 192.168.123.249 table local
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再次在 &lt;code&gt;node1&lt;/code&gt; 上抓包可以发现源IP已经变成了 &lt;code&gt;192.168.123.249&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ tcpdump -i ens160 host 172.20.166.130 -nn

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on ens160, link-type EN10MB (Ethernet), capture size 262144 bytes
03:39:42.824412 IP 192.168.123.249.52684 &amp;gt; 172.20.166.130.8080: Flags [S], seq 3520353543, win 43690, options [mss 65495,sackOK,TS val 360153716 ecr 0,nop,wscale 7], length 0
03:39:42.824542 IP 172.20.166.130.8080 &amp;gt; 192.168.123.249.52684: Flags [S.], seq 4057001749, ack 3520353544, win 28960, options [mss 1460,sackOK,TS val 360143668 ecr 360153716,nop,wscale 7], length 0
03:39:42.824706 IP 192.168.123.249.52684 &amp;gt; 172.20.166.130.8080: Flags [.], ack 1, win 342, options [nop,nop,TS val 360153716 ecr 360143668], length 0
03:39:42.825066 IP 192.168.123.249.52684 &amp;gt; 172.20.166.130.8080: Flags [P.], seq 1:84, ack 1, win 342, options [nop,nop,TS val 360153716 ecr 360143668], length 83: HTTP: GET / HTTP/1.1
03:39:42.825112 IP 172.20.166.130.8080 &amp;gt; 192.168.123.249.52684: Flags [.], ack 84, win 227, options [nop,nop,TS val 360143669 ecr 360153716], length 0
03:39:42.825589 IP 172.20.166.130.8080 &amp;gt; 192.168.123.249.52684: Flags [P.], seq 1:174, ack 84, win 227, options [nop,nop,TS val 360143669 ecr 360153716], length 173: HTTP: HTTP/1.1 200 OK
03:39:42.825735 IP 192.168.123.249.52684 &amp;gt; 172.20.166.130.8080: Flags [.], ack 174, win 350, options [nop,nop,TS val 360153717 ecr 360143669], length 0
03:39:42.825787 IP 192.168.123.249.52684 &amp;gt; 172.20.166.130.8080: Flags [F.], seq 84, ack 174, win 350, options [nop,nop,TS val 360153717 ecr 360143669], length 0
03:39:42.825882 IP 172.20.166.130.8080 &amp;gt; 192.168.123.249.52684: Flags [F.], seq 174, ack 85, win 227, options [nop,nop,TS val 360143669 ecr 360153717], length 0
03:39:42.826002 IP 192.168.123.249.52684 &amp;gt; 172.20.166.130.8080: Flags [.], ack 175, win 350, options [nop,nop,TS val 360153718 ecr 360143669], length 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;
&lt;p id=&#34;div-border-top-red&#34;&gt;
&lt;strong&gt;问题2：&lt;/strong&gt;在集群内某个节点主机上通过 &lt;code&gt;SVC IP+Port&lt;/code&gt; 访问 &lt;code&gt;service kubernetes&lt;/code&gt; 时，如果该节点是 master 节点（即 kube-apiserver 运行在该节点上），那么可以访问，如果该节点不是 master 节点，那么无法访问。
&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;原因和问题1类似，可以通过修改路由表解决：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 例如在 node3 节点上
$ ip route replace local 10.254.0.1 dev kube-dummy-if proto kernel scope host src 192.168.123.249 table local
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;
&lt;p id=&#34;div-border-top-red&#34;&gt;
&lt;strong&gt;问题3：&lt;/strong&gt;在某个 pod 内访问该 pod 本身的 &lt;code&gt;ClusterIP:Port&lt;/code&gt;，如果 lvs 转到后端的 IP 是该 pod 的 IP，那么无法访问，如果不是则可以访问。
&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kube-proxy&lt;/code&gt; 的 iptables 模式也有同样的问题，这个问题可以忽略。&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;问题1和问题2修改路由表可以通过批量 shell 脚本来解决：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/bin/sh

default_if=$(ip route|grep default|awk &#39;{print $5}&#39;)
localip=$(ip a show ${default_if}|egrep -v inet6|grep inet|awk &#39;{print $2}&#39;|awk -F&amp;quot;/&amp;quot; &#39;{print $1}&#39;)
svc_ip=$(ip route show table local|egrep -v broadcast|grep kube-dummy-if|awk &#39;{print $2}&#39;)

for ip in $svc_ip; do
ip route replace local $ip dev kube-dummy-if proto kernel scope host src $localip table local;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果想要在创建 &lt;code&gt;service&lt;/code&gt; 时自动修改路由表，最好还是将该 fix 整合进 kube-router 的源码中。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-5-参考-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;5. 参考&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudnativelabs/kube-router/blob/master/docs/README.md&#34; target=&#34;_blank&#34;&gt;Kube-router Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jianshu.com/p/d69b40580c87&#34; target=&#34;_blank&#34;&gt;kube-router之负载均衡器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudnativelabs/kube-router/issues/376&#34; target=&#34;_blank&#34;&gt;bad routing from host to service IP on same host&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>LVS负载均衡之持久性连接介绍</title>
      <link>https://www.yangcs.net/posts/lvs-persistent-connection/</link>
      <pubDate>Wed, 18 Apr 2018 11:18:06 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/lvs-persistent-connection/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-前言-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. 前言&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;在实际生产环境中，往往需要根据业务应用场景来设置 &lt;code&gt;lvs&lt;/code&gt; 的会话超时时间以及防 &lt;code&gt;session&lt;/code&gt; 连接丢失的问题提，如在业务支付环节，如若 &lt;code&gt;session&lt;/code&gt; 丢失会导致重复扣款问题，严重影响到安全性，本小节解将会讲到关于 &lt;code&gt;lvs&lt;/code&gt; 持久性连接问题。&lt;/p&gt;

&lt;h3 id=&#34;为什么用到持久连接&#34;&gt;为什么用到持久连接？&lt;/h3&gt;

&lt;p&gt;在 Web 服务通信中，当用户在一个网站浏览了A网页并跳转到B网页，此时服务器就认为B网页是一个新的用户请求，你之前的登陆的信息就都丢失了。&lt;/p&gt;

&lt;p&gt;为了记录用户的会话信息，我们的开发者就在客户端/服务器端软件提供了 &lt;code&gt;cookie/session&lt;/code&gt; 机制，当你访问网站时，服务器端建立一个 session 会话区，并建立一个 cookie 与这个 session 绑定，将信息发送给你的浏览器。&lt;/p&gt;

&lt;p&gt;这样，只要你的 cookie 存在，服务器端的 session 存在，那么当你打开新页面的时候，服务器依然会认识你。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在做了负载均衡的时候，上面的机制就出现了问题。假设有以下场景：&lt;/strong&gt;&lt;/p&gt;

&lt;p id=&#34;div-border-top-red&#34;&gt;某电商网站为了实现更多用户的访问，提供了A、B两台服务器，并在前面做了 LVS 负载均衡。于是某用户打开了该购物网站，选中了一件衣服，并加入了购物车(此时背后的操作是：LVS 负载均衡器接受了用户请求，并将其分发到了选中的服务器，并将用户添加了一件衣服记录到这个会话的 session 中)。这时当用户打开了第二个网页，又选中了一件帽子并加入购物车(此时背后的操作是：LVS 负载均衡器接受了用户请求，进行计算，将其发送到选中的服务器上，该服务器将用户添加了一件帽子记录到 session 中)。&lt;br /&gt;&lt;br /&gt;由于 LVS 是一个四层负载均衡器，仅能根据 &lt;code&gt;IP:Port&lt;/code&gt; 对数据报文进行分发，不能确保将同一用户根据 session 发往同一个服务器，也就是用户第一次被分配到了A服务器，而第二次可能分配到了B服务器，但是B服务器并没有A服务器用户的 session 记录，直接导致这个例子里的用户发现自己的购物车没有了之前的衣服，而仅有帽子。这是不可接受的。&lt;/p&gt;

&lt;p&gt;为了避免上面的问题，一般站点会有两种方法解决该问题：&lt;/p&gt;

&lt;p&gt;&lt;font color=&#34;#2780e3&#34;&gt;
1. 将来自于同一个用户的请求发往同一个服务器&lt;br /&gt;
2. 将 session 信息在服务器集群内共享，每个服务器都保存整个集群的 session 信息&lt;br /&gt;
3. 建立一个 session 存储池，所有 session 信息都保存到存储池中
&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
当然通过 session 共享解决是比较完美的，但实现起来相对复杂：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一需要额外增加服务器设备&lt;/li&gt;
&lt;li&gt;二需要代码改动，在用户操作前，需要先获取该用户的session信息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总结下来，第一种方法是最简单的。&lt;/p&gt;

&lt;h3 id=&#34;hash算法与持久连接&#34;&gt;hash算法与持久连接&lt;/h3&gt;

&lt;p&gt;LVS 的八种轮询算法中有（Source Hashing）源地址 hash，它和持久连接的作用都是&lt;font color=&#34;#df3e3e&#34;&gt;将来自同一个IP的请求都转发到同一个 Server&lt;/font&gt;，从而保证了 session 会话定位的问题。两者的不同是：&lt;/p&gt;

&lt;h4 id=&#34;source-hashing-算法&#34;&gt;Source Hashing 算法&lt;/h4&gt;

&lt;p&gt;该算法在内核中会自动维护一个哈希表，此哈希表中用每一个请求的源IP地址经过哈希计算得出的值作为键，把请求所到达的 RS 的地址作为值。&lt;/p&gt;

&lt;p&gt;在后面的请求中，每一个请求会先经过此哈希表，如果请求在此哈希表中有键值，那么直接定向至特定 RS，如没有，则会新生成一个键值，以便后续请求的定向。&lt;/p&gt;

&lt;p&gt;但是此种方法在时间的记录上比较模糊（依据TCP的连接时长计算）。而且通过 hash 算法无法公平均担后端 real server 的请求，即不能与 rr 等算法同时使用。&lt;/p&gt;

&lt;h4 id=&#34;持久连接&#34;&gt;持久连接&lt;/h4&gt;

&lt;p&gt;此种方法实现了无论使用哪一种调度方法，持久连接功能都能保证在指定时间范围之内，来自于同一个IP的请求将始终被定向至同一个 RS，还可以把多种服务绑定后统一进行调度。&lt;/p&gt;

&lt;p&gt;在 director 内有一个 LVS 持久连接模板，模板中记录了每一个请求的来源、调度至的 RS、维护时长等等，所以，在新的请求进入时，首先在此模板中检查是否有记录（有内置的时间限制，比如限制是300秒，当在到达300秒时依然有用户访问，那么持久连接模板就会将时间增加两分钟，再计数，依次类推，每次只延长2分钟），如果该记录未超时，则使用该记录所指向的 RS，如果是超时记录或者是新请求，则会根据调度算法先调度至特定 RS，再将调度的记录添加至此表中。&lt;/p&gt;

&lt;p&gt;这并不与 SH 算法冲突，lvs 持久连接会在新请求达到时，检查后端 RS 的负载状况，这就是比较精细的调度和会话保持方法。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-2-lvs-的持久性连接有两方面-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. lvs 的持久性连接有两方面&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;1、把同一个 &lt;code&gt;client&lt;/code&gt; 的请求信息记录到 lvs 的 &lt;code&gt;hash&lt;/code&gt; 表里，保存时间使用 &lt;code&gt;persistence_timeout&lt;/code&gt; 控制，单位为秒。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;persistence_granularity&lt;/code&gt; 参数是配合 &lt;code&gt;persistence_timeout&lt;/code&gt; 的，在某些情况特别有用。他的值是子网掩码，表示持久连接的粒度，默认是 &lt;code&gt;255.255.255.255&lt;/code&gt;，也就是单独的 client ip，如果改成 &lt;code&gt;255.255.255.0&lt;/code&gt;，和 client ip 在同一个网段的都会被分配到同一个 &lt;code&gt;real server&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2、一个连接创建后空闲时的超时时间，这个时间为3种。&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;tcp:&lt;/strong&gt; tcp的空闲超时时间&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;tcpfin:&lt;/strong&gt; lvs收到客户端tcp fin的超时时间&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;udp:&lt;/strong&gt; udp的超时时间&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;p-id-h2-3-lvs-相关超时时间查看-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. lvs 相关超时时间查看&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;通过 &lt;code&gt;ipvsadm -Ln&lt;/code&gt; 可以查看 persistence_timeout 超时时间(默认超时时间 360s)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ipvsadm -Ln

IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.66.97:8080 rr persistent 10800
  -&amp;gt; 172.20.104.7:8080            Masq    1      0          0
  -&amp;gt; 172.20.135.6:8080            Masq    1      0          0
  -&amp;gt; 172.20.135.7:8080            Masq    1      0          0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过 &lt;code&gt;ipvsadm -Ln --timeout&lt;/code&gt; 可以查看 &lt;code&gt;tcp tcpfin udp&lt;/code&gt; 的超时时间（默认: 900 120 300)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ipvsadm -Ln --timeout

Timeout (tcp tcpfin udp): 900 120 300
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-id-h2-4-lvs-如何控制这些超时时间工作-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. lvs 如何控制这些超时时间工作&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ipvsadm -Lnc

IPVS connection entries
pro expire state       source             virtual            destination
TCP 01:54  TIME_WAIT   192.168.123.248:35672 10.254.66.97:8080  172.20.135.6:8080
TCP 180:03 NONE        192.168.123.248:0  10.254.66.97:8080  172.20.135.6:8080
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;当一个 client 访问 vip 的时候，这时 ipvs 就会记录一条状态为 &lt;code&gt;NONE&lt;/code&gt; 的信息，如述上所示，&lt;code&gt;expire&lt;/code&gt; 初始值为 &lt;code&gt;persistence_timeout&lt;/code&gt; 的值，然后根据时钟主键变小，在以下记录存在期间，同一 client ip 连接上来，都会被分配到同一个后端。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;TIME_WAIT&lt;/code&gt; 的值就是 tcp tcpfin udp 中的 &lt;code&gt;tcpfin&lt;/code&gt; 的超时时间，当 &lt;code&gt;NONE&lt;/code&gt; 的值为0时，如果 TIME_WAIT 还存在，那么 NONE 的值会从新变成 &lt;code&gt;persistence_timeout&lt;/code&gt; 的值，再减少，直到 TIME_WAIT 消失以后，NONE 才会消失，只要 NONE 存在，同一 client 的访问，都会分配到统一 real server。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;p-id-h2-5-lvs-关于相关超时时间的设置-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;5. lvs 关于相关超时时间的设置&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;code&gt;persistence_timeout&lt;/code&gt; 可以通过 &lt;code&gt;ipvsadm -p timeout&lt;/code&gt; 来设置，默认 360 秒。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ipvsadm -A -t 192.168.20.154:80 -s rr -p 60
&lt;/code&gt;&lt;/pre&gt;

&lt;p id=&#34;div-border-top-purple&#34;&gt;上面命令中红色标记的 80 端口，表示如果是同一客户端访问服务器的 80 端口，会被定义到同一个 real server，如果把 80 端口改为 &lt;code&gt;0&lt;/code&gt;，那么同一客户端访问服务器的任何服务都会被转发到同一个 real server。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tcp tcpfin udp&lt;/code&gt; 可以通过 &lt;code&gt;ipvsadm --set 对应超时时间&lt;/code&gt; 来设置。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ipvsadm --set tcp tcpfin udp
&lt;/code&gt;&lt;/pre&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;&lt;code&gt;tcpfin&lt;/code&gt; 的值最好小于 &lt;code&gt;persistence_timeout&lt;/code&gt; 的值，这样比较方便计算，也有利于 &lt;code&gt;tcpfin&lt;/code&gt; 回收&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&#34;p-id-h2-6-持久连接定义与原理-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;6. 持久连接定义与原理&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;定义&#34;&gt;定义&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;持久连接是指无论使用什么算法，LVS 持久都能实现在一定时间内，将来自同一个客户端请求派发至此前选定的 &lt;code&gt;RS&lt;/code&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;原理&#34;&gt;原理&lt;/h3&gt;

&lt;p&gt;当使用 LVS 持久性的时候，Director 在内部使用一个连接根据记录称之为 &lt;code&gt;持久连接模板&lt;/code&gt; 来确保所有来自同一个客户端的请求被分发到同一台 &lt;code&gt;Real Server&lt;/code&gt; 上。&lt;/p&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;持久连接模板是指每一个客户端及分配给它的 &lt;code&gt;RS&lt;/code&gt; 的映射关系。&lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&#34;持久连接分类&#34;&gt;持久连接分类&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1、 &lt;font color=&#34;red&#34;&gt;持久端口连接，&lt;/font&gt;简称 PPC（Persistent Port Connections）&lt;/strong&gt;：将来自于同一个客户端对同一个集群某个服务的请求，始终定向至此前选定的 &lt;code&gt;RS&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;例如：&lt;/strong&gt;&lt;code&gt;client----&amp;gt;LVS(80)----&amp;gt;RS1 或 client----&amp;gt;LVS(23)----&amp;gt;RS2&lt;/code&gt;&lt;br /&gt;
&lt;strong&gt;缺陷：&lt;/strong&gt;期望访问不同的端口到同一台 &lt;code&gt;RS&lt;/code&gt; 上，无法实现。&lt;/p&gt;

&lt;p&gt;配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ipvsadm -A -t 172.16.100.1:80 -s rr -p 3600
$ ipvsadm -a -t 172.16.100.1:80 -r 172.16.100.10 -g -w 2
$ ipvsadm -a -t 172.16.100.1:80 -r 172.16.100.11 -g -w 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;2、&lt;font color=&#34;red&#34;&gt;持久客户端连接，&lt;/font&gt;简称 PCC（Persistent Client Connections）：&lt;/strong&gt;将来自于同一个客户端对所有端口的请求，始终定向至此前选定的 &lt;code&gt;RS&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;&lt;code&gt;PCC&lt;/code&gt; 是一个虚拟服务没有端口号（或者端口号为 0），以 &lt;code&gt;-p&lt;/code&gt; 来标识服务。&lt;br /&gt;
&lt;strong&gt;缺陷：&lt;/strong&gt;定向所有服务，期望访问不同的 Real Server 无法实现。&lt;/p&gt;

&lt;p&gt;配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ipvsadm -A -t 172.16.100.1:0 -s rr -p 3600
$ ipvsadm -a -t 172.16.100.1:0 -r 172.16.100.10 -g -w 2
$ ipvsadm -a -t 172.16.100.1:0 -r 172.16.100.11 -g -w 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;3、&lt;font color=&#34;red&#34;&gt;基于防火墙设置端口绑定的持久连接，&lt;/font&gt;简称 PNMPP（Persistent Netfilter Marked Packet Persistence）：&lt;/strong&gt;例如后台 real server 同时提供 &lt;code&gt;80&lt;/code&gt; 和 &lt;code&gt;443&lt;/code&gt; 端口的服务，并且两个服务之间有联系，这时就要用到 PNMPC&lt;/p&gt;

&lt;p&gt;先对某一特定类型的数据包打上标记，然后再将基于某一类标记的服务送到后台的 &lt;code&gt;Real Server&lt;/code&gt; 上去，后台的 Real Server 并不识别这些标记。将&lt;code&gt;持久连接&lt;/code&gt;和&lt;code&gt;防火墙标记&lt;/code&gt;结合起来就能够实现端口姻亲功能，只要是来自某一客户端的对某一特定服务（需要不同的端口）的访问都定义到同一台 Real Server 上去。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;案例：&lt;/strong&gt;一个用户在访问购物网站时同时使用 &lt;code&gt;HTTP（80）&lt;/code&gt;和 &lt;code&gt;HTTPS（443）&lt;/code&gt;两种协议，我们需要将其定义到同一台 Real Server 上，而其他的服务不受限制。&lt;/p&gt;

&lt;p&gt;配置：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ iptables -t mangle -A PREROUTING -d 172.16.100.1 -i eth0 -p tcp --dport 80 -j MARK --set-mark 8
$ iptables -t mangle -A PREROUTING -d 172.16.100.1 -i eth0 -p tcp --dport 443 -j MARK --set-mark 8
$ ipvsadm -A -f 8 -s rr -p 600
$ ipvsadm -a -f 8 -r 172.16.100.10 -g -w 2
$ ipvsadm -a -f 8 -r 172.16.100.11 -g -w 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-id-h2-7-总结-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;7. 总结&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;如何设置 &lt;code&gt;lvs&lt;/code&gt; 持久性连接需要根据业务场景来选择，比如电商平台，对应的持久性连接应该是 &lt;code&gt;PNMPP&lt;/code&gt;，另外还需要根据连接类型，比如长连接和短连接，来设置相关超时时间，总之,根据应用场景来选择！&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>CRI-O 简介</title>
      <link>https://www.yangcs.net/posts/cri-o/</link>
      <pubDate>Tue, 03 Apr 2018 08:11:38 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/cri-o/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;上一篇 &lt;a href=&#34;https://www.yangcs.net/posts/container-runtime&#34; target=&#34;_blank&#34;&gt;https://www.yangcs.net/posts/container-runtime/&lt;/a&gt; 介绍了什么是容器运行时，并列出了不同的容器运行时。本篇重点介绍其中的一种容器运行时 &lt;code&gt;CRI-O&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-cri-o-的诞生-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. CRI-O 的诞生&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;当容器运行时（Container Runtime）的标准被提出以后，Red Hat 的一些人开始想他们可以构建一个更简单的运行时，而且这个运行时仅仅为 Kubernetes 所用。这样就有了 &lt;code&gt;skunkworks&lt;/code&gt;项目，最后定名为 &lt;code&gt;CRI-O&lt;/code&gt;， 它实现了一个最小的 CRI 接口。在 2017 Kubecon Austin 的一个演讲中， Walsh 解释说， ”CRI-O 被设计为比其他的方案都要小，遵从 Unix 只做一件事并把它做好的设计哲学，实现组件重用“。&lt;/p&gt;

&lt;p&gt;根据 Red Hat 的 CRI-O 开发者 Mrunal Patel 在研究里面说的， 最开始 Red Hat 在 2016 年底为它的 OpenShift 平台启动了这个项目，同时项目也得到了 &lt;code&gt;Intel&lt;/code&gt; 和 &lt;code&gt;SUSE&lt;/code&gt; 的支持。CRI-O 与 &lt;code&gt;CRI&lt;/code&gt; 规范兼容，并且与 &lt;code&gt;OCI&lt;/code&gt; 和 Docker 镜像的格式也兼容。它也支持校验镜像的 GPG 签名。 它使用容器网络接口 Container Network Interface（CNI）处理网络，以便任何兼容 CNI 的网络插件可与该项目一起使用，OpenShift 也用它来做软件定义存储层。 它支持多个 CoW 文件系统，比如常见的 overlay，aufs，也支持不太常见的 Btrfs。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-2-cri-o-的原理及架构-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. CRI-O 的原理及架构&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;CRI-O 最出名的特点是它支持“受信容器”和“非受信容器”的混合工作负载。比如，CRI-O 可以使用 &lt;a href=&#34;https://clearlinux.org/containers&#34; target=&#34;_blank&#34;&gt;Clear Containers&lt;/a&gt; 做强隔离，这样在多租户配置或者运行非信任代码时很有用。这个功能如何集成进 Kubernetes 现在还不太清楚，Kubernetes 现在认为所有的后端都是一样的。&lt;/p&gt;

&lt;p&gt;当 Kubernetes 需要运行容器时，它会与 CRI-O 进行通信，CRI-O 守护程序与 &lt;code&gt;runc&lt;/code&gt;（或另一个符合 OCI 标准的运行时）一起启动容器。当 Kubernetes 需要停止容器时，CRI-O 会来处理，它只是在幕后管理 Linux 容器，以便用户不需要担心这个关键的容器编排。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://dn-linuxcn.qbox.me/data/attachment/album/201710/31/234945ee5euzn7hu0cnu4u.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;CRI-O 有一个有趣的架构（见下图），它重用了很多基础组件，下面我们来看一下各个组件的功能及工作流程。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/crio-architecture.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kubernetes 通知 &lt;code&gt;kubelet&lt;/code&gt; 启动一个 pod。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubelet 通过 &lt;code&gt;CRI&lt;/code&gt;(Container runtime interface) 将请求转发给 &lt;code&gt;CRI-O daemon&lt;/code&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CRI-O 利用 &lt;code&gt;containers/image&lt;/code&gt; 库从镜像仓库拉取镜像。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;下载好的镜像被解压到容器的根文件系统中，并通过 &lt;code&gt;containers/storage&lt;/code&gt; 库存储到 COW 文件系统中。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在为容器创建 &lt;code&gt;rootfs&lt;/code&gt; 之后，CRI-O 通过 &lt;a href=&#34;https://github.com/opencontainers/runtime-tools&#34; target=&#34;_blank&#34;&gt;oci-runtime-tool&lt;/a&gt; 生成一个 OCI 运行时规范 json 文件，描述如何使用 OCI Generate tools 运行容器。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;然后 CRI-O 使用规范启动一个兼容 CRI 的运行时来运行容器进程。默认的运行时是 &lt;code&gt;runc&lt;/code&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每个容器都由一个独立的 &lt;code&gt;conmon&lt;/code&gt; 进程监控，conmon 为容器中 pid 为 1 的进程提供一个 &lt;code&gt;pty&lt;/code&gt;。同时它还负责处理容器的日志记录并记录容器进程的退出代码。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;网络是通过 CNI 接口设置的，所以任何 CNI 插件都可以与 CRI-O 一起使用。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;隆重介绍一下-conmon&#34;&gt;隆重介绍一下 conmon&lt;/h3&gt;

&lt;p&gt;根据 Patel 所说，conmon 程序是“纯C编写的，用来提高稳定性和性能”，conmon 负责监控，日志，TTY 分配，以及类似 &lt;code&gt;out-of-memory&lt;/code&gt; 情况的杂事。&lt;/p&gt;

&lt;p&gt;conmon 需要去做所有 &lt;code&gt;systemd&lt;/code&gt; 不做或者不想做的事情。即使 CRI-O 不直接使用 systemd 来管理容器，它也将容器分配到 sytemd 兼容的 &lt;code&gt;cgroup&lt;/code&gt; 中，这样常规的 systemd 工具比如 &lt;code&gt;systemctl&lt;/code&gt; 就可以看见容器资源使用情况了。&lt;/p&gt;

&lt;p&gt;因为 conmon（不是CRI daemon）是容器的父进程，它允许 CRI-O 的部分组件重启而不会影响容器，这样可以保证更加平滑的升级。&lt;strong&gt;现在 Docker 部署的问题就是 Docker 升级需要重起所有的容器&lt;/strong&gt;。 通常这对于 Kubernetes 集群来说不是问题，但因为它可以将容器迁移来滚动升级。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-下一步-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 下一步&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;code&gt;CRI-O 1.0&lt;/code&gt; 在2017年10月发布，支持 Kubernetes 1.7，后来 CRI-O 1.8，1.9 相继发布，支持 Kubernetes 的 1.8， 1.9（此时版本命名规则改为与Kubernetes一致）。&lt;/p&gt;

&lt;p&gt;CRI-O 在 &lt;code&gt;Openshift 3.7&lt;/code&gt; 中作为 beta 版提供，Patel 考虑在 &lt;code&gt;Openshift 3.9&lt;/code&gt; 中让它进步一步稳定，在 3.10 中成为缺省的运行时，同时让 Docker 作为候选的运行时。&lt;/p&gt;

&lt;p&gt;下一步的工作包括集成新的 &lt;code&gt;Kata Containers&lt;/code&gt; 的这个基于 VM 的运行时，增加 &lt;code&gt;kube-spawn&lt;/code&gt; 的支持，支持更多类似 NFS， GlusterFS 的存储后端等。 团队也在讨论如何通过支持 &lt;code&gt;casync&lt;/code&gt; 或者 &lt;code&gt;libtorrent&lt;/code&gt; 来优化多节点间的镜像同步。&lt;/p&gt;

&lt;p&gt;如果你想贡献或者关注开发，就去 &lt;a href=&#34;https://github.com/kubernetes-incubator/cri-o&#34; target=&#34;_blank&#34;&gt;CRI-O 项目的 GitHub 仓库&lt;/a&gt;，然后关注 &lt;a href=&#34;https://medium.com/cri-o&#34; target=&#34;_blank&#34;&gt;CRI-O 博客&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-4-参考-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. 参考&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.projectatomic.io/blog/2017/02/crio-runtimes/&#34; target=&#34;_blank&#34;&gt;CRI-O and Alternative Runtimes in Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cri-o.io/&#34; target=&#34;_blank&#34;&gt;Lightweight Container Runtime for Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/cri-o/cri-o-support-for-kubernetes-4934830eb98e&#34; target=&#34;_blank&#34;&gt;CRI-O Support for Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://linux.cn/article-9015-1.html&#34; target=&#34;_blank&#34;&gt;CRI-O 1.0 简介&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em; 
    margin-right: 5px; 
    padding: 8px 15px; 
    letter-spacing: 2px; 
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181)); 
    background-color: rgb(63, 81, 181); 
    color: rgb(255, 255, 255); 
    border-left: 10px solid rgb(51, 51, 51); 
    border-radius:5px; 
    text-shadow: rgb(102, 102, 102) 1px 1px 1px; 
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes 中的容器运行时</title>
      <link>https://www.yangcs.net/posts/container-runtime/</link>
      <pubDate>Tue, 03 Apr 2018 06:50:43 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/container-runtime/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;容器运行时（Container Runtime）是 Kubernetes 最重要的组件之一，负责真正管理镜像和容器的生命周期。Kubelet 通过 &lt;code&gt;Container Runtime Interface (CRI)&lt;/code&gt; 与容器运行时交互，以管理镜像和容器。&lt;/p&gt;

&lt;p&gt;容器运行时接口(&lt;code&gt;Container Runtime Interface (CRI)&lt;/code&gt;) 是 Kubelet 1.5 和 kubelet 1.6 中主要负责的一块项目，它重新定义了 Kubelet Container Runtime API，将原来完全面向 Pod 级别的 API 拆分成面向 &lt;code&gt;Sandbox&lt;/code&gt; 和 &lt;code&gt;Container&lt;/code&gt; 的 API，并分离镜像管理和容器引擎到不同的服务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.feisky.xyz/zh/plugins/images/cri.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;CRI 最早从从 1.4 版就开始设计讨论和开发，在 v1.5 中发布第一个测试版。在 v1.6 时已经有了很多外部容器运行时，如 frakti、cri-o 的 alpha 支持。v1.7 版本新增了 &lt;code&gt;cri-containerd&lt;/code&gt; 的 alpha 支持，而 &lt;code&gt;frakti&lt;/code&gt; 和 &lt;code&gt;cri-o&lt;/code&gt; 则升级到 beta 支持。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-cri-接口-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. CRI 接口&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;CRI 基于 &lt;code&gt;gRPC&lt;/code&gt; 定义了 &lt;code&gt;RuntimeService&lt;/code&gt; 和 &lt;code&gt;ImageService&lt;/code&gt;，分别用于容器运行时和镜像的管理。其定义在&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;v1.10+:&lt;/strong&gt; &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/kubelet/apis/cri/runtime/v1alpha2&#34; target=&#34;_blank&#34;&gt;pkg/kubelet/apis/cri/v1alpha2/runtime&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;v1.7~v1.9:&lt;/strong&gt; &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/release-1.9/pkg/kubelet/apis/cri/v1alpha1/runtime&#34; target=&#34;_blank&#34;&gt;pkg/kubelet/apis/cri/v1alpha1/runtime&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;v1.6:&lt;/strong&gt; &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/release-1.6/pkg/kubelet/api/v1alpha1/runtime&#34; target=&#34;_blank&#34;&gt;pkg/kubelet/api/v1alpha1/runtime&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubelet 作为 CRI 的客户端，而 Runtime 维护者则需要实现 CRI 服务端，并在启动 kubelet 时将其传入：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubelet --container-runtime=remote --container-runtime-endpoint=unix:///var/run/crio/crio.sock ..
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-id-h2-2-如何开发新的-container-runtime-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. 如何开发新的 Container Runtime&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;开发新的 Container Runtime 只需要实现 &lt;code&gt;CRI gRPC Server&lt;/code&gt;，包括 &lt;code&gt;RuntimeService&lt;/code&gt; 和 &lt;code&gt;ImageService&lt;/code&gt;。该 gRPC Server 需要监听在本地的 &lt;code&gt;unix socket&lt;/code&gt;（Linux 支持 unix socket 格式，Windows 支持 tcp 格式）。&lt;/p&gt;

&lt;p&gt;具体的实现方法可以参考下面已经支持的 Container Runtime 列表。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-目前支持的-container-runtime-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 目前支持的 Container Runtime&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;目前，有多家厂商都在基于 CRI 集成自己的容器引擎，其中包括:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Docker:&lt;/strong&gt; 核心代码依然保留在 kubelet 内部（&lt;code&gt;pkg/kubelet/dockershim&lt;/code&gt;），依然是最稳定和特性支持最好的 Runtime&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/kubernetes/frakti&#34; target=&#34;_blank&#34;&gt;HyperContainer&lt;/a&gt;:&lt;/strong&gt; 支持 Kubernetes v1.6+，提供基于 &lt;code&gt;hypervisor&lt;/code&gt; 和 docker 的混合运行时，适用于运行非可信应用，如多租户和 &lt;code&gt;NFV&lt;/code&gt; 等场景&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Runc&lt;/strong&gt; 有两个实现，cri-o 和 cri-containerd&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/cri-containerd&#34; target=&#34;_blank&#34;&gt;cri-containerd&lt;/a&gt;: 支持 kubernetes v1.7+&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/cri-o&#34; target=&#34;_blank&#34;&gt;cri-o&lt;/a&gt;: 支持 Kubernetes v1.6+，底层运行时支持 runc 和 intel clear container&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/kubernetes-incubator/rktlet&#34; target=&#34;_blank&#34;&gt;Rkt&lt;/a&gt;:&lt;/strong&gt; 开发中&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/Mirantis/virtlet&#34; target=&#34;_blank&#34;&gt;Mirantis&lt;/a&gt;:&lt;/strong&gt; 直接管理 &lt;code&gt;libvirt&lt;/code&gt; 虚拟机，镜像须是 &lt;code&gt;qcow2&lt;/code&gt; 格式&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/apporbit/infranetes&#34; target=&#34;_blank&#34;&gt;Infranetes&lt;/a&gt;:&lt;/strong&gt; 直接管理 IaaS 平台虚拟机，如 GCE、AWS 等&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;cri-containerd&#34;&gt;cri-containerd&lt;/h3&gt;

&lt;p&gt;以 containerd 为例，它将 &lt;code&gt;dockershim&lt;/code&gt; 和 &lt;code&gt;docker daemon&lt;/code&gt; 替换为 &lt;code&gt;cri-containerd&lt;/code&gt; 服务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kubernetes.feisky.xyz/zh/plugins/images/cri-containerd.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;而 cri-containerd 则实现了 Kubelet CRI 接口，对 Kubelet 暴露 Image Service 和 Runtime Service。在内部，它通过 containerd 的 gRPC 接口管理容器和镜像，并通过 CNI 插件给 Pod 配置网络。
&lt;img src=&#34;https://kubernetes.feisky.xyz/zh/plugins/images/containerd.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-4-cri-tools-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. CRI Tools&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;为了方便开发、调试和验证新的 Container Runtime，社区还维护了一个 &lt;a href=&#34;https://github.com/kubernetes-incubator/cri-tools&#34; target=&#34;_blank&#34;&gt;cri-tools&lt;/a&gt; 工具，它提供两个组件&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;crictl:&lt;/code&gt; 类似于 docker 的命令行工具，不需要通过 Kubelet 就可以跟 Container Runtime 通信，可用来调试或排查问题&lt;/li&gt;
&lt;li&gt;&lt;code&gt;critest:&lt;/code&gt; CRI 的验证测试工具，用来验证新的 Container Runtime 是否实现了 CRI 需要的功能&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外一个工具是 &lt;a href=&#34;https://github.com/projectatomic/libpod&#34; target=&#34;_blank&#34;&gt;libpod&lt;/a&gt;，它也提供了一个组件：&lt;a href=&#34;https://github.com/projectatomic/libpod/blob/master/cmd/podman&#34; target=&#34;_blank&#34;&gt;podman&lt;/a&gt;，功能和 &lt;code&gt;crictl&lt;/code&gt; 类似。&lt;/p&gt;

&lt;p&gt;如果想构建 oci 格式的镜像，可以使用工具：&lt;a href=&#34;https://github.com/projectatomic/buildah&#34; target=&#34;_blank&#34;&gt;buildah&lt;/a&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em; 
    margin-right: 5px; 
    padding: 8px 15px; 
    letter-spacing: 2px; 
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181)); 
    background-color: rgb(63, 81, 181); 
    color: rgb(255, 255, 255); 
    border-left: 10px solid rgb(51, 51, 51); 
    border-radius:5px; 
    text-shadow: rgb(102, 102, 102) 1px 1px 1px; 
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>docker 在本地如何管理 image（镜像）?</title>
      <link>https://www.yangcs.net/posts/how-manage-image/</link>
      <pubDate>Mon, 02 Apr 2018 05:12:18 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/how-manage-image/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;docker 里面可以通过 &lt;code&gt;docker pull&lt;/code&gt;、&lt;code&gt;docker build&lt;/code&gt;、&lt;code&gt;docker commit&lt;/code&gt;、&lt;code&gt;docker load&lt;/code&gt;、&lt;code&gt;docker import&lt;/code&gt; 等方式得到一个 image，得到 image 之后 docker 在本地是怎么存储的呢？本篇将以 &lt;code&gt;docker pull&lt;/code&gt; 为例，简述 image 的获取和存储方式。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-镜像相关的配置-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. 镜像相关的配置&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;docker 里面和 image 有关的目录为 &lt;code&gt;/var/lib/docker&lt;/code&gt;，里面存放着 image 的所有信息，可以通过下面这个 dockerd 的启动参数来修改这个目录的路径。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--graph, -g /var/lib/docker Root of the Docker runtime
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-id-h2-2-镜像的引用方式-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. 镜像的引用方式&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;在需要引用 image 的时候，比如 docker pull 的时候，或者运行容器的时候，都需要指定一个image名称，引用一个镜像有多种方式，下面以 &lt;code&gt;alpine&lt;/code&gt; 为例进行说明.&lt;/p&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;由于 sha256 码太长，所以用 abcdef... 来表示完整的 sha256，节约空间&lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&#34;docker-hub-上的官方镜像&#34;&gt;docker hub 上的官方镜像&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;alpine:&lt;/strong&gt; 官方提供的最新 alpine 镜像，对应的完整名称为 &lt;code&gt;docker.io/library/alpine:latest&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;alpine:3.7:&lt;/strong&gt; 官方提供的 alpine 3.7 镜像，对应的完整名称为 &lt;code&gt;docker.io/library/alpine:3.7&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;alpine:@sha256:abcdef&amp;hellip;:&lt;/strong&gt; 官方提供的 digest 码为 sha256:abcdef&amp;hellip; 的 alpine 镜像，对应的完整名称为 &lt;code&gt;docker.io/library/alpine@sha256:abcdef...&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;docker-hub-上的非官方-个人-镜像&#34;&gt;docker hub 上的非官方（个人）镜像&lt;/h3&gt;

&lt;p&gt;引用方式和官方镜像一样，唯一不同的是需要在镜像名称前面带上用户前缀，如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;user1/alpine:&lt;/strong&gt; 由 user1 提供的最新 alpine 镜像， 对应的完整名称为 &lt;code&gt;docker.io/user1/alpine:latest&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;user1/alpine:3.7&lt;/code&gt; 和 &lt;code&gt;user1/alpine:@sha256:abcdef...&lt;/code&gt; 这两种方式也是和上面一样，等同于 &lt;code&gt;docker.io/user1/alpine:3.7&lt;/code&gt; 和 &lt;code&gt;docker.io/user1/alpine:@sha256:abcdef...&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;自己搭建的-registry-里的镜像&#34;&gt;自己搭建的 registry 里的镜像&lt;/h3&gt;

&lt;p&gt;引用方式和 &lt;code&gt;docker hub&lt;/code&gt; 一样，唯一不同的是需要在镜像名称最前面带上地址，如：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;localhost:5000/alpine:&lt;/strong&gt; 本地自己搭建的 registry（localhost:5000）里面的官方 alpine 的最新镜像，对应的完整名称为 &lt;code&gt;localhost:5000/library/alpine:latest&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;localhost:5000/user1/alpine@sha256:a123def&amp;hellip;:&lt;/strong&gt; 本地自己搭建的 registry（localhost:5000）里面由用户 user1 提供的 digest 为 sha256:a123def 的 alpine 镜像&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其它的几种情况和上面的类似。&lt;/p&gt;

&lt;h3 id=&#34;为什么需要镜像的-digest&#34;&gt;为什么需要镜像的 digest？&lt;/h3&gt;

&lt;p&gt;对于某些 &lt;code&gt;image&lt;/code&gt; 来说，可能在发布之后还会做一些更新，比如安全方面的，这时虽然镜像的内容变了，但镜像的名称和 &lt;code&gt;tag&lt;/code&gt; 没有变，所以会造成前后两次通过同样的名称和 &lt;code&gt;tag&lt;/code&gt; 从服务器得到不同的两个镜像的问题，于是 docker 引入了镜像的 &lt;code&gt;digest&lt;/code&gt; 的概念，一个镜像的 &lt;code&gt;digest&lt;/code&gt; 就是镜像的 &lt;code&gt;manifes&lt;/code&gt; 文件的 &lt;code&gt;sha256&lt;/code&gt; 码，当镜像的内容发生变化的时候，即镜像的 &lt;code&gt;layer&lt;/code&gt; 发生变化，从而 &lt;code&gt;layer&lt;/code&gt; 的 &lt;code&gt;sha256&lt;/code&gt; 发生变化，而 &lt;code&gt;manifest&lt;/code&gt; 里面包含了每一个 &lt;code&gt;layer&lt;/code&gt; 的 &lt;code&gt;sha256&lt;/code&gt;，所以 &lt;code&gt;manifest&lt;/code&gt; 的 &lt;code&gt;sha256&lt;/code&gt; 也会发生变化，即镜像的 &lt;code&gt;digest&lt;/code&gt; 发生变化，这样就保证了 &lt;code&gt;digest&lt;/code&gt; 能唯一的对应一个镜像。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-docker-pull的大概过程-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. docker pull的大概过程&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;如果对 Image manifest，Image Config 和 Filesystem Layers 等概念不是很了解，请先参考 &lt;a href=&#34;https://segmentfault.com/a/1190000009309347&#34; target=&#34;_blank&#34;&gt;image(镜像)是什么&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;取 image 的大概过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;docker 发送 &lt;code&gt;image&lt;/code&gt; 的名称+tag（或者 digest）给 &lt;code&gt;registry&lt;/code&gt; 服务器，服务器根据收到的 image 的名称+tag（或者 digest），找到相应 image 的 &lt;code&gt;manifest&lt;/code&gt;，然后将 manifest 返回给 docker&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;docker 得到 &lt;code&gt;manifest&lt;/code&gt; 后，读取里面 image 配置文件的 &lt;code&gt;digest&lt;/code&gt;(sha256)，这个 sha256 码就是 image 的 &lt;code&gt;ID&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;根据 &lt;code&gt;ID&lt;/code&gt; 在本地找有没有存在同样 &lt;code&gt;ID&lt;/code&gt; 的 image，有的话就不用继续下载了&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果没有，那么会给 registry 服务器发请求（里面包含配置文件的 &lt;code&gt;sha256&lt;/code&gt; 和 &lt;code&gt;media type&lt;/code&gt;），拿到 image 的配置文件（&lt;code&gt;Image Config&lt;/code&gt;）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;根据配置文件中的 &lt;code&gt;diff_ids&lt;/code&gt;（每个 diffid 对应一个 layer tar 包的 sha256，tar 包相当于 layer 的原始格式），在本地找对应的 layer 是否存在&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果 layer 不存在，则根据 &lt;code&gt;manifest&lt;/code&gt; 里面 layer 的 &lt;code&gt;sha256&lt;/code&gt; 和 &lt;code&gt;media type&lt;/code&gt; 去服务器拿相应的 layer（相当去拿压缩格式的包）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;拿到后进行解压，并检查解压后 tar 包的 sha256 能否和配置文件（&lt;code&gt;Image Config&lt;/code&gt;）中的 &lt;code&gt;diff_id&lt;/code&gt; 对的上，对不上说明有问题，下载失败&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;根据 docker 所用的后台文件系统类型，解压 tar 包并放到指定的目录&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;等所有的 layer 都下载完成后，整个 image 下载完成，就可以使用了&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;对于 layer 来说，&lt;code&gt;config&lt;/code&gt; 文件中 diffid 是 layer 的 &lt;code&gt;tar&lt;/code&gt; 包的 sha256，而 &lt;code&gt;manifest&lt;/code&gt; 文件中的 digest 依赖于 media type，比如 media type 是 &lt;code&gt;tar+gzip&lt;/code&gt;，那 digest 就是 layer 的 tar 包经过 gzip 压缩后的内容的 sha256，如果 media type 就是 tar 的话，diffid 和 digest 就会一样。&lt;/p&gt;
&lt;/div&gt;

&lt;blockquote&gt;
&lt;p&gt;dockerd 和 registry 服务器之间的协议为 &lt;a href=&#34;https://docs.docker.com/registry/spec/api/&#34; target=&#34;_blank&#34;&gt;Registry HTTP API V2&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;p-id-h2-4-image-本地存放位置-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. image 本地存放位置&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;这里以 &lt;code&gt;ubuntu&lt;/code&gt; 的 image 为例，展示 docker 的 image 存储方式。&lt;/p&gt;

&lt;p&gt;先看看 ubuntu 的 &lt;code&gt;image id&lt;/code&gt; 和 &lt;code&gt;digest&lt;/code&gt;，然后再分析 image 数据都存在哪里。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker images --digests

REPOSITORY                                           TAG                 DIGEST                                                                    IMAGE ID            CREATED             SIZE
ubuntu                                               latest              sha256:e348fbbea0e0a0e73ab0370de151e7800684445c509d46195aef73e090a49bd6   f975c5035748        3 weeks ago         112MB
......
&lt;/code&gt;&lt;/pre&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;对于本地生成的镜像来说，由于没有上传到 registry 上去，所以没有 digest，因为镜像的 manifest 由 registry 生成。&lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&#34;repositories-json&#34;&gt;repositories.json&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;repositories.json&lt;/code&gt; 中记录了和本地 image 相关的 &lt;code&gt;repository&lt;/code&gt; 信息，主要是 &lt;code&gt;name&lt;/code&gt; 和 &lt;code&gt;image id&lt;/code&gt; 的对应关系，当 image 从 registry 上被 pull 下来后，就会更新该文件：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#这里目录中的 overlay2 为 docker 后台所采用的存储文件系统名称，
#如果是其他的文件系统的话，名字会是其他的，比如btrfs、aufs、devicemapper等。
$ cat /var/lib/docker/image/overlay2/repositories.json|jq .

{
    &amp;quot;Repositories&amp;quot;: {
        &amp;quot;ubuntu&amp;quot;: {
            &amp;quot;ubuntu:latest&amp;quot;: &amp;quot;sha256:f975c50357489439eb9145dbfa16bb7cd06c02c31aa4df45c77de4d2baa4e232&amp;quot;,
            &amp;quot;ubuntu@sha256:e348fbbea0e0a0e73ab0370de151e7800684445c509d46195aef73e090a49bd6&amp;quot;: &amp;quot;sha256:f975c50357489439eb9145dbfa16bb7cd06c02c31aa4df45c77de4d2baa4e232&amp;quot;
        }
        ......
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;ubuntu:&lt;/strong&gt; &lt;code&gt;repository&lt;/code&gt; 的名称，前面没有服务器信息的表示这是官方 registry(docker hub) 里面的 repository，里面包含的都是 image 标识和 image ID 的对应关系&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;ubuntu:latest 和 ubuntu@sha256:e348fbb&amp;hellip;:&lt;/strong&gt; 他们都指向同一个image（&lt;code&gt;sha256:f975c5...&lt;/code&gt;）&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;配置文件-image-config&#34;&gt;配置文件（image config）&lt;/h3&gt;

&lt;p&gt;docker 根据后台所采用的文件系统不同，在 &lt;code&gt;/var/lib/docker&lt;/code&gt; 目录下创建了不同的子目录，对于 &lt;code&gt;CentOS&lt;/code&gt; 来说，默认文件系统是 &lt;code&gt;overlay2&lt;/code&gt;。本文以 &lt;span id=&#34;inline-purple&#34;&gt;CentOS&lt;/span&gt; 为例。&lt;/p&gt;

&lt;p&gt;docker 根据第一步得到的 manifest，从 registry 拿到 config 文件，然后保存在 &lt;code&gt;image/overlay2/imagedb/content/sha256/&lt;/code&gt; 目录下，文件名称就是文件内容的 &lt;code&gt;sha256&lt;/code&gt; 码，即 &lt;code&gt;image id&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sha256sum /var/lib/docker/image/overlay2/imagedb/content/sha256/f975c50357489439eb9145dbfa16bb7cd06c02c31aa4df45c77de4d2baa4e232

f975c50357489439eb9145dbfa16bb7cd06c02c31aa4df45c77de4d2baa4e232  /var/lib/docker/image/overlay2/imagedb/content/sha256/f975c50357489439eb9145dbfa16bb7cd06c02c31aa4df45c77de4d2baa4e232

#这里我们只关注这个 image 的 rootfs，
#从 diff_ids 里可以看出 ubuntu:latest 这个 image 包含了 5 个 layer，
#从上到下依次是从底层到顶层，a94e0d...是最底层，db584c...是最顶层
$ cat /var/lib/docker/image/overlay2/imagedb/content/sha256/f975c50357489439eb9145dbfa16bb7cd06c02c31aa4df45c77de4d2baa4e232|jq .

......
  &amp;quot;rootfs&amp;quot;: {
    &amp;quot;type&amp;quot;: &amp;quot;layers&amp;quot;,
    &amp;quot;diff_ids&amp;quot;: [
      &amp;quot;sha256:a94e0d5a7c404d0e6fa15d8cd4010e69663bd8813b5117fbad71365a73656df9&amp;quot;,
      &amp;quot;sha256:88888b9b1b5b7bce5db41267e669e6da63ee95736cb904485f96f29be648bfda&amp;quot;,
      &amp;quot;sha256:52f389ea437ebf419d1c9754d0184b57edb45c951666ee86951d9f6afd26035e&amp;quot;,
      &amp;quot;sha256:52a7ea2bb533dc2a91614795760a67fb807561e8a588204c4858a300074c082b&amp;quot;,
      &amp;quot;sha256:db584c622b50c3b8f9b8b94c270cc5fe235e5f23ec4aacea8ce67a8c16e0fbad&amp;quot;
    ]
  }

......
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;layer-的-diff-id-和-digest-的对应关系&#34;&gt;layer 的 diff_id 和 digest 的对应关系&lt;/h3&gt;

&lt;p&gt;layer 的 &lt;code&gt;diff_id&lt;/code&gt; 存在 image 的配置文件中，而 layer 的 &lt;code&gt;digest&lt;/code&gt; 存在 image 的 manifest 中，他们的对应关系被存储在了 &lt;code&gt;image/overlay2/distribution&lt;/code&gt; 目录下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ tree -d /var/lib/docker/image/overlay2/distribution

/var/lib/docker/image/overlay2/distribution
├── diffid-by-digest
│   └── sha256
└── v2metadata-by-diffid
    └── sha256
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;diffid-by-digest：&lt;/strong&gt; 存放 &lt;code&gt;digest&lt;/code&gt; 到 &lt;code&gt;diffid&lt;/code&gt; 的对应关系&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;v2metadata-by-diffid：&lt;/strong&gt; 存放 &lt;code&gt;diffid&lt;/code&gt; 到 &lt;code&gt;digest&lt;/code&gt; 的对应关系&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#这里以最底层 layer(a94e0d...) 为例，查看其 digest 信息
$ cat /var/lib/docker/image/overlay2/distribution/v2metadata-by-diffid/sha256/db584c622b50c3b8f9b8b94c270cc5fe235e5f23ec4aacea8ce67a8c16e0fbad|jq .

[
  {
    &amp;quot;Digest&amp;quot;: &amp;quot;sha256:b78396653dae2bc0d9c02c0178bd904bb12195b2b4e541a92cd8793ac7d7d689&amp;quot;,
    &amp;quot;SourceRepository&amp;quot;: &amp;quot;docker.io/library/ubuntu&amp;quot;,
    &amp;quot;HMAC&amp;quot;: &amp;quot;&amp;quot;
  }
]

#根据 digest 得到 diffid
$ cat /var/lib/docker/image/overlay2/distribution/diffid-by-digest/sha256/b78396653dae2bc0d9c02c0178bd904bb12195b2b4e541a92cd8793ac7d7d689

sha256:db584c622b50c3b8f9b8b94c270cc5fe235e5f23ec4aacea8ce67a8c16e0fbad
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;layer-的元数据&#34;&gt;layer 的元数据&lt;/h3&gt;

&lt;p&gt;layer 的属性信息都放在了 &lt;code&gt;image/overlay2/layerdb&lt;/code&gt; 目录下，目录名称是 layer 的 &lt;code&gt;chainid&lt;/code&gt;，由于最底层的 layer 的 chainid 和 diffid 相同，所以这里我们用第二层（fe9a3f&amp;hellip;）作为示例：&lt;/p&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;计算 chainid 时，用到了所有祖先 layer 的信息，从而能保证根据 chainid 得到的 rootfs 是唯一的。比如我在 debian 和 ubuntu 的 image 基础上都添加了一个同样的文件，那么 commit 之后新增加的这两个 layer 具有相同的内容，相同的 diffid，但由于他们的父 layer 不一样，所以他们的 chainid 会不一样，从而根据 chainid 能找到唯一的 rootfs。计算 chainid 的方法请参考 &lt;a href=&#34;https://github.com/opencontainers/image-spec/blob/master/config.md&#34; target=&#34;_blank&#34;&gt;image spec&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#计算 chainid
#这里 88888b... 是第二层的 diffid，而 a94e0d... 是 88888b... 父层的 chainid，
#由于a94e0d...是最底层，它没有父层，所以 a94e0d... 的 chainid 就是 a94e0d...
$ echo -n &amp;quot;sha256:a94e0d5a7c404d0e6fa15d8cd4010e69663bd8813b5117fbad71365a73656df9 sha256:88888b9b1b5b7bce5db41267e669e6da63ee95736cb904485f96f29be648bfda&amp;quot;|sha256sum -

14a40a140881d18382e13b37588b3aa70097bb4f3fb44085bc95663bdc68fe20  -

#根据 chainid 来看看相应目录的内容
$ ll /var/lib/docker/image/overlay2/layerdb/sha256/14a40a140881d18382e13b37588b3aa70097bb4f3fb44085bc95663bdc68fe20

total 20K
-rw-r--r-- 1 root root   64 Apr  1 22:16 cache-id
-rw-r--r-- 1 root root   71 Apr  1 22:16 diff
-rw-r--r-- 1 root root   71 Apr  1 22:16 parent
-rw-r--r-- 1 root root    3 Apr  1 22:16 size
-rw-r--r-- 1 root root 1.5K Apr  1 22:16 tar-split.json.gz

#每个 layer 都有这样一个对应的文件夹
#cache-id 是 docker 下载 layer 的时候在本地生成的一个随机 uuid，
#指向真正存放 layer 文件的地方
$ cat /var/lib/docker/image/overlay2/layerdb/sha256/14a40a140881d18382e13b37588b3aa70097bb4f3fb44085bc95663bdc68fe20/cache-id

658299560be0fd7eaf4a14b0927e134049d13eb31070a9902b0d275836a13cfb

#diff 文件存放 layer 的 diffid
$ cat /var/lib/docker/image/overlay2/layerdb/sha256/14a40a140881d18382e13b37588b3aa70097bb4f3fb44085bc95663bdc68fe20/diff

sha256:88888b9b1b5b7bce5db41267e669e6da63ee95736cb904485f96f29be648bfda

#parent 文件存放当前 layer 的父 layer 的 diffid，
#注意：对于最底层的 layer 来说，由于没有父 layer，所以没有这个文件
$ cat /var/lib/docker/image/overlay2/layerdb/sha256/14a40a140881d18382e13b37588b3aa70097bb4f3fb44085bc95663bdc68fe20/parent

sha256:a94e0d5a7c404d0e6fa15d8cd4010e69663bd8813b5117fbad71365a73656df9

#当前 layer 的大小，单位是字节
$ cat /var/lib/docker/image/overlay2/layerdb/sha256/14a40a140881d18382e13b37588b3aa70097bb4f3fb44085bc95663bdc68fe20/size

745

#tar-split.json.gz，layer 压缩包的 split 文件，通过这个文件可以还原 layer 的 tar 包，
#在 docker save 导出 image 的时候会用到
#详情可参考 https://github.com/vbatts/tar-split
$ ll /var/lib/docker/image/overlay2/layerdb/sha256/14a40a140881d18382e13b37588b3aa70097bb4f3fb44085bc95663bdc68fe20/tar-split.json.gz

-rw-r--r-- 1 root root 1.5K Apr  1 22:16 /var/lib/docker/image/overlay2/layerdb/sha256/14a40a140881d18382e13b37588b3aa70097bb4f3fb44085bc95663bdc68fe20/tar-split.json.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-id-h2-5-layer数据-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;5. layer数据&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;以 &lt;code&gt;CentOS&lt;/code&gt; 为例，所有 layer 的文件都放在了 &lt;code&gt;/var/lib/docker/overlay2&lt;/code&gt; 目录下。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ tree -d -L 2 /var/lib/docker/overlay2

├── 658299560be0fd7eaf4a14b0927e134049d13eb31070a9902b0d275836a13cfb
│   ├── diff
│   └── work
├── 66ce99b5da081f65afea7ebaf612229179b620dc728b7407adcb44a51a27ae24
│   ├── diff
│   └── work
...
└── l
    ├── DYWQJVCIPQ2P2VFWZ4KBCV2JFW -&amp;gt; ../658299560be0fd7eaf4a14b0927e134049d13eb31070a9902b0d275836a13cfb/diff
    ├── 27O3MGWIL6SIN7K4GLVU4DLPSQ -&amp;gt; ../9028bae38f520a09220f67fbcf698aae2326c8318390a1d6005457d51ad97369/diff
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;”l“&lt;/code&gt; 目录包含一些符号链接作为缩短的层标识符. 这些缩短的标识符用来避免挂载时超出页面大小的限制。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ll /var/lib/docker/overlay2/l/

total 0
lrwxrwxrwx 1 root root 72 Mar 29 06:25 27O3MGWIL6SIN7K4GLVU4DLPSQ -&amp;gt; ../9028bae38f520a09220f67fbcf698aae2326c8318390a1d6005457d51ad97369/diff/
lrwxrwxrwx 1 root root 72 Mar 21 00:55 2AYPFAXSXLNCCEA6WRFCAPFPX3 -&amp;gt; ../23eb8415aec245a0291cca62d2da322de241263b8cbdfc690c0a77b353530b10/diff/
lrwxrwxrwx 1 root root 72 Mar 29 02:55 2H2XLZTCOYYSDT3XU2BDJRC2SB -&amp;gt; ../6b27128471bdfc742696ff9820bdfcdda73020753c26efeecea29b98096f0c5d/diff/
lrwxrwxrwx 1 root root 77 Mar 29 05:44 2JQ3OQVJBRYD75J4WTG4CSWA4Z -&amp;gt; ../641300d147b30f162167fed340cebcaae25f46db608939f6af09dbdb7078dcd4-init/diff/
lrwxrwxrwx 1 root root 72 Mar 21 00:55 2TCUOOM7Y7HMGIERRS4CX4YHVA -&amp;gt; ../cd3a3bd11269dc846ee9f79fca86c05336b8dd475d5ca8151991dc5d9fd7261f/diff/
lrwxrwxrwx 1 root root 77 Mar 29 06:24 36WQQRTYLT4P3J7DYLQAUMUPJE -&amp;gt; ../7ee9cc176abeb603ab0461650edd87890d167c579011813d0e864b7524f9fe24-init/diff/
...
&lt;/code&gt;&lt;/pre&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;注意：由于 docker 所采用的文件系统不同，&lt;code&gt;/var/lib/docker/&lt;storage-driver&gt;&lt;/code&gt; 目录下的目录结构及组织方式也会不一样，要具体文件系统具体分析，本文只介绍 overlay2 这种情况。
关于 aufs 和 btrfs 的相关特性可以参考 &lt;a href=&#34;https://segmentfault.com/a/1190000008489207&#34; target=&#34;_blank&#34;&gt;Linux 文件系统之 aufs&lt;/a&gt; 和 &lt;a href=&#34;https://segmentfault.com/a/1190000008605135&#34; target=&#34;_blank&#34;&gt;Btrfs 文件系统之 subvolume 与 snapshot&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;还是以刚才的第二层 layer（88888b&amp;hellip;）为例，看看实际的数据：&lt;/p&gt;

&lt;p&gt;最底层包含 &lt;code&gt;link&lt;/code&gt; 文件(不包含 lower 文件，因为是最底层)，在上面的结果中 &lt;code&gt;a94e0d...&lt;/code&gt; 为最底层。 这个文件记录着作为标识符的更短的符号链接的名字、最底层还有一个 &lt;code&gt;diff&lt;/code&gt; 目录(包含实际内容)。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 查看底层 a94e0d... 的 layer 存放的地方
$ cat /var/lib/docker/image/overlay2/layerdb/sha256/a94e0d5a7c404d0e6fa15d8cd4010e69663bd8813b5117fbad71365a73656df9/cache-id

8feee71ff338d03a22ef090f8e5a49771ca8c1f418db345782ff0fb9b9fff3ce

$ ll /var/lib/docker/overlay2/8feee71ff338d03a22ef090f8e5a49771ca8c1f418db345782ff0fb9b9fff3ce/

total 4.0K
drwxr-xr-x 21 root root 224 Apr  1 22:16 diff/
-rw-r--r--  1 root root  26 Apr  1 22:15 link

$ cat  /var/lib/docker/overlay2/8feee71ff338d03a22ef090f8e5a49771ca8c1f418db345782ff0fb9b9fff3ce/link

3GQZNQYZNRAXT6X453L5O73Y5U

$ ll /var/lib/docker/overlay2/l/|grep 3GQZNQYZNRAXT6X453L5O73Y5U

lrwxrwxrwx 1 root root 72 Apr  1 22:15 3GQZNQYZNRAXT6X453L5O73Y5U -&amp;gt; ../8feee71ff338d03a22ef090f8e5a49771ca8c1f418db345782ff0fb9b9fff3ce/diff/

# diff 目录下面是层的内容
$ ll /var/lib/docker/overlay2/8feee71ff338d03a22ef090f8e5a49771ca8c1f418db345782ff0fb9b9fff3ce/diff/

total 16K
drwxr-xr-x  2 root root 4.0K Feb 28 14:14 bin/
drwxr-xr-x  2 root root    6 Apr 12  2016 boot/
drwxr-xr-x  4 root root 4.0K Feb 28 14:14 dev/
drwxr-xr-x 42 root root 4.0K Feb 28 14:14 etc/
drwxr-xr-x  2 root root    6 Apr 12  2016 home/
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从第二层开始，每层镜像层包含 &lt;code&gt;lower&lt;/code&gt; 文件，该文件的内容表示父层镜像的符号链接，根据这个文件可以索引构建出整个镜像的层次结构。同时还包含 &lt;code&gt;merged&lt;/code&gt; 和 &lt;code&gt;work&lt;/code&gt; 目录。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每当启动一个容器时，会将 &lt;code&gt;link&lt;/code&gt; 指向的镜像层目录以及 &lt;code&gt;lower&lt;/code&gt; 指向的镜像层目录联合挂载到 &lt;code&gt;merged&lt;/code&gt; 目录，因此，容器内的视角就是 &lt;code&gt;merged&lt;/code&gt; 目录下的内容。&lt;/li&gt;
&lt;li&gt;而 work 目录则是用来完成如 &lt;code&gt;copy-on_write&lt;/code&gt; 的操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ tree -L 1 /var/lib/docker/overlay2/658299560be0fd7eaf4a14b0927e134049d13eb31070a9902b0d275836a13cfb

/var/lib/docker/overlay2/658299560be0fd7eaf4a14b0927e134049d13eb31070a9902b0d275836a13cfb
├── diff
├── link
├── lower
└── work

# 父层镜像的符号链接
$ cat /var/lib/docker/overlay2/658299560be0fd7eaf4a14b0927e134049d13eb31070a9902b0d275836a13cfb/lower

l/3GQZNQYZNRAXT6X453L5O73Y5U

$ ll /var/lib/docker/overlay2/658299560be0fd7eaf4a14b0927e134049d13eb31070a9902b0d275836a13cfb/diff

total 0
drwxr-xr-x 4 root root 29 Mar  6 17:17 etc/
drwxr-xr-x 2 root root 21 Mar  6 17:17 sbin/
drwxr-xr-x 3 root root 18 Feb 28 14:13 usr/
drwxr-xr-x 3 root root 17 Feb 28 14:14 var/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-id-h2-6-manifest文件去哪了-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;6. manifest文件去哪了？&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;从前面介绍 docker pull 的过程中得知，docker 是先得到 &lt;code&gt;manifest&lt;/code&gt;，然后根据 manifest 得到 &lt;code&gt;config&lt;/code&gt; 文件和 &lt;code&gt;layer&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;前面已经介绍了 config 文件和 layer 的存储位置，但唯独不见 manifest，去哪了呢？&lt;/p&gt;

&lt;p&gt;manifest 里面包含的内容就是对 config 和 layer 的 &lt;code&gt;sha256 + media type&lt;/code&gt; 描述，目的就是为了下载 config 和 layer，等 image 下载完成后，manifest 的使命就完成了，里面的信息对于 image 的本地管理来说没什么用，所以 docker 在本地没有单独的存储一份 manifest 文件与之对应。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-7-结束语-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;7. 结束语&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;本篇介绍了image在本地的存储方式，包括了 &lt;code&gt;/var/lib/docker/image&lt;/code&gt; 和 &lt;code&gt;/var/lib/docker/overlay2&lt;/code&gt; 这两个目录，但 /var/lib/docker/image 下面有两个目录没有涉及：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;/var/lib/docker/image/overlay2/imagedb/metadata&lt;/code&gt;：里面存放的是本地 image 的一些信息，从服务器上 pull 下来的 image 不会存数据到这个目录，下次有机会再补充这部分内容。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;/var/lib/docker/image/overlay2/layerdb/mounts&lt;/code&gt;: 创建 container 时，docker 会为每个 container 在 image 的基础上创建一层新的 layer，里面主要包含 /etc/hosts、/etc/hostname、/etc/resolv.conf 等文件，创建的这一层 layer 信息就放在这里，后续在介绍容器的时候，会专门介绍这个目录的内容。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;p-id-h2-8-参考-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;8. 参考&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/moby/moby&#34; target=&#34;_blank&#34;&gt;docker源代码&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes 使用集群联邦实现多集群管理</title>
      <link>https://www.yangcs.net/posts/federation/</link>
      <pubDate>Thu, 22 Mar 2018 09:36:27 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/federation/</guid>
      <description>&lt;p&gt;
在云计算环境中，服务的作用距离范围从近到远一般可以有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;同主机（Host，Node）&lt;/li&gt;
&lt;li&gt;跨主机同可用区（Available Zone）&lt;/li&gt;
&lt;li&gt;跨可用区同地区（Region）&lt;/li&gt;
&lt;li&gt;跨地区同服务商（Cloud Service Provider）&lt;/li&gt;
&lt;li&gt;跨云平台&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;K8s 的设计定位是单一集群在同一个地域内，因为同一个地区的网络性能才能满足 K8s 的调度和计算存储连接要求。&lt;/p&gt;

&lt;p&gt;但是实际情况中经常遇到的一些问题，就是单个集群通常无法跨单个云厂商的多个 Region，更不用说支持跨跨域不同的云厂商。这样会给企业带来一些担忧，如何应对可用区级别的 Fail，以及容灾备份？是否会造成厂商锁定，增加迁移成本？如何应对线上线下突发流量？如何统一管理调度容器资源？单个集群规模的上限等等。&lt;/p&gt;

&lt;p&gt;集群联邦（Federation）可以一定程度上解决这些问题。&lt;code&gt;Federation&lt;/code&gt; 是可以将分布在多个 Region 或者多个云厂商的 Kubernetes 集群整合成一个大的集群，统一管理与调度。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-kubernetes集群联邦介绍-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. Kubernetes集群联邦介绍&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;管理多个-kuberntes-集群&#34;&gt;管理多个 kuberntes 集群&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;集群联邦&lt;/strong&gt;在架构上同 kubernetes 集群很相似。有一个&lt;strong&gt;集群联邦&lt;/strong&gt;的 API server 提供一个标准的 Kubernetes API，并且通过 etcd 来存储状态。不同的是，一个通常的Kubernetes 只是管理节点计算，而&lt;strong&gt;集群联邦&lt;/strong&gt;管理所有的 kubernetes 集群。&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/federation-api-4x.png&#34; alt=&#34;&#34; /&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Federation主要包括三个组件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;federation-apiserver：&lt;/strong&gt;类似 &lt;code&gt;kube-apiserver&lt;/code&gt;，但提供的是跨集群的 REST API&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;federation-controller-manager：&lt;/strong&gt;类似 &lt;code&gt;kube-controller-manager&lt;/code&gt;，但提供多集群状态的同步机制&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;kubefed：&lt;/strong&gt;Federation 管理命令行工具&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;用户可以通过 Federation 的 API Server 注册该 Federation 的成员 &lt;code&gt;K8s Cluster&lt;/code&gt;。当用户通过 Federation 的 API Server 创建、更改 API 对象时，&lt;code&gt;Federation API Server&lt;/code&gt; 会在自己所有注册的子 K8s Cluster 都创建一份对应的 API 对象。&lt;/p&gt;

&lt;p&gt;在提供业务请求服务时，&lt;code&gt;K8s Federation&lt;/code&gt; 会先在自己的各个子 Cluster 之间做负载均衡，而对于发送到某个具体 &lt;code&gt;K8s Cluster&lt;/code&gt; 的业务请求，会依照这个 K8s Cluster 独立提供服务时一样的调度模式去做 K8s Cluster 内部的负载均衡。而Cluster 之间的负载均衡是通过域名服务的负载均衡来实现的。&lt;/p&gt;

&lt;p&gt;所有的设计都尽量不影响 K8s Cluster 现有的工作机制，这样对于每个子 K8s 集群来说，并不需要更外层的有一个 K8s Federation，也就是意味着所有现有的 K8s 代码和机制不需要因为 &lt;code&gt;Federation&lt;/code&gt; 功能有任何变化。&lt;/p&gt;

&lt;h3 id=&#34;跨集群服务发现&#34;&gt;跨集群服务发现&lt;/h3&gt;

&lt;p&gt;Kubernetes 有一个标准的插件：&lt;code&gt;kube-dns&lt;/code&gt;，这个插件可以在集群内部提供 DNS 服务，通过 DNS 解析 service 名字来访问 kubernetes 服务。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes 服务是由一组 kubernetes POD 组成的，这些 POD 是一些已经容器化了的应用，这些 POD 前面使用到了负载均衡器。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;假如我们有一个 kubernetes 集群，这个集群里面有一个服务叫做 mysql，这个服务是由一组 mysql POD 组成的。在这个 kubernetes 集群中，其他应用可以通过 DNS 来访问这个 mysql 服务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/federation-dns.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;跨集群调度&#34;&gt;跨集群调度&lt;/h3&gt;

&lt;p id=&#34;div-border-top-red&#34;&gt;为了追求高可用性和更高的性能，集群联邦能够把不同 POD 指定给不同的 Kubernetes 集群中。集群联邦调度器将决定如何在不同 kubernetes 集群中分配工作负载。&lt;/p&gt;

&lt;p&gt;通过跨集群调度，我们可以：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;跨 kubernetes 集群均匀的调度任务负载&lt;/li&gt;
&lt;li&gt;将各个 kubernetes 集群的工作负载进行最大化，如果当前 kubernetes 集群超出了承受能力，那么将额外的工作负载路由到另一个比较空闲的 kubernetes 集群中&lt;/li&gt;
&lt;li&gt;根据应用地理区域需求，调度工作负载到不同的 kubernetes 集群中，对于不同的终端用户，提供更高的带宽和更低的延迟。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;集群高可用-故障自动迁移&#34;&gt;集群高可用，故障自动迁移&lt;/h3&gt;

&lt;p&gt;集群联邦可以跨集群冗馀部署，当某个集群所在区域出现故障时，并不影响整个服务。集群联邦还可以检测集群是否为不可用状态，如果发现某个集群为不可用状态时，可以将失败的任务重新分配给集群联邦中其他可用状态的集群上。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-2-使用集群联邦实现多集群管理-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. 使用集群联邦实现多集群管理&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;系统环境&#34;&gt;系统环境&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;功能组件&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;系统组件&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;系统版本&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;设备数量&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;联邦集群控制平面&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;k8s 1.9+Federation&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CentOS 7.3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3台&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;联邦集群控制平面&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;K8s集群01&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;k8s 1.9 master+node&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CentOS 7.3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3台&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;联邦集群节点&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;安装-kubefed&#34;&gt;安装 kubefed&lt;/h3&gt;

&lt;p&gt;选择其中的一个集群作为主集群，这个主集群将运行组成联邦控制面板的所有组件。&lt;/p&gt;

&lt;p&gt;使用下列命令下载对应最新发行的 &lt;code&gt;kubefed&lt;/code&gt; 安装包并将安装包里的二进制文件解压出来：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl -LO https://storage.cloud.google.com/kubernetes-federation-release/release/${RELEASE-VERSION}/federation-client-linux-amd64.tar.gz
$ tar -xzvf federation-client-linux-amd64.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;请用&lt;a href=&#34;https://github.com/kubernetes/federation/releases&#34; target=&#34;_blank&#34;&gt;federation release page&lt;/a&gt;页面实际的版本号替换变量 &lt;code&gt;RELEASE-VERSION&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;将解压出来的内容复制到你的环境变量 &lt;code&gt;$PATH&lt;/code&gt; 里的随便一个路径， 并设置可执行权限。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cp federation/client/bin/kubefed /usr/local/bin
$ chmod +x /usr/local/bin/kubefed
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;配置-context&#34;&gt;配置 context&lt;/h3&gt;

&lt;p&gt;在准备配置联邦集群的 DCE 集群中配置两个 DCE 集群的 &lt;code&gt;context&lt;/code&gt;。让改节点能通过切换 &lt;code&gt;context&lt;/code&gt; 连接不同的子集群。&lt;/p&gt;

&lt;p&gt;先创建本地集群的 &lt;code&gt;kubeconfig&lt;/code&gt; 文件&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export KUBE_APISERVER=&amp;quot;https://192.168.123.250:6443&amp;quot;
# 设置集群参数
$ kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER}
# 设置客户端认证参数
$ kubectl config set-credentials admin \
  --client-certificate=/etc/kubernetes/ssl/admin.pem \
  --embed-certs=true \
  --client-key=/etc/kubernetes/ssl/admin-key.pem
# 设置上下文参数
$ kubectl config set-context kubernetes \
  --cluster=kubernetes \
  --user=admin
# 设置默认上下文
$ kubectl config use-context kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生成的 kubeconfig 被保存到 &lt;code&gt;~/.kube/config&lt;/code&gt; 文件。&lt;/p&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;~/.kube/config 文件拥有对该集群的最高权限，请妥善保管。&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;配置结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl config get-contexts

CURRENT   NAME          CLUSTER       AUTHINFO      NAMESPACE
*         kubernetes    kubernetes    admin
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;设置-coredns-作为集群联邦的-dns-提供商&#34;&gt;设置 CoreDNS 作为集群联邦的 DNS 提供商&lt;/h3&gt;

&lt;h4 id=&#34;1-前提&#34;&gt;1. 前提&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;为启用 &lt;code&gt;CoreDNS&lt;/code&gt; 来实现跨联邦集群的服务发现，联邦的成员集群中必须支持 &lt;code&gt;LoadBalancer&lt;/code&gt; 服务。（&lt;font color=&#34;red&#34;&gt;本地集群默认不支持 &lt;code&gt;LoadBalancer&lt;/code&gt; 服务，所以要让本地集群支持 &lt;code&gt;LoadBalancer&lt;/code&gt; 服务才能使用 &lt;code&gt;coredns&lt;/code&gt; 来实现 federation 的服务发现功能！！！&lt;/font&gt;）&lt;/li&gt;
&lt;li&gt;我们可以利用 &lt;code&gt;helm charts&lt;/code&gt; 来部署 CoreDNS。 CoreDNS 部署时会以 etcd 作为后端，并且 etcd 应预先安装。 etcd 也可以利用 helm charts 进行部署。&lt;/li&gt;
&lt;li&gt;所有加入 federation 的集群的 node 必须打上以下的标签：&lt;br /&gt;
&lt;code&gt;failure-domain.beta.kubernetes.io/region=&amp;lt;region&amp;gt;&lt;/code&gt;&lt;br /&gt;
&lt;code&gt;failure-domain.beta.kubernetes.io/zone=&amp;lt;zone&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-使本地集群支持-loadbalancer-服务&#34;&gt;2. 使本地集群支持 LoadBalancer 服务&lt;/h4&gt;

&lt;p&gt;为了使本地集群支持 &lt;code&gt;LoadBalancer&lt;/code&gt; 服务，可以参考以下两种实现方案：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/munnerz/keepalived-cloud-provider&#34; target=&#34;_blank&#34;&gt;keepalived-cloud-provider&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/google/metallb&#34; target=&#34;_blank&#34;&gt;metalLB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里我们选择使用 &lt;code&gt;metalLB&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;metalLB 的部署很简单，直接使用 yaml 文件部署：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.5.0/manifests/metallb.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体参考 &lt;a href=&#34;https://metallb.universe.tf/installation/&#34; target=&#34;_blank&#34;&gt;https://metallb.universe.tf/installation/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;部署完成后需要为 LoadBalancer 服务选择一个特定的 IP 地址池，这里通过 configmap 来创建。&lt;/p&gt;

&lt;p&gt;下面是一个简单示例：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat metallb-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 192.168.1.58-192.168.1.60

$ kubectl create -f metallb-cm.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多高级配置请参考：&lt;a href=&#34;https://metallb.universe.tf/configuration/&#34; target=&#34;_blank&#34;&gt;https://metallb.universe.tf/configuration/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;现在本地集群已经支持 LoadBalancer 服务了，下面我们开始 federation 的旅程吧！👏&lt;/p&gt;

&lt;h4 id=&#34;3-安装-helm&#34;&gt;3. 安装 helm&lt;/h4&gt;

&lt;p&gt;首先需要安装 &lt;code&gt;helm&lt;/code&gt; 客户端&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get &amp;gt; get_helm.sh
$ chmod 700 get_helm.sh
$ ./get_helm.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;创建 tiller 的 &lt;code&gt;serviceaccount&lt;/code&gt; 和 &lt;code&gt;clusterrolebinding&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl create serviceaccount --namespace kube-system tiller
$ kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后安装 helm 服务端 &lt;code&gt;tiller&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm init
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为应用程序设置 &lt;code&gt;serviceAccount&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl patch deploy --namespace kube-system tiller-deploy -p &#39;{&amp;quot;spec&amp;quot;:{&amp;quot;template&amp;quot;:{&amp;quot;spec&amp;quot;:{&amp;quot;serviceAccount&amp;quot;:&amp;quot;tiller&amp;quot;}}}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查是否安装成功：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n kube-system get pods|grep tiller

tiller-deploy-7bf964fff8-sklts                1/1       Running   0          7h

$ helm version

Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.8.2&amp;quot;, GitCommit:&amp;quot;a80231648a1473929271764b920a8e346f6de844&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
Server: &amp;amp;version.Version{SemVer:&amp;quot;v2.8.2&amp;quot;, GitCommit:&amp;quot;a80231648a1473929271764b920a8e346f6de844&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&#34;4-部署-etcd&#34;&gt;4. 部署 etcd&lt;/h4&gt;

&lt;p&gt;下载 &lt;code&gt;helm charts&lt;/code&gt; 仓库&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/kubernetes/charts.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;部署 &lt;code&gt;etcd-operator&lt;/code&gt;（etcd-operator 会通过 kubernetes 的 &lt;code&gt;CustomResourceDefinition&lt;/code&gt; 自动创建 &lt;code&gt;etcd cluster&lt;/code&gt;）&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd charts

$ helm install --name etcd-operator stable/etcd-operator --set rbac.install=true,rbac.apiVersion=v1,customResources.createEtcdClusterCRD=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查是否部署成功&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get pods

NAME                                                              READY     STATUS    RESTARTS   AGE
etcd-cluster-6skfqj9mwp                                           1/1       Running   0          7m
etcd-cluster-6w8ntzvkwm                                           1/1       Running   0          8m
etcd-cluster-mclzhqrldf                                           1/1       Running   0          7m
etcd-operator-etcd-operator-etcd-backup-operator-5df985959bvvkw   1/1       Running   0          9m
etcd-operator-etcd-operator-etcd-operator-58d98b95c-x44bz         1/1       Running   0          9m
etcd-operator-etcd-operator-etcd-restore-operator-8688c7684nmdh   1/1       Running   0          9m

$ kubectl get crd

NAME                                    AGE
etcdbackups.etcd.database.coreos.com    1d
etcdclusters.etcd.database.coreos.com   1d
etcdrestores.etcd.database.coreos.com   1d

$ kubectl get crd etcdclusters.etcd.database.coreos.com -o yaml

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
...
...
spec:
  group: etcd.database.coreos.com
  names:
    kind: EtcdCluster
    listKind: EtcdClusterList
    plural: etcdclusters
    shortNames:
    - etcd
    singular: etcdcluster
  scope: Namespaced
  version: v1beta2
...
...

$ kubectl get EtcdCluster

NAME           AGE
etcd-cluster   11m

$ kubectl get svc

NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
etcd-cluster            ClusterIP   None             &amp;lt;none&amp;gt;        2379/TCP,2380/TCP   17m
etcd-cluster-client     ClusterIP   10.254.140.7     &amp;lt;none&amp;gt;        2379/TCP            17m
etcd-restore-operator   ClusterIP   10.254.177.113   &amp;lt;none&amp;gt;        19999/TCP           18m
kubernetes              ClusterIP   10.254.0.1       &amp;lt;none&amp;gt;        443/TCP             16d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;部署成功后，可以在 host 集群内通过 &lt;a href=&#34;http://etcd-cluster-client.default:2379&#34; target=&#34;_blank&#34;&gt;http://etcd-cluster-client.default:2379&lt;/a&gt; 端点访问 etcd。&lt;/p&gt;

&lt;h4 id=&#34;5-部署-coredns&#34;&gt;5. 部署 CoreDNS&lt;/h4&gt;

&lt;p&gt;首先需要定制 &lt;code&gt;CoreDNS chart&lt;/code&gt; 模板的默认配置，它会覆盖 &lt;code&gt;CoreDNS chart&lt;/code&gt; 的默认配置参数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat Value.yaml

isClusterService: false
serviceType: &amp;quot;NodePort&amp;quot;
plugins:
  kubernetes:
    enabled: false
  etcd:
    enabled: true
    zones:
    - &amp;quot;example.com.&amp;quot;
    endpoint: &amp;quot;http://etcd-cluster-client.default:2379&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;isClusterService&lt;/code&gt;: 指定 CoreDNS 是否以集群服务的形式部署（默认为是）。 需要将其设置为 “false”，以使 CoreDNS 以 Kubernetes 应用服务的形式部署，否则会与集群的 dns 服务 kubedns 冲突。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;serviceType&lt;/code&gt;: 指定为 CoreDNS 创建的 Kubernetes 服务类型。 选择 “NodePort”，以使得 CoreDNS 服务能够从 Kubernetes 集群外部访问。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;plugins.kubernetes&lt;/code&gt;: 默认是启用的，通过将 &lt;code&gt;plugins.kubernetes.enabled&lt;/code&gt; 设置为 “false” 来禁用 plugins.kubernetes。&lt;/li&gt;
&lt;li&gt;通过将 plugins.etcd.enabled 设置为 “true” 来启用 plugins.etcd。&lt;/li&gt;
&lt;li&gt;通过设置 &lt;code&gt;plugins.etcd.zones&lt;/code&gt; 来配置 CoreDNS 被授权的 DNS 区域（联邦区域）。&lt;/li&gt;
&lt;li&gt;通过设置 &lt;code&gt;plugins.etcd.endpoint&lt;/code&gt; 来设置先前部署的 etcd 的端点。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;现在运行以下命令来部署 CoreDNS：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install --name coredns -f Values.yaml stable/coredns
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;验证部署：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get pods -l app=coredns-coredns

NAME                               READY     STATUS    RESTARTS   AGE
coredns-coredns-57b54ddb97-xffnc   1/1       Running   0          1d

$ kubectl get svc -l app=coredns-coredns

NAME              TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                                    AGE
coredns-coredns   NodePort   10.254.198.211   &amp;lt;none&amp;gt;        53:27165/UDP,53:27165/TCP,9153:26492/TCP   1d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&#34;6-使用-coredns-作为-dns-提供商来部署-federation&#34;&gt;6. 使用 CoreDNS 作为 DNS 提供商来部署 Federation&lt;/h4&gt;

&lt;p&gt;可以使用 &lt;code&gt;kubefed init&lt;/code&gt; 来部署联邦控制平面。 可以通过指定两个附加参数来选择 CoreDNS 作为 DNS 提供商。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--dns-provider=coredns
--dns-provider-config=coredns-provider.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;coredns-provider.conf 内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[Global]
etcd-endpoints = http://etcd-cluster-client.default:2379
zones = example.com.
coredns-endpoints = &amp;lt;coredns-server-ip&amp;gt;:&amp;lt;port&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;etcd-endpoints&lt;/code&gt; 是访问 etcd 的端点。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;zones&lt;/code&gt; 是 CoreDNS 被授权的联邦区域，其值与 &lt;code&gt;kubefed init&lt;/code&gt; 的 –-dns-zone-name 参数相同。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;coredns-endpoints&lt;/code&gt; 是访问 CoreDNS 服务器的端点。 这是一个 1.7 版本开始引入的可选参数。&lt;/li&gt;
&lt;/ul&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;CoreDNS 配置中的 &lt;code&gt;plugins.etcd.zones&lt;/code&gt; 与 kubefed init 的 –-dns-zone-name 参数应匹配。&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;给所有 node 打上 &lt;code&gt;region&lt;/code&gt; 和 &lt;code&gt;zone&lt;/code&gt; 的标签：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl label nodes 192.168.123.248 failure-domain.beta.kubernetes.io/zone=shanghai failure-domain.beta.kubernetes.io/region=yangpu

$ kubectl label nodes 192.168.123.249 failure-domain.beta.kubernetes.io/zone=shanghai failure-domain.beta.kubernetes.io/region=yangpu

$ kubectl label nodes 192.168.123.250 failure-domain.beta.kubernetes.io/zone=shanghai failure-domain.beta.kubernetes.io/region=yangpu
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过本条命令初始化 federation 控制平面，参数如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubefed init federation \ # 联邦的名字
  --host-cluster-context=kubernetes \ # 主集群的context名字
  --dns-provider=coredns \ # DNS服务提供商
  --dns-zone-name=&amp;quot;example.com.&amp;quot; \ # 前面注册好的域名，必须以.结束
  --dns-provider-config=&amp;quot;coredns-provider.conf&amp;quot; \ # coredns 配置文件
  --api-server-service-type=&amp;quot;NodePort&amp;quot; \
  --api-server-advertise-address=&amp;quot;192.168.123.250&amp;quot;

  Creating a namespace federation-system for federation system components... done
  Creating federation control plane service..... done
  Creating federation control plane objects (credentials, persistent volume claim)... done
  Creating federation component deployments... done
  Updating kubeconfig... done
  Waiting for federation control plane to come up..................................................................................................................................................... done
  Federation API server is running at: 10.110.151.216
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;观察以上输出信息，该命令做了以下几件事情：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;创建一个 namespace &lt;code&gt;federation-system&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get ns

NAME                STATUS    AGE
default             Active    8d
federation-system   Active    8s
kube-public         Active    8d
kube-system         Active    8d
my-namespace        Active    7d
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建两个服务 &lt;code&gt;federation-apiserver&lt;/code&gt; 和 &lt;code&gt;federation-controller-manager&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n federation-system get pods

NAME                                             READY     STATUS         RESTARTS   AGE
federation-apiserver-909415585-wktmw             1/1       Running   0          2s
federation-controller-manager-4247980660-c8ls5   1/1       Running   1          3s
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建一个 ServiceAccount &lt;code&gt;federation-controller-manager&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n federation-system get sa

NAME                            SECRETS   AGE
default                         1         31m
federation-controller-manager   1         31m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建一个 Role &lt;code&gt;federation-system:federation-controller-manager&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n federation-system get role

NAME                                              AGE
federation-system:federation-controller-manager   38m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建一个 RoleBinding &lt;code&gt;federation-system:federation-controller-manager&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n federation-system get rolebinding

NAME                                              AGE
federation-system:federation-controller-manager   39m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建一个 context &lt;code&gt;federation&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    $ kubectl config get-contexts

    CURRENT   NAME         CLUSTER      AUTHINFO     NAMESPACE
              federation   federation   federation
    *         kubernetes   kubernetes   admin
&lt;/code&gt;&lt;/pre&gt;

&lt;div id=&#34;note&#34;&gt;
&lt;p id=&#34;note-title&#34;&gt;Note&lt;/p&gt;
&lt;br /&gt;
&lt;p&gt;默认情况下，&lt;code&gt;kubefed init&lt;/code&gt; 通过动态创建 PV 的方式为 etcd 创建持久化存储。如果 kubernetes 集群不支持动态创建 PV，则可以预先创建 PV，注意 PV 要匹配 `kubefed` 的 PVC。或者使用 &lt;code&gt;hostpath&lt;/code&gt;，同时指定调度节点。&lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&#34;7-添加集群至-federation&#34;&gt;7. 添加集群至 federation&lt;/h4&gt;

&lt;p&gt;目前为止您已经成功的初始化好了 &lt;code&gt;Federation&lt;/code&gt; 的控制平面。接下来需要将各个子集群加入到 Federation 集群中。&lt;/p&gt;

&lt;p&gt;添加集群 &lt;code&gt;kubernetes&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubefed join kubernetes \ #加入联邦的集群命名名字
  --context=federation \ #联邦的context
  --cluster-context=kubernetes \ #要添加集群的context
  --host-cluster-context=kubernetes #主集群的context

$ kubectl --context=federation get cluster

NAME          STATUS    AGE
kubernetes    Ready     6d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过我的观察，以上过程在 &lt;code&gt;加入 federation 的 kubernetes 集群中&lt;/code&gt; 做了以下几件事情：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;在 namespace &lt;code&gt;federation-system&lt;/code&gt; 中创建一个 ServiceAccount &lt;code&gt;kubernetes-kubernetes&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl -n federation-system get sa

NAME                            SECRETS   AGE
default                         1         45m
federation-controller-manager   1         45m
kubernetes-kubernetes           1         8m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建一个 ClusterRole &lt;code&gt;federation-controller-manager:federation-kubernetes-kubernetes&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get clusterrole|egrep &amp;quot;NAME|federation&amp;quot;

NAME                                                                   AGE
federation-controller-manager:federation-kubernetes-kubernetes         10m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;创建一个 ClusterRoleBinding &lt;code&gt;federation-controller-manager:federation-kubernetes-kubernetes&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get clusterrolebinding|egrep &amp;quot;NAME|federation&amp;quot;

NAME                                                             AGE
federation-controller-manager:federation-kubernetes-kubernetes   11m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;整个过程是否还进行了其他操作，暂时没有发现，有待继续研究。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;介绍下集群查询，移除集群，删除联邦等命令：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;查询注册到 Federation 的 kubernetes 集群列表&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl --context=federation get clusters

NAME          STATUS    AGE
kubernetes    Ready     8m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;移除 &lt;code&gt;kubernetes&lt;/code&gt; 集群&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubefed unjoin kubernetes --host-cluster-context=kubernetes --context=federation

Successfully removed cluster &amp;quot;kubernetes&amp;quot; from federation

$ kubectl --context=federation get clusters

No resources found.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;集群联邦控制平面的删除功能还在开发中，目前可以通过删除 namespace &lt;code&gt;federation-system&lt;/code&gt; 的方法来清理（注意pv不会删除）。命令在 host-cluster-context 上执行。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl delete ns federation-system
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-id-h2-3-federation-支持的服务-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. Federation 支持的服务&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;集群联邦支持以下联邦资源，这些资源会自动在所有注册的 &lt;code&gt;kubernetes&lt;/code&gt; 集群中创建。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Federated ConfigMap&lt;/li&gt;
&lt;li&gt;Federated Service&lt;/li&gt;
&lt;li&gt;Federated DaemonSet&lt;/li&gt;
&lt;li&gt;Federated Deployment&lt;/li&gt;
&lt;li&gt;Federated Ingress&lt;/li&gt;
&lt;li&gt;Federated Namespaces&lt;/li&gt;
&lt;li&gt;Federated ReplicaSets&lt;/li&gt;
&lt;li&gt;Federated Secrets&lt;/li&gt;
&lt;li&gt;Federated Events（仅存在federation控制平面）&lt;/li&gt;
&lt;li&gt;Federated Jobs（v1.8+）&lt;/li&gt;
&lt;li&gt;Federated Horizontal Pod Autoscaling (HPA，v1.8+)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;示例：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;创建 &lt;code&gt;deployment&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat nginx-deployment.yaml

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80

$ kubectl --context=federation create ns default
$ kubectl --context=federation create -f nginx-deployment.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以通过 &lt;code&gt;kubectl scale deploy nginx --replicas=3 --context=federation&lt;/code&gt; 来扩展 nginx 副本，然后观察 nginx 应用在各个子集群中的分布情况。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl --context=kubernetes get deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;通过 Federated Service 来实现跨集群服务发现&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat nginx-svc.yaml

apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  selector:
    app: nginx

# 这会在所有注册到联邦的 kubernetes 集群中创建服务
$ kubectl --context=federation create -f nginx-svc.yaml

# 查看服务状态
$ kubectl --context=federation describe services nginx

Name:                     nginx
Namespace:                default
Labels:                   app=nginx
Annotations:              federation.kubernetes.io/service-ingresses={&amp;quot;items&amp;quot;:[{&amp;quot;cluster&amp;quot;:&amp;quot;kubernetes&amp;quot;,&amp;quot;items&amp;quot;:[{&amp;quot;ip&amp;quot;:&amp;quot;192.168.1.58&amp;quot;}]}]}
Selector:                 app=nginx
Type:                     LoadBalancer
IP:
LoadBalancer Ingress:     192.168.1.58
Port:                     http  80/TCP
TargetPort:               80/TCP
Endpoints:                &amp;lt;none&amp;gt;
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以通过 DNS 来访问联邦服务，访问格式包括以下几种：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;service name&amp;gt;.&amp;lt;namespace&amp;gt;.&amp;lt;federation&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;service name&amp;gt;.&amp;lt;namespace&amp;gt;.&amp;lt;federation&amp;gt;.svc.&amp;lt;domain&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;service name&amp;gt;.&amp;lt;namespace&amp;gt;.&amp;lt;federation&amp;gt;.svc.&amp;lt;region&amp;gt;.&amp;lt;domain&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;service name&amp;gt;.&amp;lt;namespace&amp;gt;.&amp;lt;federation&amp;gt;.svc.&amp;lt;zone&amp;gt;.&amp;lt;region&amp;gt;.&amp;lt;domain&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本例中可以通过以下几个域名来访问：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nginx.default.federation&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nginx.default.federation.svc.example.com&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nginx.default.federation.svc.shanghai.example.com&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nginx.default.federation.svc.shanghai.yangpu.example.com&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DNS 在 etcd 下的存储路径为：&lt;code&gt;/skydns&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl exec etcd-cluster-fznzsrttt9 etcdctl ls /skydns/com/example/

/skydns/com/example/kubernetes
/skydns/com/example/svc
/skydns/com/example/yangpu

$ kubectl exec etcd-cluster-fznzsrttt9 etcdctl ls /skydns/com/example/yangpu/

/skydns/com/example/yangpu/shanghai
/skydns/com/example/yangpu/svc
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-id-h2-4-参考文档-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. 参考文档&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/federation/&#34; target=&#34;_blank&#34;&gt;Kubernetes federation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/federation/set-up-coredns-provider-federation/&#34; target=&#34;_blank&#34;&gt;Set up CoreDNS as DNS provider for Cluster Federation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;style&gt;
#h2 {
    margin-bottom:2em;
    margin-right: 5px;
    padding: 8px 15px;
    letter-spacing: 2px;
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181));
    background-color: rgb(63, 81, 181);
    color: rgb(255, 255, 255);
    border-left: 10px solid rgb(51, 51, 51);
    border-radius:5px;
    text-shadow: rgb(102, 102, 102) 1px 1px 1px;
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#note {
    font-size: 1.5rem;
    font-style: italic;
    padding: 0 1rem;
    margin: 2.5rem 0;
    position: relative;
    background-color: #fafeff;
    border-top: 1px dotted #9954bb;
    border-bottom: 1px dotted #9954bb;
}
#note-title {
    padding: 0.2rem 0.5rem;
    background: #9954bb;
    color: #FFF;
    position: absolute;
    left: 0;
    top: 0.25rem;
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
    border-radius: 4px;
    -webkit-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -moz-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -ms-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    -o-transform: rotate(-5deg) translateX(-10px) translateY(-25px);
    transform: rotate(-5deg) translateX(-10px) translateY(-25px);
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes 网络扩展</title>
      <link>https://www.yangcs.net/posts/k8s-network-expand/</link>
      <pubDate>Sun, 11 Feb 2018 10:40:33 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/k8s-network-expand/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-1-kubernetes-中服务暴露的方式-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;1. Kubernetes 中服务暴露的方式&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;k8s 的服务暴露分为以下几种情况：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;hostNetwork&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;hostPort&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;NodePort&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;LoadBalancer&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Ingress&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;说是暴露 Pod 其实跟暴露 Service 是一回事，因为 Pod 就是 Service 的 backend。&lt;/p&gt;

&lt;h3 id=&#34;hostnetwork&#34;&gt;HostNetwork&lt;/h3&gt;

&lt;p&gt;这是一种直接定义 Pod 网络的方式。&lt;/p&gt;

&lt;p&gt;如果在 Pod 中使用 &lt;code&gt;hostNotwork:true&lt;/code&gt; 配置的话，在这种 pod 中运行的应用程序可以直接看到 pod 启动的主机的网络接口。在主机的所有网络接口上都可以访问到该应用程序。以下是使用主机网络的 pod 的示例定义：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: influxdb
spec:
  hostNetwork: true
  containers:
    - name: influxdb
      image: influxdb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这种 Pod 的网络模式有一个用处就是可以将网络插件包装在 Pod 中然后部署在每个宿主机上，这样该 Pod 就可以控制该宿主机上的所有网络。&lt;/p&gt;

&lt;p id=&#34;div-border-top-purple&#34;&gt;&lt;font color=&#34;red&#34;&gt;缺点：&lt;/font&gt;每次启动这个Pod的时候都可能被调度到不同的节点上，所有外部访问Pod的IP也是变化的，而且调度Pod的时候还需要考虑是否与宿主机上的端口冲突，因此一般情况下除非您知道需要某个特定应用占用特定宿主机上的特定端口时才使用 &lt;code&gt;hostNetwork: true&lt;/code&gt; 的方式。&lt;/p&gt;

&lt;h3 id=&#34;hostport&#34;&gt;hostPort&lt;/h3&gt;

&lt;p&gt;这是一种直接定义 Pod 网络的方式。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;hostPort&lt;/code&gt; 是直接将容器的端口与所调度的节点上的端口路由，这样用户就可以通过宿主机的 IP 加上来访问 Pod 了，如:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: influxdb
spec:
  containers:
    - name: influxdb
      image: influxdb
      ports:
        - containerPort: 8086
          hostPort: 8086
&lt;/code&gt;&lt;/pre&gt;

&lt;p id=&#34;div-border-top-purple&#34;&gt;&lt;font color=&#34;red&#34;&gt;缺点：&lt;/font&gt;因为 Pod 重新调度的时候该Pod被调度到的宿主机可能会变动，这样就变化了，用户必须自己维护一个 Pod 与所在宿主机的对应关系。&lt;/p&gt;

&lt;h3 id=&#34;nodeport&#34;&gt;NodePort&lt;/h3&gt;

&lt;p&gt;NodePort 在 kubenretes 里是一个广泛应用的服务暴露方式。Kubernetes 中的 service 默认情况下都是使用的 ClusterIP 这种类型，这样的 service 会产生一个 ClusterIP，这个 IP 只能在集群内部访问，要想让外部能够直接访问 service，需要将 service type 修改为  &lt;code&gt;nodePort&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  name: influxdb
  labels:
    name: influxdb
spec:
  containers:
    - name: influxdb
      image: influxdb
      ports:
        - containerPort: 8086
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时还可以给 service 指定一个 &lt;code&gt;nodePort&lt;/code&gt; 值，范围是 30000-32767，这个值在 API server 的配置文件中，用&amp;ndash; &lt;code&gt;service-node-port-range&lt;/code&gt; 定义。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kind: Service
apiVersion: v1
metadata:
  name: influxdb
spec:
  type: NodePort
  ports:
    - port: 8086
      nodePort: 30000
  selector:
    name: influxdb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;集群外就可以使用 kubernetes 任意一个节点的 IP 加上 30000 端口访问该服务了。kube-proxy 会自动将流量以 round-robin 的方式转发给该 service 的每一个 pod。&lt;/p&gt;

&lt;p id=&#34;div-border-top-purple&#34;&gt;&lt;font color=&#34;red&#34;&gt;缺点：&lt;/font&gt;所有 node 上都会开启端口监听，且需要记住端口号。&lt;/p&gt;

&lt;h3 id=&#34;loadbalancer&#34;&gt;LoadBalancer&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;LoadBalancer&lt;/code&gt; 只能在 service 上定义。这是公有云提供的负载均衡器，如 AWS、Azure、CloudStack、GCE 等。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kind: Service
apiVersion: v1
metadata:
  name: influxdb
spec:
  type: LoadBalancer
  ports:
    - port: 8086
  selector:
    name: influxdb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get svc influxdb

NAME       CLUSTER-IP     EXTERNAL-IP     PORT(S)          AGE
influxdb   10.97.121.42   10.13.242.236   8086:30051/TCP   39s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;内部可以使用 ClusterIP 加端口来访问服务，如 19.97.121.42:8086。&lt;/p&gt;

&lt;p&gt;外部可以用以下两种方式访问该服务：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用任一节点的 IP 加 30051 端口访问该服务&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;使用 &lt;code&gt;EXTERNAL-IP&lt;/code&gt; 来访问，这是一个 VIP，是云供应商提供的负载均衡器 IP，如 &lt;code&gt;10.13.242.236:8086&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p id=&#34;div-border-top-purple&#34;&gt;&lt;font color=&#34;red&#34;&gt;缺点：&lt;/font&gt;需要云服务商支持。&lt;/p&gt;

&lt;h3 id=&#34;ingress&#34;&gt;Ingress&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;Ingress&lt;/code&gt; 是自 kubernetes1.1 版本后引入的资源类型。必须要部署 Ingress controller 才能创建 Ingress 资源，Ingress controller 是以一种插件的形式提供。Ingress controller 是部署在 Kubernetes 之上的 Docker 容器。它的 Docker 镜像包含一个像 &lt;code&gt;nginx&lt;/code&gt; 或 &lt;code&gt;HAProxy&lt;/code&gt; 的负载均衡器和一个控制器守护进程。控制器守护程序从 Kubernetes 接收所需的 Ingress 配置。它会生成一个 nginx 或 HAProxy 配置文件，并重新启动负载平衡器进程以使更改生效。换句话说，Ingress controller 是由 Kubernetes 管理的负载均衡器。&lt;/p&gt;

&lt;p&gt;Kubernetes Ingress 提供了负载平衡器的典型特性：HTTP 路由，粘性会话，SSL 终止，SSL 直通，TCP 和 UDP 负载平衡等。目前并不是所有的 Ingress controller 都实现了这些功能，需要查看具体的 Ingress controller 文档。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: influxdb
spec:
  rules:
    - host: influxdb.kube.example.com
      http:
        paths:
          - backend:
              serviceName: influxdb
              servicePort: 8086
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;外部访问 URL  &lt;a href=&#34;http://influxdb.kube.example.com/ping&#34; target=&#34;_blank&#34;&gt;http://influxdb.kube.example.com/ping&lt;/a&gt; 访问该服务，入口就是 80 端口，然后 &lt;code&gt;Ingress controller&lt;/code&gt; 直接将流量转发给后端 Pod，不需再经过 kube-proxy 的转发，比 LoadBalancer 方式更高效。&lt;/p&gt;

&lt;p id=&#34;div-border-top-purple&#34;&gt;&lt;font color=&#34;red&#34;&gt;缺点：&lt;/font&gt;80 端口暴露 必需通过域名引入，而且一次只能一条规则，很麻烦。&lt;/p&gt;

&lt;p&gt;但是在正常的虚拟机环境下，我们只需要一个 &lt;code&gt;IP 地址+端口&lt;/code&gt; 即可访问服务。&lt;/p&gt;

&lt;p&gt;为什么我们不能做到像访问虚拟机一样直接访问 k8s 集群服务呢？当然可以，以下架构可以实现：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;打通 k8s 网络和物理网络直通&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;物理网络的 dns 域名服务直接调用 k8s-dns 域名服务直接互访&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;p-id-h2-2-集群环境-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;2. 集群环境&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;架构环境&#34;&gt;架构环境&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;k8s 集群网络：&lt;code&gt;172.28.0.0/16&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;k8s-service 网络：&lt;code&gt;10.96.0.0/12&lt;/code&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;物理机网络：&lt;code&gt;192.168.0.0/16&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;k8s-集群节点&#34;&gt;k8s 集群节点&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get cs

NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-0               Healthy   {&amp;quot;health&amp;quot;: &amp;quot;true&amp;quot;}

$ kubectl get nodes -owide

NAME      STATUS    AGE       VERSION   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION
node1     Ready     13d       v1.7.11   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-514.el7.x86_64
node2     Ready     13d       v1.7.11   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-514.el7.x86_64
node3     Ready     13d       v1.7.11   &amp;lt;none&amp;gt;        CentOS Linux 7 (Core)   3.10.0-514.el7.x86_64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;角色定义：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;角色名称&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;IP 地址&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;主机名&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;边界网关路由器&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;192.168.2.173&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;calico-gateway&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;边界 dns 代理服务器&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;192.168.1.62&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;node3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;假设我们要访问 k8s 中的 &lt;code&gt;dao-2048&lt;/code&gt; 服务：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl get svc|egrep &#39;NAME|2048&#39;

NAME                              CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
dao-2048                          10.98.217.155    &amp;lt;none&amp;gt;        80/TCP           13m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;该方案的架构原理如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;          +-----------------+
          |                 |
          |  192.168.0.0/16 |          # 物理网络以域名或tcp方式发起访问k8s service以及端口
          |                 |
          +-----------------+
                   |
                   |
+------------------------------------+
| dao-2048.default.svc.cluster.local |  # 请求k8s服务所在空间的服务名，完整域名
+------------------------------------+
                   |
                   |
          +-----------------+
          |                 |           # dns代理服务以ingress-udp pod的模式运行在此节点udp53号端口上，
          |   192.168.1.62  |           # 为物理网络提供仿问k8s-dns的桥梁解析dns
          |                 |           # 此节点应固定做为一个节点布署，所有外部机器设置dns为此 192.168.1.62
          +-----------------+
                   |
                   |
          +-----------------+
          |                 |
          |  10.98.217.155  |           # 获取 svc 的实际 clusterip
          |                 |
          +-----------------+
                   |
                   |
          +-----------------+           # 边界网关,用于物理网络连接k8s集群，需要开启内核转发：net.ipv4.ip_forward=1
          |                 |           # 所有外部物理机加一条静态路由：访问 k8s 网络 10.96.0.0/12 网段必需经过网关 192.168.2.173
          |  192.168.2.173  |           # ip route add 10.96.0.0/12 via 192.168.2.173
          |                 |           # 边界网关运行 kube-proxy 用于防火墙规则同步实现 svc 分流，此节点不运行 kubele 服务，不受 k8s 管控
          +-----------------+
                   |
                   |
         +-------------------+
         |                   |
         |  calico-Iface接口  |
         |                   |
         +-------------------+
                   |
                   |
          +-----------------+
          |   k8s 集群网络   |            # 流量最终到达 k8s 集群
          +-----------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下为该方案的实施步骤。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-3-部署边界-dns-代理服务器-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;3. 部署边界 dns 代理服务器&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;布署 dns 代理服务节点为外部提供 dns 服务,以 &lt;code&gt;hostNetwork: true&lt;/code&gt; 为非 k8s 集群网络物理机节点提供 dns 服务&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd ~/dns-udp; ll ./

total 20K
-rw-r--r--. 1 root root 1.2K Feb 12 05:13 default-backend.yaml
-rw-r--r--. 1 root root  140 Feb 12 05:14 nginx-udp-ingress-configmap.yaml
-rw-r--r--. 1 root root 1.8K Feb 12 05:35 nginx-udp-ingress-controller.yaml
-rw-r--r--. 1 root root 2.4K Feb 12 05:15 rbac.yaml

$ cat default-backend.yaml

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: default-http-backend
  labels:
    app: default-http-backend
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: default-http-backend
  template:
    metadata:
      labels:
        app: default-http-backend
    spec:
      terminationGracePeriodSeconds: 60
      containers:
      - name: default-http-backend
        # Any image is permissible as long as:
        # 1. It serves a 404 page at /
        # 2. It serves 200 on a /healthz endpoint
        image: gcr.io/google_containers/defaultbackend:1.4
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        ports:
        - containerPort: 8080
        resources:
          limits:
            cpu: 10m
            memory: 20Mi
          requests:
            cpu: 10m
            memory: 20Mi
---

apiVersion: v1
kind: Service
metadata:
  name: default-http-backend
  namespace: kube-system
  labels:
    app: default-http-backend
spec:
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: default-http-backend
    
$ cat nginx-udp-ingress-configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-udp-ingress-configmap
  namespace: kube-system
data:
  53: &amp;quot;kube-system/kube-dns:53&amp;quot;
  
$ cat nginx-udp-ingress-controller.yaml

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-udp-ingress-controller
  labels:
    k8s-app: nginx-udp-ingress-lb
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: nginx-udp-ingress-lb
  template:
    metadata:
      labels:
        k8s-app: nginx-udp-ingress-lb
        name: nginx-udp-ingress-lb
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                - node3
      hostNetwork: true
      serviceAccountName: nginx-ingress-serviceaccount
      terminationGracePeriodSeconds: 60
      containers:
      - image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2
        name: nginx-udp-ingress-lb
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          timeoutSeconds: 1
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        ports:
        - containerPort: 80
          hostPort: 80
        - containerPort: 443
          hostPort: 443
        - containerPort: 53
          hostPort: 53
        args:
        - /nginx-ingress-controller
        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap

$ cat rbac.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: kube-system

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
rules:
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - &amp;quot;extensions&amp;quot;
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
        - events
    verbs:
        - create
        - patch
  - apiGroups:
      - &amp;quot;extensions&amp;quot;
    resources:
      - ingresses/status
    verbs:
      - update

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: kube-system
rules:
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - configmaps
    resourceNames:
      # Defaults to &amp;quot;&amp;lt;election-id&amp;gt;-&amp;lt;ingress-class&amp;gt;&amp;quot;
      # Here: &amp;quot;&amp;lt;ingress-controller-leader&amp;gt;-&amp;lt;nginx&amp;gt;&amp;quot;
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - &amp;quot;ingress-controller-leader-nginx&amp;quot;
    verbs:
      - get
      - update
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - &amp;quot;&amp;quot;
    resources:
      - endpoints
    verbs:
      - get

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: kube-system

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: kube-system
    
$ kubectl create -f ./

deployment &amp;quot;default-http-backend&amp;quot; created
service &amp;quot;default-http-backend&amp;quot; created
configmap &amp;quot;nginx-udp-ingress-configmap&amp;quot; created
deployment &amp;quot;nginx-udp-ingress-controller&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过 nginx 反向代理 &lt;code&gt;kube-dns&lt;/code&gt; 服务，同时以 &lt;code&gt;hostNetwork: true&lt;/code&gt; 向集群外部暴露 53 端口，为非 k8s 集群网络物理机节点提供 dns 服务。&lt;/p&gt;

&lt;h2 id=&#34;p-id-h2-4-部署-gateway-边界网关节点-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;4. 部署 gateway 边界网关节点&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;此节点只运行 &lt;code&gt;calico&lt;/code&gt; 和 &lt;code&gt;kube-proxy&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&#34;首先开启内核转发&#34;&gt;首先开启内核转发&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ echo &#39;net.ipv4.ip_forward=1&#39; &amp;gt;&amp;gt;/etc/sysctl.conf
$ sysctl -p
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;运行-calico&#34;&gt;运行 calico&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run --net=host --privileged --name=calico-node -d --restart=always \
  -v /etc/etcd/ssl:/etc/kubernetes/ssl \
  -e ETCD_ENDPOINTS=https://192.168.1.60:12379 \
  -e ETCD_KEY_FILE=/etc/kubernetes/ssl/peer-key.pem \
  -e ETCD_CERT_FILE=/etc/kubernetes/ssl/peer-cert.pem \
  -e ETCD_CA_CERT_FILE=/etc/kubernetes/ssl/ca.pem \
  -e NODENAME=${HOSTNAME} \
  -e IP= \
  -e CALICO_IPV4POOL_CIDR=172.28.0.0/16 \
  -e NO_DEFAULT_POOLS= \
  -e AS= \
  -e CALICO_LIBNETWORK_ENABLED=true \
  -e IP6= \
  -e CALICO_NETWORKING_BACKEND=bird \
  -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \
  -v /var/run/calico:/var/run/calico \
  -v /lib/modules:/lib/modules \
  -v /run/docker/plugins:/run/docker/plugins \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v /var/log/calico:/var/log/calico \
  calico/node:v2.6.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要提前将相关证书拷贝到 &lt;code&gt;/etc/kubernetes/ssl/&lt;/code&gt; 目录下。&lt;/p&gt;

&lt;p id=&#34;div-border-top-red&#34;&gt;&lt;font color=&#34;blue&#34;&gt;注意：&lt;/font&gt;此处的 &lt;code&gt;-e CALICO_IPV4POOL_CIDR=172.28.0.0/16&lt;/code&gt; 要与 k8s 集群网络的网段一致&lt;/p&gt;

&lt;h3 id=&#34;创建边界路由器&#34;&gt;创建边界路由器&lt;/h3&gt;

&lt;p&gt;以下命令在 k8s 的 master 节点上进行操作&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat bgpPeer.yaml

apiVersion: v1
kind: bgpPeer
metadata:
  peerIP: 192.168.2.173
  scope: global
spec:
  asNumber: 64512
  
$ calicoctl  create -f bgpPeer.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看 node 情况&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ calicoctl node status

Calico process is running.

IPv4 BGP status
+---------------+-------------------+-------+------------+-------------+
| PEER ADDRESS  |     PEER TYPE     | STATE |   SINCE    |    INFO     |
+---------------+-------------------+-------+------------+-------------+
| 192.168.1.61  | node-to-node mesh | up    | 2018-01-29 | Established |
| 192.168.1.62  | node-to-node mesh | up    | 2018-02-09 | Established |
| 192.168.2.173 | node-to-node mesh | up    | 2018-02-09 | Established |
| 192.168.2.173 | global            | start | 2018-02-09 | Idle        |
+---------------+-------------------+-------+------------+-------------+

IPv6 BGP status
No IPv6 peers found.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看全局对等体节点&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ calicoctl get bgpPeer --scope=global

SCOPE    PEERIP          NODE   ASN     
global   192.168.2.173          64512
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;部署-kube-proxy&#34;&gt;部署 kube-proxy&lt;/h3&gt;

&lt;h4 id=&#34;安装-conntrack&#34;&gt;安装 conntrack&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ yum install -y conntrack-tools
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;创建-kube-proxy-的-service-配置文件&#34;&gt;创建 kube-proxy 的 service 配置文件&lt;/h4&gt;

&lt;p&gt;文件路径 &lt;code&gt;/usr/lib/systemd/system/kube-proxy.service&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/proxy
ExecStart=/usr/local/bin/kube-proxy \
        --logtostderr=true \
        --v=2 \
        --bind-address=192.168.2.173 \
        --hostname-override=192.168.2.173 \
        --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
        --proxy-mode=iptables \
        --cluster-cidr=172.28.0.0/16
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要提前将 &lt;code&gt;kube-proxy.kubeconfig&lt;/code&gt; 文件拷贝到 &lt;code&gt;/etc/kubernetes/&lt;/code&gt; 目录下。&lt;/p&gt;

&lt;h4 id=&#34;启动-kube-proxy&#34;&gt;启动 kube-proxy&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ systemctl daemon-reload
$ systemctl enable kube-proxy
$ systemctl start kube-proxy
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;p-id-h2-5-测试网关和-dns-解析以及服务访问情况-p&#34;&gt;&lt;p id=&#34;h2&#34;&gt;5. 测试网关和 dns 解析以及服务访问情况&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;找台集群外的机器来验证，这台机器只有一个网卡，没有安装 &lt;code&gt;calico&lt;/code&gt;。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;添加路由&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ip route add 10.96.0.0/12 via 192.168.2.173 dev ens160
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;修改 dns 为 &lt;code&gt;192.168.1.62&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat /etc/resolv.conf

nameserver 192.168.1.62
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 223.5.5.5
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;解析 &lt;code&gt;dao-2048&lt;/code&gt; 服务的域名&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ dig dao-2048.default.svc.cluster.local

; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.9.4-RedHat-9.9.4-51.el7_4.2 &amp;lt;&amp;lt;&amp;gt;&amp;gt; dao-2048.default.svc.cluster.local
;; global options: +cmd
;; Got answer:
;; -&amp;gt;&amp;gt;HEADER&amp;lt;&amp;lt;- opcode: QUERY, status: NOERROR, id: 57053
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;dao-2048.default.svc.cluster.local. IN A

;; ANSWER SECTION:
dao-2048.default.svc.cluster.local. 5 IN A      10.98.217.155

;; Query time: 1 msec
;; SERVER: 192.168.1.62#53(192.168.1.62)
;; WHEN: Mon Feb 12 06:46:38 EST 2018
;; MSG SIZE  rcvd: 79
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;访问 &lt;code&gt;dao-2048&lt;/code&gt; 服务&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl dao-2048.default.svc.cluster.local

* About to connect() to dao-2048.default.svc.cluster.local port 80 (#0)
*   Trying 10.98.217.155...
* Connected to dao-2048.default.svc.cluster.local (10.98.217.155) port 80 (#0)
&amp;gt; GET / HTTP/1.1
&amp;gt; User-Agent: curl/7.29.0
&amp;gt; Host: dao-2048.default.svc.cluster.local
&amp;gt; Accept: */*
&amp;gt; 
&amp;lt; HTTP/1.1 200 OK
&amp;lt; Server: nginx/1.10.1
&amp;lt; Date: Mon, 12 Feb 2018 11:47:58 GMT
&amp;lt; Content-Type: text/html
&amp;lt; Content-Length: 4085
&amp;lt; Last-Modified: Sun, 11 Feb 2018 11:31:27 GMT
&amp;lt; Connection: keep-alive
&amp;lt; ETag: &amp;quot;5a80298f-ff5&amp;quot;
&amp;lt; Accept-Ranges: bytes
&amp;lt; 
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
  &amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt;
  &amp;lt;title&amp;gt;2048&amp;lt;/title&amp;gt;
.........
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;font color=&#34;red&#34;&gt;成功访问！&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果是 windows 用户，添加路由可以用管理员打开 cmd 命令运行：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ route ADD -p 172.28.0.0 MASK 255.255.0.0 192.168.2.173
&lt;/code&gt;&lt;/pre&gt;

&lt;p id=&#34;div-border-top-red&#34;&gt;&lt;font color=&#34;blue&#34;&gt;PS：&lt;/font&gt;如果你不想一台台机器加路由和 dns，你可以把路由信息加入物理路由器上，这样就不用每台机都加路由和 dns 了，直接打通所有链路。&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.csdn.net/idea77/article/details/73863822&#34; target=&#34;_blank&#34;&gt;k8s-dns-gateway 网关网络扩展实战&lt;/a&gt;&lt;/p&gt;

&lt;style&gt;
#h2{
    margin-bottom:2em; 
    margin-right: 5px; 
    padding: 8px 15px; 
    letter-spacing: 2px; 
    background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181)); 
    background-color: rgb(63, 81, 181); 
    color: rgb(255, 255, 255); 
    border-left: 10px solid rgb(51, 51, 51); 
    border-radius:5px; 
    text-shadow: rgb(102, 102, 102) 1px 1px 1px; 
    box-shadow: rgb(102, 102, 102) 1px 1px 2px;
}
#inline-yellow {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #f0ad4e;
}
#inline-green {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #5cb85c;
}
#inline-blue {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #2780e3;
}
#inline-purple {
display:inline;
padding:.2em .6em .3em;
font-size:80%;
font-weight:bold;
line-height:1;
color:#fff;
text-align:center;
white-space:nowrap;
vertical-align:baseline;
border-radius:0;
background-color: #9954bb;
}
#div-border-left-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #df3e3e;
}
#div-border-left-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #f0ad4e;
}
#div-border-left-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #5cb85c;
}
#div-border-left-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #2780e3;
}
#div-border-left-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-left-width: 5px;
border-radius: 3px;
border-left-color: #9954bb;
}
#div-border-right-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #df3e3e;
}
#div-border-right-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #f0ad4e;
}
#div-border-right-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #5cb85c;
}
#div-border-right-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #2780e3;
}
#div-border-right-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-right-width: 5px;
border-radius: 3px;
border-right-color: #9954bb;
}
#div-border-top-red {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #df3e3e;
}
#div-border-top-yellow {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #f0ad4e;
}
#div-border-top-green {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #5cb85c;
}
#div-border-top-blue {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #2780e3;
}
#div-border-top-purple {
display: block;
padding: 10px;
margin: 10px 0;
border: 1px solid #ccc;
border-top-width: 5px;
border-radius: 3px;
border-top-color: #9954bb;
}
&lt;/style&gt;</description>
    </item>
    
    <item>
      <title>calico Router reflection(RR) 模式介绍及部署</title>
      <link>https://www.yangcs.net/posts/calico-rr/</link>
      <pubDate>Thu, 01 Feb 2018 11:03:49 +0000</pubDate>
      
      <guid>https://www.yangcs.net/posts/calico-rr/</guid>
      <description>&lt;p&gt;
&lt;iframe frameborder=&#34;no&#34; border=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; width=0 height=0 src=&#34;http://o7z41ciog.bkt.clouddn.com/Two%20Steps%20From%20Hell%20-%20Star%20Sky.mp3&#34;&gt;&lt;/iframe&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-markdown-1-style-margin-bottom-2em-margin-right-5px-padding-8px-15px-letter-spacing-2px-background-image-linear-gradient-to-right-bottom-rgb-0-188-212-rgb-63-81-181-background-color-rgb-63-81-181-color-rgb-255-255-255-border-left-10px-solid-rgb-51-51-51-border-radius-5px-text-shadow-rgb-102-102-102-1px-1px-1px-box-shadow-rgb-102-102-102-1px-1px-2px-1-名词解释-p&#34;&gt;&lt;p markdown=&#34;1&#34; style=&#34;margin-bottom:2em; margin-right: 5px; padding: 8px 15px; letter-spacing: 2px; background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181)); background-color: rgb(63, 81, 181); color: rgb(255, 255, 255); border-left: 10px solid rgb(51, 51, 51); border-radius:5px; text-shadow: rgb(102, 102, 102) 1px 1px 1px; box-shadow: rgb(102, 102, 102) 1px 1px 2px;&#34;&gt;1. 名词解释&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;endpoint&lt;/code&gt;：接入到网络中的设备称为 endpoint ❤️&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AS&lt;/code&gt;：网络自治系统，一个完全自治的网络，通过 BGP 协议与其它 AS 交换路由信息&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ibgp&lt;/code&gt;：AS 内部的 BGP Speaker，与同一个 AS 内部的 ibgp、ebgp 交换路由信息&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;ebgp&lt;/code&gt;：AS 边界的 BGP Speaker，与同一个 AS 内部的 ibgp、其它 AS 的 ebgp 交换路由信息&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;workloadEndpoint&lt;/code&gt;：Calico 网络中的分配虚拟机、容器使用的 endpoint&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;hostEndpoints&lt;/code&gt;：Calico 网络中的物理机(node)的地址&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;p-markdown-1-style-margin-bottom-2em-margin-right-5px-padding-8px-15px-letter-spacing-2px-background-image-linear-gradient-to-right-bottom-rgb-0-188-212-rgb-63-81-181-background-color-rgb-63-81-181-color-rgb-255-255-255-border-left-10px-solid-rgb-51-51-51-border-radius-5px-text-shadow-rgb-102-102-102-1px-1px-1px-box-shadow-rgb-102-102-102-1px-1px-2px-2-组网原理-p&#34;&gt;&lt;p markdown=&#34;1&#34; style=&#34;margin-bottom:2em; margin-right: 5px; padding: 8px 15px; letter-spacing: 2px; background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181)); background-color: rgb(63, 81, 181); color: rgb(255, 255, 255); border-left: 10px solid rgb(51, 51, 51); border-radius:5px; text-shadow: rgb(102, 102, 102) 1px 1px 1px; box-shadow: rgb(102, 102, 102) 1px 1px 2px;&#34;&gt;2. 组网原理&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;code&gt;Calico&lt;/code&gt; 组网的核心原理就是IP路由，每个容器或者虚拟机会分配一个 &lt;code&gt;workload-endpoint&lt;/code&gt;(wl)。&lt;/p&gt;

&lt;p&gt;从 nodeA 上的容器 A 内访问 nodeB 上的容器 B 时:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-config&#34;&gt;+--------------------+              +--------------------+ 
|   +------------+   |              |   +------------+   | 
|   |            |   |              |   |            |   | 
|   |    ConA    |   |              |   |    ConB    |   | 
|   |            |   |              |   |            |   | 
|   +-----+------+   |              |   +-----+------+   | 
|         |          |              |         |          | 
|       wl-A         |              |       wl-B         | 
|         |          |              |         |          |
+-------node-A-------+              +-------node-B-------+ 
        |    |                               |    |
        |    | type1.  in the same lan       |    |
        |    +-------------------------------+    |
        |                                         |
        |      type2. in different network        |
        |             +-------------+             |
        |             |             |             |
        +-------------+   Routers   |-------------+
                      |             |
                      +-------------+

从 ConA 中发送给 ConB 的报文被 nodeA 的 wl-A 接收，根据 nodeA 上的路由规则，经过各种 iptables 规则后，转发到 nodeB。

如果 nodeA 和 nodeB 在同一个二层网段，下一条地址直接就是 node-B，经过二层交换机即可到达。
如果 nodeA 和 nodeB 在不同的网段，报文被路由到下一跳，经过三层交换或路由器，一步步跳转到 node-B。
&lt;/code&gt;&lt;/pre&gt;

&lt;p id=&#34;div-border-left-red&#34;&gt;核心问题是，nodeA 怎样得知下一跳的地址？答案是 node 之间通过 BGP 协议交换路由信息。&lt;/p&gt;

&lt;p&gt;每个 node 上运行一个软路由软件 &lt;code&gt;bird&lt;/code&gt;，并且被设置成 &lt;code&gt;BGP Speaker&lt;/code&gt;，与其它 node 通过 BGP 协议交换路由信息。&lt;/p&gt;

&lt;p&gt;可以简单理解为，每一个 node 都会向其它 node 通知这样的信息:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;我是X.X.X.X，某个IP或者网段在我这里，它们的下一跳地址是我。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过这种方式每个 node 知晓了每个 &lt;code&gt;workload-endpoint&lt;/code&gt; 的下一跳地址。&lt;/p&gt;

&lt;h2 id=&#34;p-markdown-1-style-margin-bottom-2em-margin-right-5px-padding-8px-15px-letter-spacing-2px-background-image-linear-gradient-to-right-bottom-rgb-0-188-212-rgb-63-81-181-background-color-rgb-63-81-181-color-rgb-255-255-255-border-left-10px-solid-rgb-51-51-51-border-radius-5px-text-shadow-rgb-102-102-102-1px-1px-1px-box-shadow-rgb-102-102-102-1px-1px-2px-3-bgp-与-as-p&#34;&gt;&lt;p markdown=&#34;1&#34; style=&#34;margin-bottom:2em; margin-right: 5px; padding: 8px 15px; letter-spacing: 2px; background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181)); background-color: rgb(63, 81, 181); color: rgb(255, 255, 255); border-left: 10px solid rgb(51, 51, 51); border-radius:5px; text-shadow: rgb(102, 102, 102) 1px 1px 1px; box-shadow: rgb(102, 102, 102) 1px 1px 2px;&#34;&gt;3. BGP 与 AS&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;code&gt;BGP&lt;/code&gt; 是路由器之间的通信协议，主要用于 &lt;code&gt;AS&lt;/code&gt;（Autonomous System,自治系统）之间的互联。&lt;/p&gt;

&lt;p&gt;AS，自治系统，是一个自治的网络，拥有独立的交换机、路由器等，可以独立运转。&lt;/p&gt;

&lt;p&gt;每个 AS 拥有一个全球统一分配的 16 位的 ID 号，其中 64512 到 65535 共 1023 个 AS 号码被预留用于本地或者私用。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# calico默认使用的AS号是64512，可以修改：

# 查看
$ calicoctl config get asNumber

# 设置
$ calicoctl config set asNumber 64512
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;AS 内部有多个 &lt;code&gt;BGP speaker&lt;/code&gt;，分为 &lt;code&gt;ibgp&lt;/code&gt;、&lt;code&gt;ebgp&lt;/code&gt;，&lt;code&gt;ebgp&lt;/code&gt; 还与其它的 AS 中的 &lt;code&gt;ebgp&lt;/code&gt; 建立 BGP 连接。&lt;/p&gt;

&lt;p&gt;AS 内部的 &lt;code&gt;BGP speaker&lt;/code&gt; 通过 BGP 协议交换路由信息，最终每一个 &lt;code&gt;BGP speaker&lt;/code&gt; 拥有整个 AS 的路由信息。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;BGP speaker&lt;/code&gt; 一般是网络中的物理路由器，calico 将 node 改造成了一个路由器（软件bird)，node 上的虚拟机、容器等就是接入这个路由器的设备。&lt;/p&gt;

&lt;p&gt;AS 内部的 &lt;code&gt;BGP Speaker&lt;/code&gt; 之间有两种互联方式:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;全互联模式模式&lt;/li&gt;
&lt;li&gt;Router reflection(RR) 模式&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;3-1-bgp-speaker全互联模式&#34;&gt;3.1 BGP Speaker全互联模式&lt;/h3&gt;

&lt;p&gt;全互联模式，就是一个 &lt;code&gt;BGP Speaker&lt;/code&gt; 需要与其它所有的 &lt;code&gt;BGP Speaker&lt;/code&gt; 建立 bgp 连接（形成一个bgp mesh）。&lt;/p&gt;

&lt;p&gt;网络中 bgp 总连接数是按照 O(n^2) 增长的，有太多的 &lt;code&gt;BGP Speaker&lt;/code&gt; 时，会消耗大量的连接。&lt;/p&gt;

&lt;p&gt;Calico 默认使用全互联的方式，扩展性比较差，只能支持小规模集群，可以打开/关闭全互联模式：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ calicoctl config set nodeTonodeMesh off
$ calicoctl config set nodeTonodeMesh on
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-2-bgp-speaker-rr-模式&#34;&gt;3.2 BGP Speaker RR 模式&lt;/h3&gt;

&lt;p&gt;RR模式，就是在网络中指定一个或多个 &lt;code&gt;BGP Speaker&lt;/code&gt; 作为 反射路由（Router Reflector），RR 与所有的 &lt;code&gt;BGP Speaker&lt;/code&gt; 建立 bgp 连接。&lt;/p&gt;

&lt;p&gt;每个 &lt;code&gt;BGP Speaker&lt;/code&gt; 只需要与 RR 交换路由信息，就可以得到全网路由信息。&lt;/p&gt;

&lt;p&gt;RR 必须与所有的 &lt;code&gt;BGP Speaker&lt;/code&gt; 建立 BGP 连接，以保证能够得到全网路由信息。&lt;/p&gt;

&lt;p&gt;在 Calico 中可以通过 &lt;code&gt;Global Peer&lt;/code&gt; 实现 RR 模式。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Global Peer&lt;/code&gt; 是一个 &lt;code&gt;BGP Speaker&lt;/code&gt; ，需要手动在 Calico 中创建，所有的 node 都会与 &lt;code&gt;Global peer&lt;/code&gt; 建立 BGP 连接。&lt;/p&gt;

&lt;p&gt;关闭了全互联模式后，再将 RR 作为 &lt;code&gt;Global Peers&lt;/code&gt; 添加到 Calico 中，Calico 网络就切换到了 RR 模式，可以支撑容纳更多的 node。&lt;/p&gt;

&lt;h2 id=&#34;p-markdown-1-style-margin-bottom-2em-margin-right-5px-padding-8px-15px-letter-spacing-2px-background-image-linear-gradient-to-right-bottom-rgb-0-188-212-rgb-63-81-181-background-color-rgb-63-81-181-color-rgb-255-255-255-border-left-10px-solid-rgb-51-51-51-border-radius-5px-text-shadow-rgb-102-102-102-1px-1px-1px-box-shadow-rgb-102-102-102-1px-1px-2px-4-rr-模式部署-p&#34;&gt;&lt;p markdown=&#34;1&#34; style=&#34;margin-bottom:2em; margin-right: 5px; padding: 8px 15px; letter-spacing: 2px; background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181)); background-color: rgb(63, 81, 181); color: rgb(255, 255, 255); border-left: 10px solid rgb(51, 51, 51); border-radius:5px; text-shadow: rgb(102, 102, 102) 1px 1px 1px; box-shadow: rgb(102, 102, 102) 1px 1px 2px;&#34;&gt;4. RR 模式部署&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;集群环境：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;IP&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;主机名&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;10.10.31.190&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;kube-master&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;10.10.31.193&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;kube-node1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;10.10.31.194&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;kube-node2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;10.10.31.168&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;node1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&#34;4-1-在-node1-节点上启动反射路由实例&#34;&gt;4.1 在 node1 节点上启动反射路由实例&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker run --privileged --net=host -d \
             -e IP=&amp;lt;IPv4_RR&amp;gt; \
             [-e IP6=&amp;lt;IPv6_RR&amp;gt;] \
             -e ETCD_ENDPOINTS=&amp;lt;https://ETCD_IP:PORT&amp;gt; \
             -v &amp;lt;FULL_PATH_TO_CERT_DIR&amp;gt;:&amp;lt;MOUNT_DIR&amp;gt; \
             -e ETCD_CA_CERT_FILE=&amp;lt;MOUNT_DIR&amp;gt;/&amp;lt;CA_FILE&amp;gt; \
             -e ETCD_CERT_FILE=&amp;lt;MOUNT_DIR&amp;gt;/&amp;lt;CERT_FILE&amp;gt; \
             -e ETCD_KEY_FILE=&amp;lt;MOUNT_DIR&amp;gt;/&amp;lt;KEY_FILE&amp;gt; \
             calico/routereflector:v0.4.0

# &amp;lt;FULL_PATH_TO_CERT_DIR&amp;gt; 是你的宿主机的 etcd 证书和秘钥的存放目录
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-2-配置反射路由的集群&#34;&gt;4.2 配置反射路由的集群&lt;/h3&gt;

&lt;p&gt;反射路由关于 ipv4 的配置在 etcd 中的存储路径为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/calico/bgp/v1/rr_v4/&amp;lt;RR IPv4 address&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ipv6 的配置在 etcd 中的存储路径为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/calico/bgp/v1/rr_v6/&amp;lt;RR IPv6 address&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数据格式为 json：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;ip&amp;quot;: &amp;quot;&amp;lt;IP address of Route Reflector&amp;gt;&amp;quot;,
  &amp;quot;cluster_id&amp;quot;: &amp;quot;&amp;lt;Cluster ID for this RR (see notes)&amp;gt;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过 curl 将该条目添加到 etcd 中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# IPv4 entries
$ curl --cacert &amp;lt;path_to_ca_cert&amp;gt; --cert &amp;lt;path_to_cert&amp;gt; --key &amp;lt;path_to_key&amp;gt; -L https://&amp;lt;ETCD_IP:PORT&amp;gt;:2379/v2/keys/calico/bgp/v1/rr_v4/&amp;lt;IPv4_RR&amp;gt; -XPUT -d value=&amp;quot;{\&amp;quot;ip\&amp;quot;:\&amp;quot;&amp;lt;IPv4_RR&amp;gt;\&amp;quot;,\&amp;quot;cluster_id\&amp;quot;:\&amp;quot;&amp;lt;CLUSTER_ID&amp;gt;\&amp;quot;}&amp;quot;
# IPv6 entries
$ curl --cacert &amp;lt;path_to_ca_cert&amp;gt; --cert &amp;lt;path_to_cert&amp;gt; --key &amp;lt;path_to_key&amp;gt; -L https://&amp;lt;ETCD_IP:PORT&amp;gt;:2379/v2/keys/calico/bgp/v1/rr_v6/&amp;lt;IPv6_RR&amp;gt; -XPUT -d value=&amp;quot;{\&amp;quot;ip\&amp;quot;:\&amp;quot;&amp;lt;IPv6_RR&amp;gt;\&amp;quot;,\&amp;quot;cluster_id\&amp;quot;:\&amp;quot;&amp;lt;CLUSTER_ID&amp;gt;\&amp;quot;}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl --cacert &amp;lt;path_to_ca_cert&amp;gt; --cert &amp;lt;path_to_cert&amp;gt; --key &amp;lt;path_to_key&amp;gt; -L https://10.10.31.190:2379/v2/keys/calico/bgp/v1/rr_v4/10.10.31.168 -XPUT -d value=&amp;quot;{\&amp;quot;ip\&amp;quot;:\&amp;quot;10.10.31.168\&amp;quot;,\&amp;quot;cluster_id\&amp;quot;:\&amp;quot;1.0.0.1\&amp;quot;}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-3-配置-calico-使用反射路由&#34;&gt;4.3 配置 calico 使用反射路由&lt;/h3&gt;

&lt;p&gt;关闭全互联模式&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ calicoctl config set nodeToNodeMesh off
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;确定你的网络的 AS 号码&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ calicoctl get nodes --output=wide

# 或者使用以下命令
$ calicoctl config get asNumber
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 RR 作为 &lt;code&gt;Global Peers&lt;/code&gt; 添加到 Calico 中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ calicoctl create -f - &amp;lt;&amp;lt; EOF
apiVersion: v1
kind: bgpPeer
metadata:
  peerIP: &amp;lt;IP_RR&amp;gt;
  scope: global
spec:
  asNumber: &amp;lt;AS_NUM&amp;gt;
EOF

# &amp;lt;IP_RR&amp;gt;：反射路由的 ipv4 或 ipv6 地址
# &amp;lt;AS_NUM&amp;gt;：网络的 AS 号码
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于 BGP 协议使用 TCP 179 端口进行通信，可以在 node1 上查看一下&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ss -tnp|grep 179
ESTAB      0      0      10.10.31.168:179                10.10.31.196:55967               users:((&amp;quot;bird&amp;quot;,pid=10601,fd=8))
ESTAB      0      0      10.10.31.168:56393              10.10.31.193:179                 users:((&amp;quot;bird&amp;quot;,pid=10601,fd=9))
ESTAB      0      0      10.10.31.168:41164              10.10.31.194:179                 users:((&amp;quot;bird&amp;quot;,pid=10601,fd=10))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 node1 上查看反射路由配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker exec 52e584f5bcf3 cat /config/bird.cfg
# Generated by confd
router id 10.10.31.168;

# Watch interface up/down events.
protocol device {
  scan time 2;    # Scan interfaces every 2 seconds
}

# Template for all BGP clients
template bgp bgp_template {
  debug off;
  description &amp;quot;Connection to BGP peer&amp;quot;;
  multihop;
  import all;        # Import all routes, since we don&#39;t know what the upstream
                     # topology is and therefore have to trust the ToR/RR.
  export all;        # Export all.
  source address 10.10.31.168;  # The local address we use for the TCP connection
  graceful restart;  # See comment in kernel section about graceful restart.
}



# ------------- RR-to-RR full mesh -------------



# For RR 10.10.31.168
# Skipping ourselves




# ------------- RR as a global peer -------------



# This RR is a global peer with *all* calico nodes.




# Peering with Calico node node1
protocol bgp Global_10_10_31_193 from bgp_template {
  local as 64512;
  neighbor 10.10.31.193 as 64512;
  rr client;
  rr cluster id 1.0.0.1;
}




# Peering with Calico node node2
protocol bgp Global_10_10_31_194 from bgp_template {
  local as 64512;
  neighbor 10.10.31.194 as 64512;
  rr client;
  rr cluster id 1.0.0.1;
}




# Peering with Calico node node3
protocol bgp Global_10_10_31_196 from bgp_template {
  local as 64512;
  neighbor 10.10.31.196 as 64512;
  rr client;
  rr cluster id 1.0.0.1;
}






# ------------- RR as a node-specific peer -------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 kube-node1 上查看 calico 的配置&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker exec 7fcb072515d6 cat /etc/calico/confd/config/bird.cfg
# Generated by confd
include &amp;quot;bird_aggr.cfg&amp;quot;;
include &amp;quot;custom_filters.cfg&amp;quot;;
include &amp;quot;bird_ipam.cfg&amp;quot;;


router id 10.10.31.193;



# Configure synchronization between routing tables and kernel.
protocol kernel {
  learn;             # Learn all alien routes from the kernel
  persist;           # Don&#39;t remove routes on bird shutdown
  scan time 2;       # Scan kernel routing table every 2 seconds
  import all;
  export filter calico_ipip; # Default is export none
  graceful restart;  # Turn on graceful restart to reduce potential flaps in
                     # routes when reloading BIRD configuration.  With a full
                     # automatic mesh, there is no way to prevent BGP from
                     # flapping since multiple nodes update their BGP
                     # configuration at the same time, GR is not guaranteed to

# Watch interface up/down events.
protocol device {
  

  debug { states };


  scan time 2;    # Scan interfaces every 2 seconds
}

protocol direct {
  

  debug { states };


  interface -&amp;quot;cali*&amp;quot;, &amp;quot;*&amp;quot;; # Exclude cali* but include everything else.
}


# Template for all BGP clients
template bgp bgp_template {
  

  debug { states };


  description &amp;quot;Connection to BGP peer&amp;quot;;
  local as 64512;
  multihop;
  gateway recursive; # This should be the default, but just in case.
  import all;        # Import all routes, since we don&#39;t know what the upstream
                     # topology is and therefore have to trust the ToR/RR.
  export filter calico_pools;  # Only want to export routes for workloads.
  next hop self;     # Disable next hop processing and always advertise our
                     # local address as nexthop
  source address 10.10.31.193;  # The local address we use for the TCP connection
  add paths on;
  graceful restart;  # See comment in kernel section about graceful restart.
}


# ------------- Global peers -------------



# For peer /global/peer_v4/10.10.31.168
protocol bgp Global_10_10_31_168 from bgp_template {
  neighbor 10.10.31.168 as 64512;
}




# ------------- Node-specific peers -------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&#34;p-markdown-1-style-margin-bottom-2em-margin-right-5px-padding-8px-15px-letter-spacing-2px-background-image-linear-gradient-to-right-bottom-rgb-0-188-212-rgb-63-81-181-background-color-rgb-63-81-181-color-rgb-255-255-255-border-left-10px-solid-rgb-51-51-51-border-radius-5px-text-shadow-rgb-102-102-102-1px-1px-1px-box-shadow-rgb-102-102-102-1px-1px-2px-5-多-cluster-id-实例拓扑-p&#34;&gt;&lt;p markdown=&#34;1&#34; style=&#34;margin-bottom:2em; margin-right: 5px; padding: 8px 15px; letter-spacing: 2px; background-image: linear-gradient(to right bottom, rgb(0, 188, 212), rgb(63, 81, 181)); background-color: rgb(63, 81, 181); color: rgb(255, 255, 255); border-left: 10px solid rgb(51, 51, 51); border-radius:5px; text-shadow: rgb(102, 102, 102) 1px 1px 1px; box-shadow: rgb(102, 102, 102) 1px 1px 2px;&#34;&gt;5. 多 cluster ID 实例拓扑&lt;/p&gt;&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;当拓扑包含了多个反射路由时,BGP 利用集群 id 来保证分配路由时不陷入循环路由。
反射路由镜像帮助每个反射路由提供固定的集群 id 而不是依赖单一平行原则进行配置,这简化了整个网络的配置,但也给拓扑带来了一些限制:&lt;/p&gt;

&lt;p&gt;The Route Reflector image provided assumes that it has a fixed cluster ID for each Route Reflector rather than being configurable on a per peer basis.&lt;/p&gt;

&lt;p&gt;For example, the topology outlined in the diagram below is based on the Top of Rack model:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Each rack is assigned its own cluster ID (a unique number in IPv4 address format).&lt;/li&gt;
&lt;li&gt;Each node (server in the rack) peers with a redundant set of route reflectors specific to that rack.&lt;/li&gt;
&lt;li&gt;All of the ToR route reflectors form a full mesh with each other.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://o7z41ciog.bkt.clouddn.com/mesh-topology.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For example, to set up the topology described above, you would:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Spin up nodes N1 - N9&lt;/li&gt;
&lt;li&gt;Spin up Route Reflectors RR1 - RR6&lt;/li&gt;
&lt;li&gt;Add node specific peers, peering:
N1, N2 and N3 with RR1 and RR2
N4, N5 and N6 with RR3 and RR4
N7, N8 and N9 with RR5 and RR6&lt;/li&gt;
&lt;li&gt;Add etcd config for the Route Reflectors:
RR1 and RR2 both using the cluster ID 1.0.0.1
RR2 and RR3 both using the cluster ID 1.0.0.2
RR4 and RR5 both using the cluster ID 1.0.0.3&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
